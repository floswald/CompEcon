[
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets and Resources to Learn Julia",
    "section": "",
    "text": "My Julia Bootcamp\nOfficial Julia Manual is a great place to start.\nThe learning section of julialang.org introduces the Julia Academy, which are expert-led videos on diverse topics, all as self-contained courses and very accessible - outstanding material.\nFastrack to Julia cheatsheet.\nMATLAB-Julia-Python comparative cheatsheet by QuantEcon group Quantitative Economics with Julia by Perla, Sargent and Stachurski, in particular the initial sections.\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Setup",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "cheatsheets.html#learning-julia",
    "href": "cheatsheets.html#learning-julia",
    "title": "Cheatsheets and Resources to Learn Julia",
    "section": "",
    "text": "My Julia Bootcamp\nOfficial Julia Manual is a great place to start.\nThe learning section of julialang.org introduces the Julia Academy, which are expert-led videos on diverse topics, all as self-contained courses and very accessible - outstanding material.\nFastrack to Julia cheatsheet.\nMATLAB-Julia-Python comparative cheatsheet by QuantEcon group Quantitative Economics with Julia by Perla, Sargent and Stachurski, in particular the initial sections.",
    "crumbs": [
      "Home",
      "Setup",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "cheatsheets.html#learning-julia-libraries",
    "href": "cheatsheets.html#learning-julia-libraries",
    "title": "Cheatsheets and Resources to Learn Julia",
    "section": "Learning Julia Libraries",
    "text": "Learning Julia Libraries\n\nDataFrames.jl tutorial\nPlotting with Makie.jl and the Makie.jl documentation\nPlots.jl tutorial and Plots.jl cheatsheet",
    "crumbs": [
      "Home",
      "Setup",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "notebooks/week8/funcapprox.html",
    "href": "notebooks/week8/funcapprox.html",
    "title": "Function Approximation",
    "section": "",
    "text": "Collegio Carlo Alberto, Spring 2025\n© Florian Oswald, 2026"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#outline",
    "href": "notebooks/week8/funcapprox.html#outline",
    "title": "Function Approximation",
    "section": "Outline",
    "text": "Outline\n\nOverview of Approximation Methods\n\nInterpolation\nRegression\n\nPolynomial Interpolation\nSpline Interpolation\nMultidimensional Approximation"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#approximation-methods",
    "href": "notebooks/week8/funcapprox.html#approximation-methods",
    "title": "Function Approximation",
    "section": "Approximation Methods",
    "text": "Approximation Methods\n\nConfronted with a non-analytic function \\(f\\) (i.e. something not like \\(log(x)\\)), we need a way to numerically represent \\(f\\) in a computer.\n\nIf your problem is to compute a value function in a dynamic problem, you often don’t have an analytic representation of \\(V\\).\nIf you need to compute an equilibrium distribution for your model, you probably can’t tell it’s from one parametric family or another. Hence you need to approximate it.\n\nApproximations use data of some kind which informs us about \\(f\\). Most commonly, we know the function values \\(f(x_i)\\) at a corresponding finite set of points \\(X = \\{x_i\\}_{i=1}^N\\).\nThe task of approximation is to take that data and tell us what the function value is at \\(f(y),y\\not \\in X\\).\nTo an economist this should sound very familiar: take a dataset, learn it’s structure, and make predictions.\nThe only difference is that we can do much better here, because we have more degree’s of freedom (we can choose our \\(X\\) in \\(Y=\\beta X + \\epsilon\\))"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#isnt-this-machine-learning",
    "href": "notebooks/week8/funcapprox.html#isnt-this-machine-learning",
    "title": "Function Approximation",
    "section": "Isn’t this Machine Learning?",
    "text": "Isn’t this Machine Learning?\n\nYou should notice that this sounds very similar to what machine learning attempts to do as well: make predictions on unseen data.\nSo: what is different here?\n\nWe focus mostly on in-sample prediction.\nMachine learning exerts a lot of computational effort to train a model once very well, then uses the model to make many, many predictions.\nWe want to estimate structural economic models by changing the parameters of the model often; we would need to train the model very often, and use it make a prediction exactly once.\nThat said, there are many frontier methods which use deep learning to solve economic models: https://web.stanford.edu/~maliars/Files/JME2021.pdf, https://onlinelibrary.wiley.com/doi/full/10.1111/iere.12575 and references therein."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#some-taxonomy",
    "href": "notebooks/week8/funcapprox.html#some-taxonomy",
    "title": "Function Approximation",
    "section": "Some Taxonomy",
    "text": "Some Taxonomy\n\nLocal Approximations: approximate function and it’s derivative \\(f,f'\\) at a single point \\(x_0\\). Taylor Series: \\[ f(x) = f(x_0) + (x-x_0)f'(x_0) + \\frac{(x-x_0)^2}{2}f''(x_0) + \\dots + \\frac{(x-x_0)^n}{n!}f^{n}(x_0) \\]\nInterpolation or Colocation: find a function \\(\\hat{f}\\) that is a good fit to \\(f\\), and require that \\(\\hat{f}\\) passes through the points. If we think of there being a residual \\(\\epsilon_i = f(x_i) - \\hat{f}(x_i)\\) at each grid point \\(i\\), this methods succeeds in setting \\(\\epsilon_i=0,\\forall i\\).\nRegression: Minimize some notion of distance (or squared distance) between \\(\\hat{f}\\) and \\(f\\), without the requirement of pass through."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#doing-interpolation-in-julia",
    "href": "notebooks/week8/funcapprox.html#doing-interpolation-in-julia",
    "title": "Function Approximation",
    "section": "Doing Interpolation in Julia",
    "text": "Doing Interpolation in Julia\n\nIn practice, you will make heavy use of high-quality interpolation packages in julia.\nList in the end.\nNevertheless, function approximation is extremely problem-specific, so sometimes a certain approach does not work for your problem.\nThis is why we will go through the mechanics of some common methods.\nI would like you to know where to start drilling if you need to go and hack somebody elses code.\n\n\nusing CSV\nusing DataFrames\nusing StatsPlots\n\niris = CSV.read((joinpath(dirname(pathof(DataFrames)),\n                                 \"..\", \"docs\", \"src\", \"assets\", \"iris.csv\")),\n                       DataFrame)\ndescribe(iris)\n\n\nsize(iris)\n\n\nThis dataset is like a matrix.\nWe could think of each row as a vector. A tuple of input data, like \\(x_1 = (5.1,3.5,1.4,0.2)\\), and a one-dimensional ouput \\(y_1\\), the species label, like “setosa”. Stack the \\(x\\) to get \\(X\\) of size (150,4).\nHow can we do mathematical operations on vectors \\(x\\) like addition and scalar multiplication?\nLike, suppose we want to standardize the data in \\(X\\)? We would like to create those two objects (each of them a length 4 vector)\n\n\\[\\begin{align}\n\\mu & = \\frac{1}{150} \\sum_{i=1}^{150} x_i \\in \\mathbb{R}^4 \\\\\n\\sigma & = \\sqrt{\\frac{1}{150} \\sum_{i=1}^{150}\n\\left(x_i - \\mu_i \\right)^2} \\in \\mathbb{R}^4 \\\\\n\\end{align}\\]\n\n(the interesting part is the \\(\\sum_{i=1}^{150} x_i\\) when \\(x_i \\in \\mathbb{R}^4\\) )\nin order to then do\n\n\\[\\frac{x_1 - \\mu}{\\sigma},\\frac{x_2 - \\mu}{\\sigma},\\dots,\\frac{x_{150} - \\mu}{\\sigma}\\]\n\nusing Chain\n@chain iris begin\n    stack(_, [:SepalLength, :SepalWidth, :PetalLength, :PetalWidth])\n    subset(:Species =&gt; x -&gt; x .== \"Iris-setosa\") \n    @df density(:value, group = :variable, xlab = \"Setosa Measures\")\nend\n\n\nusing Chain\nusing Statistics\n@chain iris begin\n    subset(:Species =&gt; x -&gt; x .== \"Iris-setosa\") \n    transform( [x =&gt; (z -&gt; (z .- mean(z)) ./ std(z)) =&gt; x \n         for x in [:SepalLength, :SepalWidth, :PetalLength, :PetalWidth] ]\n    )\n    stack(_, [:SepalLength, :SepalWidth, :PetalLength, :PetalWidth])\n    @df density(:value, group = :variable, xlab = \"Standardized Setosa Measures\")\nend\n\n\nWhat is a Vector Space?\nA Vector Space is a structure \\((V,F,+,\\cdot)\\), where: 1. \\(V\\) is a set of vectors 2. \\(+: V \\times V \\mapsto V\\) is the addition operation, with\n1. $x + y = y + x$  \n1. $x + (y + z ) = (x + y ) + z$  \n1. There is a null vector: $x + 0 = x, 0 \\in V$  \n1. there is an inverse $-x \\in V, x + (-x) = 0$\n\n\\(F\\) is a field of scalars: \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)\n\\(\\cdot : F \\times V \\mapsto V\\) is scalar multiplication, satisfying\n\n\\(a(bx) = (ab) x\\)\n\\(a(x + y) = ax + ay\\)\n\\(1x = x\\)"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#examples-of-vector-spaces",
    "href": "notebooks/week8/funcapprox.html#examples-of-vector-spaces",
    "title": "Function Approximation",
    "section": "Examples of Vector Spaces",
    "text": "Examples of Vector Spaces\n\n\\((\\mathbb{R}^n,\\mathbb{R},+,\\cdot)\\): the most widely encountered vector space - particularly with \\(n=2\\).\n\\(\\mathbb{R}[x] = \\left\\{\\sum_{i=0}^n p_i x^i : p_i \\in \\mathbb{R}, n = 0,1,\\dots \\right\\}\\): We can add 2 polynomials: \\[(p + q)(x) = p(x) + q(x),\\] and also multiply with a scalar \\[(cp)(x) = c p(x)\\] which makes \\((\\mathbb{R}[x],\\mathbb{R},+,\\cdot)\\) a valid vector space."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#a-linear-basis",
    "href": "notebooks/week8/funcapprox.html#a-linear-basis",
    "title": "Function Approximation",
    "section": "A Linear Basis",
    "text": "A Linear Basis\n\n\\(V\\) could contain infinitely many vectors.\nWe can reduce this complexity by finding a set of vectors from which to easily generate the other vectors.\n\n\\[ \\begin{align}\ne_1 &= (1,0,\\dots,0)\\\\\ne_2 &= (0,1,\\dots,0)\\\\\n& \\vdots\\\\\ne_n &= (0,0,\\dots,1)\n\\end{align} \\]\nwith those \\(E = (e_1,e_2,\\dots, e_n)\\), we can represent any \\(x \\in V\\) as\n\\[x = \\sum_{i=1}^n e_i x_i, x_i \\in \\mathbb{R}, e_i \\in \\mathbb{R}^n\\]\n\n\\(E\\) is a vector space basis. With addition and scalar multiplication defined, we can create any vector \\(x\\in V\\), i.e. we can span the entire vector space.\n\\(E\\) is like a skeleton for \\(V\\) (\\(\\mathbb{R}^n\\) in this example)."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#linear-combinations",
    "href": "notebooks/week8/funcapprox.html#linear-combinations",
    "title": "Function Approximation",
    "section": "Linear Combinations",
    "text": "Linear Combinations\n\nIf instead of \\(e_i\\) we write \\(v_i\\) for any vector, and \\(b_i \\in \\mathbb{R}\\) a weight , we can write any other vector\\[z = \\sum_{i=1}^n v_i b_i, v_i \\in \\mathbb{R}^n\\] as a linear combination\nPotentially, there are multiple combinations of vectors which result in the same combination, e.g. for \\(v_1 = (1,0),v_2 = (0,1), v_3= (1,1)\\), we can have \\[(2,1) = 2 v_1 + v_2 = v_1 + v_3\\] i.e. there are 2 equivalent ways to generate \\((2,1)\\) from those vectors.\nThe set \\(S = \\{v_1 , v_2 v_3 \\}\\) contains redundant information."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#linear-independence",
    "href": "notebooks/week8/funcapprox.html#linear-independence",
    "title": "Function Approximation",
    "section": "Linear Independence",
    "text": "Linear Independence\nLet \\(V\\) be a vector space and \\(S=\\{v_1,\\dots,v_n\\}\\) be a subset of its vectors. \\(S\\) is said to be linearly dependent if it only contains the null vector, or if there is a nonzero \\(v_k\\) which can be expressed as a linear combination of the others \\[v_1,\\dots,v_{k-1},v_{k+1},\\dots,v_n\\]\nIf \\(S\\) is not linearly dependent, it is linearly independent.\nConsider \\(v_k\\), which we write as a linear combination of the other members of \\(S\\):\n\\[v_k = \\sum_{i=1}^{k-1} b_i v_i + \\sum_{i=k+1}^n b_i v_i\\] for nonzero \\(v_k\\). We already say \\(S\\) is linearly dependent with this. But look:\nSubstracting \\(v_k\\) on both sides, we get the null vector:\n\\[0 = \\sum_{i=1}^{n} b_i v_i\\]\nfor some weights \\(b_i\\), where it has to be that \\(b_k =-1\\). So: we can obtain the null vector from a nontrivial linear combination.\n\nBasis Definition\nLet \\(V\\) be a vector space and \\(S\\) be a subset of it’s vectors. \\(S\\) is called basis of \\(V\\) if 1. \\(S\\) is linearly independent 2. \\(S\\) spans \\(V\\), i.e. \\(span(S) = V\\)\nWe obtain the span of a set of vectors \\(S\\) by creating all possible linear combinations amongst them. For example, the span of two linearly independent vectors is the plane.\n\nThe elements of a basis set are called basis vectors.\n\n\n\nBasis Example in R2\n\nThe set \\(\\mathbb{R}^2\\) of ordered pairs of real numbers is a vector space \\(V\\) for\n\ncomponent-wise addition: \\((a,b) + (c,d) = (a+c, b+d)\\)\nscalar multiplication: \\(\\lambda (a,b) = (\\lambda a, \\lambda b), \\lambda \\in \\mathbb{R}\\).\n\nOne basis called the standard basis of \\(V\\) consists of two vectors:\n\n\\(e_1 = (1,0)\\)\n\\(e_2 = (0,1)\\)\n\nAny vector \\(v=(a,b) \\in \\mathbb{R}^2\\) can be uniquely written as \\(v = ae_1 + be_2\\)\nAny other pair of linearly independent vectors like \\((1,1)\\) and \\((-1,2)\\) is also a basis of \\(\\mathbb{R}^2\\)."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#what-is-a-basis-function",
    "href": "notebooks/week8/funcapprox.html#what-is-a-basis-function",
    "title": "Function Approximation",
    "section": "What is a Basis function?",
    "text": "What is a Basis function?\n\nNow we know that every vector \\(\\mathbf{v}\\) in a vector space \\(V\\) can be represented by a linear combination of basis vectors.\nSimilarly, we can represent every continuous function in a particular function space \\(F\\) by a linear combination of basis functions.\nAnother good name for basis functions is blending functions. We often use a mixture of several basis functions to find an approximation."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#approximating-functions",
    "href": "notebooks/week8/funcapprox.html#approximating-functions",
    "title": "Function Approximation",
    "section": "Approximating Functions",
    "text": "Approximating Functions\n\nLet \\(F\\) by the space of continuous real-valued functions with domain \\(\\mathbf{x}\\in \\mathbb{R}\\). \\(F\\) is a vector space.\nNext, we define an inner-product operation on that space: \\[\n  &lt;g,h&gt; = \\int_\\mathbf{x} g(x) h(x) w(x) dx\n  \\] where functions \\(g,h,w \\in F\\) and \\(w\\) is a weighting function.\nthe pair \\(\\{F, &lt;.,.&gt;\\}\\) form an inner-product vector space.\nNow we want to approximate a known function \\(f:\\mathbf{x}\\mapsto\\mathbb{R}\\) in \\(\\{F, &lt;.,.&gt;\\}\\)\nLet’s define \\(\\hat{f}(\\cdot;c)\\) to be our parametric approximation function. We generically define this as\n\n\\[  \n    \\hat{f}(x;c) = \\sum_{j=0}^{J-1} c_j \\phi_j(x)\n\\]\nwhere * \\(\\phi_j : \\mathbb{R}^d \\mapsto \\mathbb{R}\\) is called a basis function, and \\(\\Phi_J = \\{\\phi_j\\}_{j=0}^{J-1}\\). * \\(c={c_0,c_1,\\dots,c_{J-1}}\\) is a coefficient vector * The integer \\(J\\) is the order of the interpolation. * Our problem is to choose \\((\\phi_i,c)\\) in some way. * We want to minimize the residual function, i.e. \\(\\gamma(x,c) \\equiv f(x) - \\sum_{j=0}^{J-1} c_j \\phi_j(x)\\)\n\nTo find coefficients \\(c\\), one could for example minimize the squared errors, OLS: \\[\n  c^* = \\arg \\min \\int_\\mathbf{x} \\gamma(x,c) w(x) dx\n  \\]\nIn standard OLS, we’d set the weighting function to \\(w(x) = 1\\)\n\nWe will construct a grid of \\(M\\geq J\\) points \\({x_1,\\dots,x_M}\\) within the domain \\(\\mathbb{R}^d\\), and we will denote the residuals at each grid point by \\(\\epsilon = {\\epsilon_1,\\dots,\\epsilon_M}\\):\n\\[\n    \\left[\\begin{array}{c}\n        \\epsilon_1 \\\\\n         \\vdots \\\\\n        \\epsilon_M \\\\ \\end{array} \\right]  = \\left[\\begin{array}{c} f(x_1) \\\\ \\vdots \\\\ f(x_M)  \\end{array} \\right] - \\left[\\begin{array}{ccc}\n        \\phi_1(x_1) & \\dots & \\phi_J(x_1) \\\\   \n        \\vdots      & \\ddots & \\vdots \\\\   \n        \\phi_1(x_M) & \\dots & \\phi_J(x_M)    \n        \\end{array} \\right]  \\cdot\n        \\left[\\begin{array}{c} c_1 \\\\ \\vdots \\\\ c_J  \\end{array} \\right] \\\\\n        \\mathbf{\\epsilon} = \\mathbf{y} - \\mathbf{\\Phi c}\n\\]\n\nInterpolation or colocation occurs when \\(J=M\\), i.e. we have a square matrix of basis functions, and can exactly solve this.\nWe basically need to solve the system\n\n\\[\n\\begin{aligned}\n\\sum_{j=1}^n c_j \\phi_j(x_i) &= f(x_i),\\forall i=1,2,\\dots,n \\\\\n                  \\mathbf{\\Phi c}&= \\mathbf{y}\n\\end{aligned}\n\\]\nwhere the second line uses vector notation, and \\(\\mathbf{y}\\) has all values of \\(f\\). * Solution: \\(\\mathbf{c}= \\mathbf{\\Phi}^{-1}y\\)."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#regression-basics",
    "href": "notebooks/week8/funcapprox.html#regression-basics",
    "title": "Function Approximation",
    "section": "Regression Basics",
    "text": "Regression Basics\n\nIf we have more evaluation points than basis functions, \\(M&gt;J\\) say, interpolation nodes than basis functions, we cannot do that. Instead we can define a loss function, and minimize it.\nIn the case of squared loss, of course, this leads to the least squares solution:\n\n\\[\n\\begin{aligned} e_i &= f(x_i) - \\sum_{j=1}^n c_j \\phi_j(x_i) \\\\\n        \\min_c e_i^2 & \\implies \\\\\n        c            &= (\\Phi'\\Phi)^{-1} \\Phi'y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#spectral-and-finite-element-methods",
    "href": "notebooks/week8/funcapprox.html#spectral-and-finite-element-methods",
    "title": "Function Approximation",
    "section": "Spectral and Finite Element Methods",
    "text": "Spectral and Finite Element Methods\n\nSpectral Methods are such that the basis functions are non-zero over the entire domain of \\(f\\).\n\nPolynomial interpolation\nChebychev interpolation\n\nFinite Element methods are such that basis functions are non-zero only on a subset of the domain.\n\nSplines\n\nLinear splines, i.e. splines of degree 1, a.k.a. linear approximation\nHigher order splines, mainly the cubic spline."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#what-makes-a-good-approximation",
    "href": "notebooks/week8/funcapprox.html#what-makes-a-good-approximation",
    "title": "Function Approximation",
    "section": "What makes a good Approximation?",
    "text": "What makes a good Approximation?\n\nShould be arbitrarily accurate as we increase \\(n\\).\n\\(\\Phi\\) Should be efficiently (fast) computable. If \\(\\Phi\\) were differentiable, we could easily get e.g. \\(\\hat{f}'(x) = \\sum_{j=1}^J c_j \\phi_j'(x_i)\\)\n\\(c\\) Should be efficiently (fast) computable."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#what-polynomial-to-use-what-form-for-phi",
    "href": "notebooks/week8/funcapprox.html#what-polynomial-to-use-what-form-for-phi",
    "title": "Function Approximation",
    "section": "What Polynomial to use? What form for \\(\\Phi\\)?",
    "text": "What Polynomial to use? What form for \\(\\Phi\\)?\n\nIn principle the monomial basis could be used. It is just the power functions of \\(x\\): \\(1,x,x^2,x^3,\\dots\\)\nStacking this up for each evaluation node gives the Vandermonde Matrix:\n\n\\[\nV = \\left[\\begin{matrix}\n        1 & x_1 & \\dots & x_1^{n-2} & x_1^{n-1} \\\\\n        1 & x_2 & \\dots & x_2^{n-2} & x_2^{n-1} \\\\\n        \\vdots & \\vdots & \\ddots &  & \\vdots \\\\\n        1 & x_m & \\dots & x_m^{n-2} & x_m^{n-1}\n        \\end{matrix} \\right]\n\\]\nfor the case with \\(m\\) evaluation nodes for \\(x\\), and \\(n\\) basis functions for each \\(x_i\\).\n\nChoosing Basis Matrices\n\nIf we choose \\(\\Phi\\) so that it’s elements share a lot of information, it will be hard to invert it.\nSo, if the elements share very little information, it’s easier to invert, hence good for us\nFor example, if we have \\(\\Phi\\) close to diagonal (i.e. different basis \\(\\phi_j(x)\\) are non-zero at different \\(x\\)).\nSuch \\(\\Phi\\) are generated by using orthogonal Polynomials (under our inner product definition).\nAn orthogonal Polynomial has 2 sequences of polynomials orthogonal under an inner product: their inner product is zero under a certain weighting function \\(w(x)\\).\n\n\n\nChoosing Basis Matrices\n\nFor example, the Chebyshev polynomials are orthogonal wrt. to weight \\(\\frac{1}{\\sqrt{1-x^2}}\\)\nfor degrees \\(n,m\\) of that polynomial, \\(T_n,T_m\\), we have \\[\n  \\int_{-1}^1 T_n(x)T_m(x)\\,\\frac{dx}{\\sqrt{1-x^2}}=\n  \\begin{cases}\n      0 & n\\ne m \\\\ \\pi & n=m=0 \\\\ \\frac{\\pi}{2} & n=m\\ne 0\n  \\end{cases}\n  \\]\n\n\n\nChebyshev Polynomials of the first kind\n\nThere is a nice recursion to get \\(T_n(x)\\): \\[\\begin{align}\n      T_0(x) & = 1 \\\\\n      T_1(x) & = x \\\\\n      T_{n+1}(x) & = 2xT_n(x) - T_{n-1}(x).\n  \\end{align}\\]\nCode that up and compute \\(T_{3}(0.5)\\)!\nCheck you get the same as with the alternative closed form \\(T_n(x) = \\cos(n \\arccos(x))\\)\n\n\nfunction T(x,n)\n    @assert (x &gt;= -1) & (x &lt;= 1)\n    if n == 0\n        return 1.0\n    elseif n==1\n        return x\n    else\n        2*x*T(x,n-1) - T(x,n-2)\n    end\nend\nT2(x,n) = cos(n* acos(x))\n@show T(0.5,3)\nusing Test\n@assert T2(0.5,3) == T(0.5,3)        \n\n\n# weighting function\nw(x) = 1.0 / sqrt(1-x^2)\nusing Statistics\nusing Test\n\n# simple integration works for n not equal m\n@assert isapprox(mean(T(x,0) * T(x,1) * w(x) for x in range(-0.99,stop = 0.99, length=100)), 0.0, atol = 1e-16)\n@assert isapprox(mean(T(x,4) * T(x,5) * w(x) for x in range(-0.99,stop = 0.99, length=100)), 0.0, atol = 1e-16)"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#chebyshev-nodes",
    "href": "notebooks/week8/funcapprox.html#chebyshev-nodes",
    "title": "Function Approximation",
    "section": "Chebyshev Nodes",
    "text": "Chebyshev Nodes\n\nChebyshev Nodes are the roots of the chebyshev polynomial (i.e. where \\(P(x) = 0\\)).\nThey are defined in the interval \\([-1,1]\\) as \\[ x_i = \\cos\\left(\\frac{2k-1}{2n} \\pi\\right), k=1,\\dots,n \\]\nWhich maps to general interval \\([a,b]\\) as \\[ x_i = \\frac{1}{2} (a+b) + \\frac{1}{2} (b-a) \\cos\\left(\\frac{2k-1}{2n} \\pi\\right) , k=1,\\dots,n \\]\nChebyshev nodes are not evenly spaced: there are more points towards the boundaries. You might remember that from Gaussian Integration nodes.\n\n\nusing Plots\nusing FastGaussQuadrature\ngcnodes = gausschebyshev(11)  # generate 21 Chebyshev Nodes\ngr()\nscatter(gcnodes,ones(21),ylims=(0.9,1.1),legend=false,size=(600,100),yticks=nothing)\n\n\nConstructing \\(\\Phi\\) as \\(T\\) evaluated at the Chebyshev Nodes\n\nCombining Chebyshev nodes evaluated at the roots \\(T\\) of the Cheby polynomial to construct \\(\\Phi\\) is a particularly good idea.\nDoing so, we obtain an interpolation matrix \\(\\Phi\\) with typical element \\[ \\phi_{ij} = \\cos\\left( \\frac{(n-i+0.5)(j-1)\\pi}{n}\\right)  \\]\nAnd we obtain that \\(\\Phi\\) is indeed orthogonal \\[ \\Phi^T \\Phi = \\text{diag}\\{n,n/2,n/2,\\dots,n/2\\}  \\]\n\n\nusing BasisMatrices\nx = range(-4, stop = 4 ,length = 10)\nϕ = Basis(ChebParams(length(x),-4,4))\nϕ\n\n\nS, y = nodes(ϕ)\n\n\nΦ = BasisMatrix(ϕ, Expanded(), S, 0)\n\n\nΦ.vals[1]' * Φ.vals[1]"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#chebyshev-interpolation-proceedure",
    "href": "notebooks/week8/funcapprox.html#chebyshev-interpolation-proceedure",
    "title": "Function Approximation",
    "section": "(Chebyshev) Interpolation Proceedure",
    "text": "(Chebyshev) Interpolation Proceedure\n\nLet’s summarize this proceedure.\nInstead of Chebyshev polynomials we could be using any other suitable family of polynomials.\nTo obtain a Polynomial interpolant \\(\\hat{f}\\), we need:\n\na function to \\(f\\) interpolate. We need to be able to get the function values somehow.\nA set of (Chebyshev) interpolation nodes at which to compute \\(f\\)\nAn interpolation matrix \\(\\Phi\\) that corresponds to the nodes we have chosen.\nA resulting coefficient vector \\(c\\)\n\nTo obtain the value of the interpolation at \\(x'\\) off our grid, we also need a way to evaluate \\(\\Phi(x')\\).\n\nEvaluate the Basis function \\(\\Phi\\) at \\(x'\\): get \\(\\Phi(x')\\)\nobtain new values as \\(y = \\Phi(x') c\\)."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#polynomial-interpolation-with-julia-approxfun.jl",
    "href": "notebooks/week8/funcapprox.html#polynomial-interpolation-with-julia-approxfun.jl",
    "title": "Function Approximation",
    "section": "Polynomial Interpolation with Julia: ApproxFun.jl",
    "text": "Polynomial Interpolation with Julia: ApproxFun.jl\n\nApproxFun.jl is a Julia package based on the Matlab package chebfun. It is quite amazing.\nMore than just function approximation. This is a toolbox to actually work with functions.\ngiven 2 functions \\(f,g\\), we can do algebra with them, i.e. \\(h(x) = f(x) + g(x)^2\\)\nWe can differentiate and integrate\nSolve ODE’s and PDE’s\nrepresent periodic functions\nHead over to the website and look at the readme.\n\n\nusing LinearAlgebra, SpecialFunctions, Plots, ApproxFun\nx = Fun(identity,0..10)\nf = sin(x^2)\ng = cos(x)\n\nh = f + g^2\nr = roots(h)\nrp = roots(h')\n\nusing Plots\nplot(h,labels = \"h\")\nscatter!(r,h.(r),labels=\"roots h\")\nscatter!(rp,h.(rp),labels = \"roots h'\")\n\n\n# works even with discontinuities!\nff = x-&gt;sign(x-0.1)/2 + cos(4*x);  # sign introduces a jump at 0.1\nx  = Fun(identity)  # set up a function space\nspace(x)\nf  = ff(x)  # define ff on that space\nplot(f) # plot\n\n\n# whats the first deriv of that function at at 0.785?\nprintln(f'(0.785))\n# and close to the jump, at 0.100001?\nprintln(f'(0.1000001))\n\n\nThe main purpose of this package is to manipulate analytic functions, i.e. function with an algebraic representation.\nThere is the possibility to supply a set of data points and fit a polynomial:\n\n\nS = Chebyshev(1..2);\nn = 100; m = 50;\np = range(1,stop=2,length=n);   # a non-default grid\nv = exp.(p);           # values at the non-default grid\nV = Array{Float64}(undef,n,m); # Create a Vandermonde matrix by evaluating the basis at the grid\nfor k = 1:m\n    V[:,k] = Fun(S,[zeros(k-1);1]).(p)\nend\nf = Fun(S,V\\v);\n@show f(1.1)\n@show exp(1.1)"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#splines-piecewise-polynomial-approximation",
    "href": "notebooks/week8/funcapprox.html#splines-piecewise-polynomial-approximation",
    "title": "Function Approximation",
    "section": "Splines: Piecewise Polynomial Approximation",
    "text": "Splines: Piecewise Polynomial Approximation\n\nSplines are a finite element method, i.e. there are regions of the function domain where some basis functions are zero.\nAs such, they provide a very flexible framework for approximation instead of high-order polynomials.\n\nKeep in mind that Polynomials basis functions are non-zero on the entire domain. Remember the Vandermonde matrix.\n\nThey bring some element of local approximation back into our framework. What happens at one end of the domain to the function is not important to what happens at the other end.\nLooking back at the previous plot of random data: we are searching for one polynomial to fit all those wiggles. A spline will allow us to design different polynomials in different parts of the domain.\n\n\nSplines: Basic Setup\n\nThe fundamental building block is the knot vector, or the breakpoints vector \\(\\mathbf{z}\\) of length \\(p\\). An element of \\(\\mathbf{z}\\) is \\(z_i\\).\n\\(\\mathbf{z}\\) is ordered in ascending order.\n\\(\\mathbf{z}\\) spans the domain \\([a,b]\\) of our function, and we have that \\(a=z_1,b=z_p\\)\nA spline is of order k if the polynomial segments are k-th order polynomials.\nLiterature: [@deboor]  is the definitive reference for splines.\n\n\n\nSplines: Characterization\n\nGiven \\(p\\) knots, there are \\(p-1\\) polynomial segments of order \\(k\\), each characterized by \\(k+1\\) coefficients, i.e. a total of \\((p-1)(k+1)\\) parameters.\nHowever, we also require the spline to be continuous and differentiable of degree \\(k-1\\) at the \\(p-2\\) interior breakpoints.\nImposing that uses up an additional \\(k(p-2)\\) conditions.\nWe are left with \\(n = (p-1)(k+1) - k(p-2) = p+k-1\\) free parameters.\nA Spline of order \\(k\\) and \\(p\\) knots can thus be written as a linear combination of it’s \\(n = p+k-1\\) basis functions."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#b-splines-definition",
    "href": "notebooks/week8/funcapprox.html#b-splines-definition",
    "title": "Function Approximation",
    "section": "B-Splines: Definition",
    "text": "B-Splines: Definition\n\nWe mostly use Basis Splines, or B-Splines.\nHere is a recursive definition of a B-Spline (and what is used in ApproXD):\nDenote the \\(j\\)-th basis function of degree \\(k\\) with knot vector \\(\\mathbf{z}\\) at \\(x\\) as \\(B_j^{k,\\mathbf{z}} (x)\\)\nAgain, there are \\(n = k + p - 1\\) \\(B\\)’s (where \\(p\\)= length(z))\nWe can define \\(B_j^{k,\\mathbf{z}} (x)\\) recursively like this:\n\\[\n  B_j^{k,\\mathbf{z}} (x) = \\frac{x-z_{j-k}}{z_j - z_{j-k}} B_{j-1}^{k-1,\\mathbf{z}} (x)  + \\frac{z_{j+1}-x}{z_{j+1} - z_{j+1-k}} B_{j}^{k-1,\\mathbf{z}} (x), j=1,\\dots,n\n  \\]\nThe recursion starts with\n\n\\[ B_j^{0,\\mathbf{z}} (x) = \\begin{cases}\n    1 & \\text{if }z_j \\leq x &lt;  z_{j+1}\\\\\n    0 & \\text{otherwise.}\n    \\end{cases}\n\\]\n\nFor this formulation to work, we need to extend the knot vector for \\(j&lt;1,j&gt;p\\):\n\n\\[ z_j = \\begin{cases}\n    a & \\text{if }j \\leq 1\\\\\n    b & \\text{if }j \\geq p\n    \\end{cases}\n\\]\n\nAnd we need to set the endpoints \\[ B_0^{k-1,\\mathbf{z}} = B_n^{k-1,\\mathbf{z}} =0 \\]\nYou may see that this gives rise to a triangular computation strategy, as pointed out here."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#b-splines-derivatives-and-integrals",
    "href": "notebooks/week8/funcapprox.html#b-splines-derivatives-and-integrals",
    "title": "Function Approximation",
    "section": "B-Splines: Derivatives and Integrals",
    "text": "B-Splines: Derivatives and Integrals\n\nThis is another very nice thing about B-Splines.\nThe derivative wrt to it’s argument \\(x\\) is\n\\[ \\frac{d B_j^{k,\\mathbf{z}} (x)}{dx} = \\frac{k}{z_j - z_{j-k}} B_{j-1}^{k-1,\\mathbf{z}} (x)  + \\frac{k}{z_{j+1} - z_{j+1-k}} B_{j}^{k-1,\\mathbf{z}} (x), j=1,\\dots,n\n  \\]\nSimilarly, the Integral is just the sum over the basis functions: \\[ \\int_a^x B_j^{k,\\mathbf{z}} (y) dy = \\sum_{i=j}^n \\frac{z_i - z_{i-k}}{k} B_{i+1}^{k+1,\\mathbf{z}} (x)  \\]\n\n\nusing ApproXD   \n# ] dev https://github.com/floswald/ApproXD.jl\nbs = BSpline(7,3,0,1) #7 knots, degree 3 in [0,1]\n# how many basis functions? (go back 1 slide.)\n# getNumCoefs(bs)\nx = range(0,stop =1.0, length = 500)\nB = Array(getBasis(collect(x),bs))\nplot(x,B,layout=(3,3),grid=false,ylim=(-0.1,1.1),legend=false,linewidth=2,linecolor=:black)\n\n\n# Notice that placing each of those panels on top of each other generates a sparse matrix!\nplot(x,B,grid=false,ylim=(-0.1,1.1),legend=false, lw = 3, color = reshape(Plots.distinguishable_colors(9),1,9))\n\nhttps://jipolanco.github.io/BSplineKit.jl/dev/generated/approximation/ https://fluxml.ai/Flux.jl/stable/guide/models/quickstart/#man-quickstart"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#linear-b-spline-a-useful-special-case",
    "href": "notebooks/week8/funcapprox.html#linear-b-spline-a-useful-special-case",
    "title": "Function Approximation",
    "section": "Linear B-Spline: A useful special case",
    "text": "Linear B-Spline: A useful special case\n\nThis is connecting the dots with a straight line\nThis may incur some approximation error if the underlying function is very curved between the dots.\nHowever, it has some benefits:\n\nit is shape-preserving,\nit is fast,\nit is easy to build.\n\nFor a linear spline with evenly spaced breakpoints, this becomes almost trivial.\n\nLet’s define \\(h = \\frac{b-a}{n-1}\\) as the distance between breakpoints.\nOur basis function becomes very simple, giving us a measure of how far \\(x\\) is from the next knot:\n\n\n\\[ \\phi_j (x) = \\begin{cases}\n    1 - \\frac{|x-z_j|}{h} & \\text{if } |x-z_j| \\leq h \\\\\n    0                    & \\text{otherwise}\n    \\end{cases}\n\\]\n\nNotice that each interior basis function (i.e. not 0 and not \\(n\\)) has witdth \\(2h\\).\n\n\nbs = BSpline(9,1,0,1) #9 knots, degree 1 in [0,1]\nx = range(0,stop = 1.0,length = 500)\nB = Array(getBasis(collect(x),bs))\nplot(x,B,layout=(3,3),grid=false,ylim=(-0.1,1.1),legend=false,linewidth=2,linecolor=:black)"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#linear-b-spline-evaluation",
    "href": "notebooks/week8/funcapprox.html#linear-b-spline-evaluation",
    "title": "Function Approximation",
    "section": "Linear B-Spline: Evaluation",
    "text": "Linear B-Spline: Evaluation\n\nIn order to evaluate the linear interpolator, we need to know only one thing: Which knot span is active, i.e. what is \\(j\\) s.t. \\(x\\in [z_j, z_{j+1}]\\)?\nThis is a classic problem in computer science. Binary search.\njulia implements searchsortedlast.\nOnce we know \\(j\\), it’s easy to get the interpolated value as \\[ \\hat{f}(x) = \\frac{x-z_j}{h}  f(z_{j+1}) + \\frac{z_{j+1}-x}{h}  f(z_{j}) \\]"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#the-importance-of-knot-placement",
    "href": "notebooks/week8/funcapprox.html#the-importance-of-knot-placement",
    "title": "Function Approximation",
    "section": "The Importance of Knot Placement",
    "text": "The Importance of Knot Placement\n\nWe just talked about equally spaced knots. This is just a special case.\nB-Splines give us the flexibility to place the knots where we want.\nContrary to Polynomial interpolations (where we cannot choose the evaluation nodes), this is very helpful in cases where we know that a function is very curved in a particular region.\nCanonical Example: Runge’s function: \\(f(x) = (1+25x^2)^{-1}\\).\nAlso: If you know that your function has a kink (i.e. a discontinuous first derivative) at \\(\\hat{x}\\), then you can stack breakpoints on top of each other at \\(\\hat{x}\\)\nThat’s going to be part of your homework!\n\n\ngg(x) = (1+25x^2)^(-1)\nplot(gg,lw = 4,leg = false)"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#interpolation-with-interpolations.jl",
    "href": "notebooks/week8/funcapprox.html#interpolation-with-interpolations.jl",
    "title": "Function Approximation",
    "section": "Interpolation with Interpolations.jl",
    "text": "Interpolation with Interpolations.jl\n\nInterpolations.jl assumes that data is uniformly spaced on grid 1:N\nSame for multiple dimensions\nHowever, we can scale 1:N to different domains\nfinally, we can also supply our own, non-uniform grids.\nCan get gradients of interpolations right away.\n\n\n# interpolation\n# restart kernel!\nusing Interpolations\nA = rand(10,5)\nitp = interpolate(A, Interpolations.BSpline(Quadratic(Reflect(OnCell()))))\nitp(1.9,4.9)\n\n\nA\n\n\nA[2,5]\n\n\nA_x = 1.:2.:40.   # x in [1,40]\nA = [log(x) for x in A_x] # f(x)\nitp = interpolate(A, Interpolations.BSpline(Cubic(Interpolations.Line(OnGrid()))))\nsitp = scale(itp, A_x)  # scale x-axis of interpolator\n@show sitp(3.) # exactly log(3.)\n@show sitp(3.5) # approximately log(3.5)\n@show itp(3);  # is the 3rd index(!) in A, not the *value* 3\n\n\n# Same for 2D\nA_x1 = 1:.1:10\nA_x2 = 1:.5:20\nfff(x1, x2) = log(x1+x2)\nA = [fff(x1,x2) for x1 in A_x1, x2 in A_x2]\nitp = interpolate(A, Interpolations.BSpline(Cubic(Interpolations.Line(OnGrid()))))\nsitp = scale(itp, A_x1, A_x2)\nsitp(5., 10.) # exactly log(5 + 10)\nsitp(5.6, 7.1) # approximately log(5.6 + 7.1)\n\n\nVery often we need a gridded interpolation. I.e. we supply the function values on an irregular grid.\nFor this occasion, the GriddedInterpolation type is useful.\nFor now this only works in 3 modes:\n\nGridded(Linear())\nGridded(Constant()) nearest neighbor\nNoInterp (you must supply index ON grid)\n\n\n\nA = rand(20)\nA_x = collect(1.0:2.0:40.0)\nknots = (A_x,)\nitp = interpolate(knots, A, Gridded(Linear()))\nitp(2.0)\n\n\n# 2D\nA = rand(8,20)\nknots = ([x^2 for x = 1:8], [0.2y for y = 1:20])\nitp = interpolate(knots, A, Gridded(Linear()))\nitp(4,1.2)  # approximately A[2,6]\n\n\n# we can mix modes across dimensions!\nitp = interpolate(knots, A, (Gridded(Linear()),Gridded(Constant())))\nitp(4,1.2)\n\n\nWhat about vector valued interpolations?\nSuppose we have a function \\(f : \\mathbb{R} \\mapsto \\mathbb{R}^2\\)\nEconomics example:\n\n\\[  \nf(x) = \\left(\\begin{array}{c} \\text{savings}(x) \\\\ \\text{consumption}(x) \\end{array} \\right)\n\\]\n\n\\(x\\) is cash on hand.\nWe often have situations where several functions are defined on a common support \\(x\\).\nwhat is \\(f(1.75)\\)?\n\n\nusing StaticArrays\nx = range(1,stop = 3, length = 200)  # cash on hand\na = Float64[log(1+j)*i for j in x, i in 1:2]   # cons and save function\nb = reinterpret(SVector{2,Float64}, a')[:]\nitp = interpolate(b, Interpolations.BSpline(Quadratic(Reflect(OnCell()))))\n@show itp(3)\nsitp = scale(itp,x)\n@show sitp(3);\n\n\nv = sitp(1.75)  # get interpolated values for both function\nplot(x,a,labels=[\"save\" \"cons\"],yticks=convert(Array{Float64},v),xlabel=\"current cash\",lw = 3)\nvline!([1.75],lab=\"\")\nhline!([v[1],v[2]],lab=\"\")"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#the-compecon-toolbox-of-miranda-and-fackler",
    "href": "notebooks/week8/funcapprox.html#the-compecon-toolbox-of-miranda-and-fackler",
    "title": "Function Approximation",
    "section": "The CompEcon Toolbox of Miranda and Fackler",
    "text": "The CompEcon Toolbox of Miranda and Fackler\n\nanother good alternative:\nCompEcon.jl"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#multidimensional-approximation",
    "href": "notebooks/week8/funcapprox.html#multidimensional-approximation",
    "title": "Function Approximation",
    "section": "Multidimensional Approximation",
    "text": "Multidimensional Approximation\n\nUp to now, most of what we did was in one dimesion.\nEconomic problems often have more dimension than that.\n\nThe number of state variables in your value functions are the number of dimensions.\n\nWe can readily extend what we learned into more dimensions.\nHowever, we will quickly run into feasibility problems: hello curse of dimensionality."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#tensor-product-of-univariate-basis-functions-product-rule",
    "href": "notebooks/week8/funcapprox.html#tensor-product-of-univariate-basis-functions-product-rule",
    "title": "Function Approximation",
    "section": "Tensor Product of univariate Basis Functions: Product Rule",
    "text": "Tensor Product of univariate Basis Functions: Product Rule\n\nOne possibility is to approximate e.g. the 2D function \\(f(x,y)\\) by \\[ \\hat{f}(x,y) = \\sum_{i=1}^n \\sum_{j=1}^m c_{i,j} \\phi_i^x(x) \\phi_j^y(y)  \\]\n\nhere \\(\\phi_i^x\\) is the basis function in \\(x\\) space,\nyou can see that the coefficient vector \\(c_{i,j}\\) is indexed in two dimensions now.\nNotice that our initial notation was general enough to encompass this case, as we defined the basis functions as \\(\\mathbb{R}^d \\mapsto \\mathbb{R}\\). So with the product rule, this mapping is just given by \\(\\phi_i^x(x) \\phi_j^y(y)\\).\n\nThis formulation requires that we take the product of \\(\\phi_i^x(x), \\phi_j^y(y)\\) at all combinations of their indices, as is clear from the summations.\nThis is equivalent to the tensor product between \\(\\phi_i^x\\) and \\(\\phi_j^y\\)."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#computing-coefficients-from-tensor-product-spaces",
    "href": "notebooks/week8/funcapprox.html#computing-coefficients-from-tensor-product-spaces",
    "title": "Function Approximation",
    "section": "Computing Coefficients from Tensor Product Spaces",
    "text": "Computing Coefficients from Tensor Product Spaces\n\nExtending this into \\(D\\) dimensions, where in each dim \\(i\\) we have \\(n_i\\) basis functions, we get \\[ \\hat{f}(x_1,x_2,\\dots,x_D) = \\sum_{i_1=1}^{n_1} \\sum_{i_2=1}^{n_2} \\dots  \\sum_{i_D=1}^{n_D} c_{i_1,i_2,\\dots,i_D} \\phi_{i_1}(x_1) \\phi_{i_2}(x_2) \\dots \\phi_{i_D}(x_D)  \\]\nIn Vector notation \\[\n  \\hat{f}(x_1,x_2,\\dots,x_D) =  \\left[ \\phi_{D}(x_D) \\otimes \\phi_{D-1}(x_{D-1})  \\otimes \\dots  \\otimes  \\phi_{1}(x_1) \\right]  c \\] where \\(c\\) is is an \\(n=\\Pi_{i=1}^D n_i\\) column vector\nThe solution is the interpolation equation as before, \\[ \\begin{aligned}\\Phi c =& y \\\\\n                  \\Phi   =& \\Phi_D \\otimes \\Phi_{D-1} \\otimes \\dots \\otimes \\Phi_{1} \\end{aligned} \\]\n\n\nThe Problem with Tensor Product of univariate Basis Functions\n\nWhat’s the problem?\nWell, solving \\(\\Phi c = y\\) is hard.\nIf we have as many evaluation points as basis functions in each dimension, i.e. if each single \\(\\Phi_i\\) is a square matrix, \\(\\Phi\\) is of size (n,n).\nInverting this is extremely hard even for moderately sized problems.\nSometimes it’s not even possible to allocate \\(\\Phi\\) in memory.\nHere it’s important to remember the sparsity structure of a spline basis function.\n\n\nusing ApproXD\ngr()\nbs = ApproXD.BSpline(7,3,0,1) #7 knots, degree 3 in [0,1]\nn = 500\neval_points = collect(range(0,stop = 1.0,length = n))\nB = Array(ApproXD.getBasis(eval_points,bs))\nys = [string(\"Basis\",i) for i = size(B)[2]-1:-1:0]\nheatmap(eval_points,ys,reverse(B',dims=1))\n\n\nThis is a cubic spline basis. at most \\(k+1=4\\) basis are non-zero for any \\(x\\).\n\n\nheatmap(eval_points,ys,reverse(B',dims=1) .&gt; 0, cbar = false)"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#using-sparsity-of-splines",
    "href": "notebooks/week8/funcapprox.html#using-sparsity-of-splines",
    "title": "Function Approximation",
    "section": "Using Sparsity of Splines",
    "text": "Using Sparsity of Splines\n\nIt may be better to store the splines in sparse format.\nLook at object B by typing B and typeof(B)\nThere are sparse system solvers available.\nCreating and storing the inverse of \\(\\Phi\\) destroys the sparsity structure (inverse of a sparse matrix is not sparse), and may not be a good idea.\nLook back at Computing coefficients form the tensor product\nWe only have to sum over the non-zero entries! Every other operation is pure cost.\nThis is implemented in ApproXD.jl for example via\n\n    function evalTensor2{T}(mat1::SparseMatrixCSC{T,Int64},\n                            mat2::SparseMatrixCSC{T,Int64},\n                            c::Vector{T})"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#high-dimensional-functions-introducing-the-smolyak-grid",
    "href": "notebooks/week8/funcapprox.html#high-dimensional-functions-introducing-the-smolyak-grid",
    "title": "Function Approximation",
    "section": "High Dimensional Functions: Introducing the Smolyak Grid",
    "text": "High Dimensional Functions: Introducing the Smolyak Grid\n\nThis is a modification of the Tensor product rule.\nIt elemininates points from the full tensor product according to their importance for the quality of approximation.\nThe user controls this quality parameter, thereby increasing/decreasing the size of the grid.\n[@jmmv] is a complete technical reference for this method.\n[@maliar-maliar] chapter 4 is very good overview of this topic, and the basis of this part of the lecture."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#the-smolyak-grid-in-2-dimensions",
    "href": "notebooks/week8/funcapprox.html#the-smolyak-grid-in-2-dimensions",
    "title": "Function Approximation",
    "section": "The Smolyak Grid in 2 Dimensions",
    "text": "The Smolyak Grid in 2 Dimensions\n\nApproximation level \\(\\mu \\in \\mathbb{N}\\) governs the quality of the approximation.\nStart with a unidimensional grid of points \\(x\\): \\[ x = \\left\\{-1,\\frac{-1}{\\sqrt{2}},0,\\frac{1}{\\sqrt{2}},1\\right\\} \\] which are 5 Chebyshev nodes (it’s not important that those are Chebyshev nodes, any grid will work).\nA 2D tensor product \\(x\\otimes x\\) gives 25 grid points \\[ x\\otimes x=\\left\\{(-1,-1),(-1,\\frac{-1}{\\sqrt{2}}),\\dots,(1,1)\\right\\} \\]\nThe Smolyak method proceeds differently.\nWe construct three nested sets:\n\n\\[ \\begin{array}{l}\n        i=1 : S_1 = \\{0\\} \\\\\n        i=2 : S_2 = \\{0,-1,1\\} \\\\\n        i=3 : S_3 = \\left\\{-1,\\frac{-1}{\\sqrt{2}},0,\\frac{1}{\\sqrt{2}},1\\right\\}  \\end{array} \\]\n\nThen, we construct all possible 2D tensor products using elements from these nested sets in a table (next slide).\nFinally, we select only those elements of the table, that satisfy the Smolyak rule: \\[ i_1 + i_2 \\leq d + \\mu \\] where \\(i_1,i_2\\) are column and row index, respectively, and \\(d,\\mu\\) are the number of dimensions and the quality of approximation."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#selecting-elements",
    "href": "notebooks/week8/funcapprox.html#selecting-elements",
    "title": "Function Approximation",
    "section": "Selecting Elements",
    "text": "Selecting Elements\n\nDenote the Smolyak grid for \\(d\\) dimensions at level \\(\\mu\\) by \\(\\mathcal{H}^{d,\\mu}\\).\nif \\(\\mu=0\\) we have \\(i_1+i_2\\leq 2\\). Only one point satisfies this, and \\[ \\mathcal{H}^{2,0} = \\{(0,0)\\} \\]\nif \\(\\mu=1\\) we have \\(i_1+i_2\\leq 3\\). Three cases satisfy this:\n\n\\(i_1 = 1, i_2=1 \\rightarrow (0,0)\\)\n\\(i_1 = 1, i_2=2 \\rightarrow (0,0),(0,-1),(0,1)\\)\n\\(i_1 = 2, i_2=1 \\rightarrow (0,0),(-1,0),(1,0)\\)\n\n\nTherefore, the unique elements from the union of all of those is \\[ \\mathcal{H}^{2,1} = \\{(0,0),(-1,0),(1,0),(0,-1),(0,1)\\} \\]\n\nif \\(\\mu=2\\) we have \\(i_1+i_2\\leq 4\\). Six cases satisfy this:\n\n\\(i_1 = 1, i_2=1\\)\n\\(i_1 = 1, i_2=2\\)\n\\(i_1 = 2, i_2=1\\)\n\\(i_1 = 1, i_2=3\\)\n\\(i_1 = 2, i_2=2\\)\n\\(i_1 = 3, i_2=1\\)\n\n\nTherefore, the unique elements from the union of all of those is \\[ \\mathcal{H}^{2,2} = \\left\\{(-1,1),(0,1),(1,1),(-1,0),(0,0),(1,0),(-1,-1),(0,-1),(1,-1),\\left(\\frac{-1}{\\sqrt{2}},0\\right),\\left(\\frac{1}{\\sqrt{2}},0\\right),\\left(0,\\frac{-1}{\\sqrt{2}}\\right),\\left(0,\\frac{1}{\\sqrt{2}}\\right)\\right\\} \\]\n\nNote that those elements are on the diagonal from top left to bottom right expanding through all the tensor products on table 3."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#size-of-smolyak-grids",
    "href": "notebooks/week8/funcapprox.html#size-of-smolyak-grids",
    "title": "Function Approximation",
    "section": "Size of Smolyak Grids",
    "text": "Size of Smolyak Grids\n\nThe Smolyak grid grows much slower (at order \\(d\\) to a power of \\(\\mu\\)) than the Tensor grid (exponential growth)\n\n\n\n\n[@maliar-maliar] figure 2: Tensor vs Smolyak in 2D\n\n\n\n\n\n[@maliar-maliar] figure 4: Tensor vs Smolyak in 2D, number of grid points"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#smolyak-polynomials",
    "href": "notebooks/week8/funcapprox.html#smolyak-polynomials",
    "title": "Function Approximation",
    "section": "Smolyak Polynomials",
    "text": "Smolyak Polynomials\n\nCorresponding to the construction of grid points, there is the Smolyak way of constructing polynomials.\nThis works exactly as before. We start with a one-dimensional set of basis functions (again Chebyshev here, again irrelevant): \\[ \\left\\{1,x,2x^2-1,4x^3-3x,8x^4-8x^2+1\\right\\} \\]\nThree nested sets:\n\n\\[ \\begin{array}{l}\n    i=1 : S_1 = \\{1\\} \\\\\n    i=2 : S_2 = \\{1,x,2x^2-1\\} \\\\\n    i=3 : S_3 = \\left\\{1,x,2x^2-1,4x^3-3x,8x^4-8x^2+1\\right\\}  \\end{array}\n\\]\n\nDenoting \\(\\mathcal{P}^{d,\\mu}\\) the Smolyak polynomial, we follow exactly the same steps as for the grids to select elements of the full tensor product table 5:\n\n\n\n\n[@maliar-maliar] figure 5: All Smolyak Polynomials in 2D"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#smolyak-interpolation",
    "href": "notebooks/week8/funcapprox.html#smolyak-interpolation",
    "title": "Function Approximation",
    "section": "Smolyak Interpolation",
    "text": "Smolyak Interpolation\nThis proceeds as in the previouses cases:\n\nEvaluate \\(f\\) at all grid points \\(\\mathcal{H}^{d,\\mu}\\).\nEvaluate the set of basis functions given by \\(\\mathcal{P}^{d,\\mu}\\) at all grid points \\(\\mathcal{H}^{d,\\mu}\\).\nSolve for the interpolating coefficients by inverting the Basis function matrix."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#extensions",
    "href": "notebooks/week8/funcapprox.html#extensions",
    "title": "Function Approximation",
    "section": "Extensions",
    "text": "Extensions\n\nThere is a lot of redundancy in computing the grids the way we did it.\nMore sophisticated approaches take care not to compute repeated elements."
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#smolyak-grids-in-julia",
    "href": "notebooks/week8/funcapprox.html#smolyak-grids-in-julia",
    "title": "Function Approximation",
    "section": "Smolyak Grids in Julia",
    "text": "Smolyak Grids in Julia\n\nThere are at least 2 julia packages that implement this idea:\n\nhttps://github.com/QuantEcon/BasisMatrices.jl\nhttps://github.com/RJDennis/SmolyakApprox.jl"
  },
  {
    "objectID": "notebooks/week8/funcapprox.html#more-on-sparse-grids",
    "href": "notebooks/week8/funcapprox.html#more-on-sparse-grids",
    "title": "Function Approximation",
    "section": "More on Sparse Grids",
    "text": "More on Sparse Grids\n\nSparse Grids are widely used.\nThe Tasmanian library is excellent.\nThe Tasmanian.jl is a simple wrapper to that library.\nSimon Scheidegger has many useful resources on his website, in particular their joint paper with Johannes Brumm features an application of sparse grids for high dimensional economics problems.\n\nusing Tasmanian\nTasmanian.ex2()\n\nTasmanian.ex3()"
  },
  {
    "objectID": "lectures/plotting.html",
    "href": "lectures/plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "Good plotting is one of the main requirements we have for a fully fledged computing language. We want to be able to quickly make good looking plots of data objects (like for instance the R library ggplot2 allows us to do), and we want to have the ability to build up plots step by step as for instance the python library matplotlib allows us to do. There are several solutions available in julia:\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "Makie.jl Plotting"
    ]
  },
  {
    "objectID": "lectures/plotting.html#backends",
    "href": "lectures/plotting.html#backends",
    "title": "Plotting",
    "section": "Backends",
    "text": "Backends\n\nYou can choose different engines which will draw your plot, each with strengths and weaknesses. Find out more here\nWe will be using CairoMakie which is good for static output (like pdf or png pictures).",
    "crumbs": [
      "Home",
      "Lectures",
      "Makie.jl Plotting"
    ]
  },
  {
    "objectID": "lectures/plotting.html#getting-started-install-the-cairomakie-package",
    "href": "lectures/plotting.html#getting-started-install-the-cairomakie-package",
    "title": "Plotting",
    "section": "Getting Started: Install the CairoMakie package",
    "text": "Getting Started: Install the CairoMakie package\n\nthis is following closely the makie.jl getting started page\n\n\nCreate a folder makie_tutorial on your computer somewhere. Maybe ~/makie_tutorial ?\nStart julia by typing julia on your terminal and hitting enter (or double click the icon in Applications)\nin the REPL, we need to move into the makie_tutorial folder now. do this:\ncd(\"/path/to/your/makie_tutorial\")\nActivate the current folder to be our project:\nusing Pkg\nPkg.activate(\".\")  # \".\" is the name of the current directory ;-)\nAdd the CairoMakie package:\nPkg.add(\"CairoMakie\")  # this will take a while!\nAfter this has finished, check the makie_tutorial folder: there are 2 new files now: Project.toml and Manifest.toml\nWe can now use the CairoMakie package! 🎉",
    "crumbs": [
      "Home",
      "Lectures",
      "Makie.jl Plotting"
    ]
  },
  {
    "objectID": "lectures/plotting.html#first-plot",
    "href": "lectures/plotting.html#first-plot",
    "title": "Plotting",
    "section": "First Plot",
    "text": "First Plot\n\n# this only works after completing the above steps!\nusing CairoMakie\n\nIf executing using CairoMakie worked for you, it means you are good to go. Let’s get some data…\n\n# hover top right to copy this\nseconds = 0:0.1:2\nmeasurements = [8.2, 8.4, 6.3, 9.5, 9.1, 10.5, 8.6, 8.2, 10.5, 8.5, 7.2,\n        8.8, 9.7, 10.8, 12.5, 11.6, 12.1, 12.1, 15.1, 14.7, 13.1];\n\n…and make a lineplot from it:\n\nlines(seconds, measurements)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDepending on your system, lines(seconds, measurements) should have triggered a viewer of some sort. In VSCode, the plotting pane should have opened\n\n\nLet’s do a scatter plot now:\n\nscatter(seconds, measurements)\n\n\n\n\n\n\n\n\nGreat. Now let’s an exponential fit line to this. Suppose we know the formula to be f(x) = exp(x) + 7. That’s just another line plot:\n\nscatter(seconds, exp.(seconds) .+ 7)\n\n\n\n\n\n\n\n\nFinally, we want to have those all in the same plot. We will use the ! version of the plotting functions!\n\nscatter(seconds, measurements)\nlines!(seconds, exp.(seconds) .+ 7)\ncurrent_figure()",
    "crumbs": [
      "Home",
      "Lectures",
      "Makie.jl Plotting"
    ]
  },
  {
    "objectID": "lectures/plotting.html#figures-and-axis",
    "href": "lectures/plotting.html#figures-and-axis",
    "title": "Plotting",
    "section": "Figures and Axis",
    "text": "Figures and Axis\nThere is another way to compose this plot, more useful if you want more fine-grained control. We need a\n\nFigure\nAxis\n\nA plot is part of an Axis object, which itself is part of a Figure.\n\nf = Figure()\nax = Axis(f[1,1]) # ax sits at [1,1] of f\nscatter!(ax, seconds, measurements)  # plot *into* ax\nlines!(ax, seconds, exp.(seconds) .+ 7) # again\nf  # look at f\n\n\n\n\n\n\n\n\nThis is useful if we have many (sub) plots which we want to collect in a single figure, or for precise labelling. Let’s label that figure now:\n\nf = Figure()\nax = Axis(f[1, 1],\n    title = \"Experimental data and exponential fit\",\n    xlabel = \"Time (seconds)\",\n    ylabel = \"Value\",\n)\nscatter!(ax, seconds, measurements)\nlines!(ax, seconds, exp.(seconds) .+ 7)\nf",
    "crumbs": [
      "Home",
      "Lectures",
      "Makie.jl Plotting"
    ]
  },
  {
    "objectID": "lectures/plotting.html#styling-our-plot",
    "href": "lectures/plotting.html#styling-our-plot",
    "title": "Plotting",
    "section": "Styling our Plot",
    "text": "Styling our Plot\nLet’s change a few aestetic aspects of our plot, like color or linestyle:\n\nf = Figure()\nax = Axis(f[1, 1],\n    title = \"Experimental data and exponential fit\",\n    xlabel = \"Time (seconds)\",\n    ylabel = \"Value\",\n)\nscatter!(ax, seconds, measurements, color = :tomato)\nlines!(ax, seconds, exp.(seconds) .+ 7, color = :tomato, linestyle = :dash)\nf\n\n\n\n\n\n\n\n\nThere are many aspects of any plotting type that you can style. Checkout the reference for the scatter function here. For example, we could change the marker symbol from the default (:circle:) to something funnier:\n\n# notice: overplots the `ax` in `f` from above!\nscatter!(ax, seconds, measurements, marker = '😉', \n    markersize = 20)\nf\n\n\n\n\n\n\n\n\nFinally, let us add a legend in a certain position:\n\nf = Figure()\nax = Axis(f[1, 1],\n    title = \"Experimental data and exponential fit\",\n    xlabel = \"Time (seconds)\",\n    ylabel = \"Value\",\n)\nscatter!(\n    ax,\n    seconds,\n    measurements,\n    color = :tomato,\n    label = \"Measurements\"\n)\nlines!(\n    ax,\n    seconds,\n    exp.(seconds) .+ 7,\n    color = :tomato,\n    linestyle = :dash,\n    label = L\"f(x) = \\exp(x) + 7\",\n)\naxislegend(position = :rb)\nf",
    "crumbs": [
      "Home",
      "Lectures",
      "Makie.jl Plotting"
    ]
  },
  {
    "objectID": "lectures/plotting.html#saving-plot-to-disk",
    "href": "lectures/plotting.html#saving-plot-to-disk",
    "title": "Plotting",
    "section": "Saving Plot to Disk",
    "text": "Saving Plot to Disk\nFinally, it’s straightforward to save our plot to disk in a variety of formats:\n\nsave(\"first_figure.png\", f)\nsave(\"first_figure.svg\", f)\nsave(\"first_figure.pdf\", f)\n\nThose files are now in your makie_tutorial folder!",
    "crumbs": [
      "Home",
      "Lectures",
      "Makie.jl Plotting"
    ]
  },
  {
    "objectID": "lectures/lecture5.html",
    "href": "lectures/lecture5.html",
    "title": "Lecture 5: Dynamic Programming",
    "section": "",
    "text": "We introduced the Dynamic Programming technique by means of the well known cake eating problem. We showed probably the simplest way to solve for the optimal decision rule, on a discrete grid. We also showed that this can lead to really poor results, so implementation details matter here. We introduced uncertainty and we looked at an Euler Equation solver for a consumption savings problem.\n\n\n\nTopic\nNotebook\n\n\n\n\nDynamic Programming 1\ndownload notebook\n\n\nDynamic Programming 2\ndownload notebook\n\n\nEndogenous Grid Method (EGM)\ndownload notebook\n\n\nThe Shimer-Smith Model\ndownload notebook\n\n\nThe Ayiagari Model\ndownload notebook\n\n\n\n\n\n\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "5 - Dynamic Programming"
    ]
  },
  {
    "objectID": "lectures/lecture6.html",
    "href": "lectures/lecture6.html",
    "title": "Lecture 6: Data",
    "section": "",
    "text": "Data in julia means: DataFrames.jl and friends.\nNew users are adivsed to start with the new users section in the manual. I advise you to go through that.\nIn class, instead, we will take the deep dive into the DataFrames.jl package with a showcase.\nThere is a notebook for you here\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "6 - Data"
    ]
  },
  {
    "objectID": "lectures/lecture6.html#important-packages-for-data",
    "href": "lectures/lecture6.html#important-packages-for-data",
    "title": "Lecture 6: Data",
    "section": "Important Packages for data",
    "text": "Important Packages for data\n\nDataFrames.jl\nDataFramesMeta.jl\nChain.jl\nChevrons",
    "crumbs": [
      "Home",
      "Lectures",
      "6 - Data"
    ]
  },
  {
    "objectID": "lectures/lecture6.html#notebooks",
    "href": "lectures/lecture6.html#notebooks",
    "title": "Lecture 6: Data",
    "section": "Notebooks",
    "text": "Notebooks\n\nDataFrames Showcase here\nEcon-inspired showcase here. Notice you need to also download the data in the same directory.",
    "crumbs": [
      "Home",
      "Lectures",
      "6 - Data"
    ]
  },
  {
    "objectID": "lectures/lecture6.html#further-dataframes.jl-resources",
    "href": "lectures/lecture6.html#further-dataframes.jl-resources",
    "title": "Lecture 6: Data",
    "section": "Further DataFrames.jl Resources",
    "text": "Further DataFrames.jl Resources\n\nThe Manual is very well done, and lists a bunch of good resources on the landing page.\nThe ultimate database-ops benchmark\nBogumił’s Blog\nThis tutorial series is very good for the DataFrames.jl package: https://github.com/bkamins/Julia-DataFrames-Tutorial\nThe julia academy data science tutorial is full of excellent stuff\nThe Turing Institute’s Data Sciences tutorials give a good intro to data structures and machine learning in julia.",
    "crumbs": [
      "Home",
      "Lectures",
      "6 - Data"
    ]
  },
  {
    "objectID": "lectures/lecture2.html",
    "href": "lectures/lecture2.html",
    "title": "Week 2 - Examples",
    "section": "",
    "text": "In this section we present some simple usage examples of julia in the wild. We will again have a few notebooks to go through.\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "2 - Examples"
    ]
  },
  {
    "objectID": "lectures/lecture2.html#notebooks",
    "href": "lectures/lecture2.html#notebooks",
    "title": "Week 2 - Examples",
    "section": "Notebooks",
    "text": "Notebooks\n\n\n\nTopic\nrendered Notebook\nsource of notebook\n\n\n\n\nDefining Types\nclick\nclick\n\n\nDifferential Equations\nclick\nclick\n\n\nArrays\nclick\nget source from rendered",
    "crumbs": [
      "Home",
      "Lectures",
      "2 - Examples"
    ]
  },
  {
    "objectID": "lectures/lecture1.html",
    "href": "lectures/lecture1.html",
    "title": "Programming Languages and - Why Julia?",
    "section": "",
    "text": "TipInstallation\n\n\n\nMake sure to check out the installation page on this website before you come to class.\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "1 - Why `julia`?"
    ]
  },
  {
    "objectID": "lectures/lecture1.html#basics-repl-vscode-notebooks",
    "href": "lectures/lecture1.html#basics-repl-vscode-notebooks",
    "title": "Programming Languages and - Why Julia?",
    "section": "Basics: REPL, VSCode, Notebooks",
    "text": "Basics: REPL, VSCode, Notebooks\nWe will repeat the basics of julia in a very condensed manner by going through steps 1-7 of MoJuWo live in class.\njulia REPL:\n\n\n\nThis is the julia REPL\n\n\nVScode:\n\n\n\nThis is VSCode\n\n\nNotice how there is a julia REPL running as part of the VSCode editor. This is called an integrated development environment (IDE). Very similar to an R session connected to RStudio, or the python interpreter being run through PyCharm etc.",
    "crumbs": [
      "Home",
      "Lectures",
      "1 - Why `julia`?"
    ]
  },
  {
    "objectID": "lectures/lecture1.html#introduction-and-why-julia",
    "href": "lectures/lecture1.html#introduction-and-why-julia",
    "title": "Programming Languages and - Why Julia?",
    "section": "Introduction and: Why Julia?",
    "text": "Introduction and: Why Julia?\nIn this section we want to understand why julia is a good choice for computational tasks - for Economists and others.\nSlides for intro: Why Julia? (does not run in my safari browser! 🤷🏻‍♂️)",
    "crumbs": [
      "Home",
      "Lectures",
      "1 - Why `julia`?"
    ]
  },
  {
    "objectID": "lectures/lecture1.html#notebooks",
    "href": "lectures/lecture1.html#notebooks",
    "title": "Programming Languages and - Why Julia?",
    "section": "Notebooks",
    "text": "Notebooks\nWe use several Pluto.jl notebooks in this lecture. Here are four rendered ones:\n\n\n\nTopic\nrendered Notebook\nsource of notebook\n\n\n\n\nVariables\nclick\nclick\n\n\nFunctions\nclick\nclick\n\n\n\nPlease download and run the 00-fast.jl notebook on your own computer for the benchmarks! (Right click and copy link, then insert into a Pluto browser!)🏎️\n\nResources\n\nModern Julia Workflow\nStartHere.jl\nAruoba and Fernandez-Villaverde\nJesus’ Julia Tutorial\nChapter 1 of Julia for Data Analysis",
    "crumbs": [
      "Home",
      "Lectures",
      "1 - Why `julia`?"
    ]
  },
  {
    "objectID": "lectures/lecture8.html",
    "href": "lectures/lecture8.html",
    "title": "Function Approximation",
    "section": "",
    "text": "NoteProblem statement\n\n\n\nSuppose you have this problem to solve.\n\\[ \\begin{align}\nV(a,z) &= \\max_{a'} u((1+r)a + z - a') + \\beta \\mathbb{E}\\left[ V(a',z') | z \\right]\\\\\n\\end{align} \\]\n\nSuppose you know the values of \\(V(a_i,z_j)\\), but only on two discrete grids \\((\\mathcal{A}, \\mathcal{Z})\\).\nIn order to perform the \\(\\max_{a'}\\) operation, you need to evaluate \\(V(a',z_j)\\), where \\(a' \\notin \\mathcal{A}\\). How do you go about this task?\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "8 - Function Approximation"
    ]
  },
  {
    "objectID": "lectures/lecture8.html#some-taxonomy",
    "href": "lectures/lecture8.html#some-taxonomy",
    "title": "Function Approximation",
    "section": "Some Taxonomy",
    "text": "Some Taxonomy\n\nLocal Approximations: approximate function and it’s derivative \\(f,f'\\) at a single point \\(x_0\\). Taylor Series: \\[ f(x) = f(x_0) + (x-x_0)f'(x_0) + \\frac{(x-x_0)^2}{2}f''(x_0) + \\dots + \\frac{(x-x_0)^n}{n!}f^{n}(x_0) \\]\nInterpolation or Colocation: find a function \\(\\hat{f}\\) that is a good fit to \\(f\\), and require that \\(\\hat{f}\\) passes through the points. If we think of there being a residual \\(\\epsilon_i = f(x_i) - \\hat{f}(x_i)\\) at each grid point \\(i\\), this methods succeeds in setting \\(\\epsilon_i=0,\\forall i\\).\nRegression: Minimize some notion of distance (or squared distance) between \\(\\hat{f}\\) and \\(f\\), without the requirement of pass through.\n\n\nDoing Interpolation in Julia\n\nIn practice, you will make heavy use of high-quality interpolation packages in julia.\nList in the end.\nNevertheless, function approximation is extremely problem-specific, so sometimes a certain approach does not work for your problem.\nThis is why we will go through the mechanics of some common methods.\nI would like you to know where to start drilling if you need to go and hack somebody elses code.",
    "crumbs": [
      "Home",
      "Lectures",
      "8 - Function Approximation"
    ]
  },
  {
    "objectID": "lectures/lecture8.html#representing-data",
    "href": "lectures/lecture8.html#representing-data",
    "title": "Function Approximation",
    "section": "Representing Data",
    "text": "Representing Data\nLet’s have a look at the famous Iris Dataset.\n\n\nusing CSV\nusing DataFrames\nusing StatsPlots\nusing Chain\nusing Statistics\n\niris = CSV.read(\n    (joinpath(dirname(pathof(DataFrames)),\n     \"..\", \"docs\", \"src\", \"assets\", \"iris.csv\")),\n    DataFrame)\nfirst(iris,5)\n\n5×5 DataFrame\n\n\n\nRow\nSepalLength\nSepalWidth\nPetalLength\nPetalWidth\nSpecies\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nString15\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\nThis dataset is like a matrix.\nWe could think of each row as a vector. A tuple of input data, like \\(x_1 = (5.1,3.5,1.4,0.2)\\), and a one-dimensional ouput \\(y_1\\), the species label, like “setosa”. Stack the \\(x\\) to get \\(X\\) of size (150,4).\n\nHow are the different measures distributed?\n\n@chain iris begin\n    stack(_, Not(:Species))\n    subset(:Species =&gt; x -&gt; x .== \"Iris-setosa\") \n    @df density(:value, group = :variable, xlab = \"Setosa Measures\",lw = 2)\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThose measurements seem to be on different scales. The SepalLength measure seems to have an average of around 5, where as PetalWidth is smaller than 1 in most cases.\n\nHow can we do mathematical operations on vectors \\(x\\) like addition and scalar multiplication?\nLike, suppose we want to standardize the data in iris? We would like to create those two objects (each of them a length 4 vector), standing for the mean and std deviation (across rows) of all 150 samples.\n\n\\[\\begin{align}\n\\mu & = \\frac{1}{150} \\sum_{i=1}^{150} x_i \\in \\mathbb{R}^4 \\\\\n\\sigma & = \\sqrt{\\frac{1}{150} \\sum_{i=1}^{150}\n\\left(x_i - \\mu_i \\right)^2} \\in \\mathbb{R}^4 \\\\\n\\end{align}\\]\n\n(the interesting part is the \\(\\sum_{i=1}^{150} x_i\\) when \\(x_i \\in \\mathbb{R}^4\\) )\nin order to then standardize each vector in turn like that (here \\(x\\) is a 4-vector!):\n\n\\[\\frac{x_1 - \\mu}{\\sigma},\\frac{x_2 - \\mu}{\\sigma},\\dots,\\frac{x_{150} - \\mu}{\\sigma}\\]\n\n@chain iris begin\n    subset(:Species =&gt; x -&gt; x .== \"Iris-setosa\") \n    transform( [x =&gt; (z -&gt; (z .- mean(z)) ./ std(z)) =&gt; x \n         for x in [:SepalLength, :SepalWidth, :PetalLength, :PetalWidth] ]\n    )\n    stack(_, Not(:Species))\n    @df density(:value, group = :variable, xlab = \"Standardized Setosa Measures\")\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we have done right now, i.e. to transform this data via addition and multiplication into a common space is very important. In fact, it is related to the concept of Vector Spaces.",
    "crumbs": [
      "Home",
      "Lectures",
      "8 - Function Approximation"
    ]
  },
  {
    "objectID": "lectures/lecture8.html#vector-spaces",
    "href": "lectures/lecture8.html#vector-spaces",
    "title": "Function Approximation",
    "section": "Vector Spaces",
    "text": "Vector Spaces\nA Vector Space is a structure \\((V,F,+,\\cdot)\\), where:\n\n\\(V\\) is a set of vectors\n\\(+: V \\times V \\mapsto V\\) is the addition operation, with\n\n\\(x + y = y + x\\)\n\n\\(x + (y + z ) = (x + y ) + z\\)\n\nThere is a null vector: \\(x + 0 = x, 0 \\in V\\)\n\nthere is an inverse \\(-x \\in V, x + (-x) = 0\\)\n\n\\(F\\) is a field of scalars: \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)\n\\(\\cdot : F \\times V \\mapsto V\\) is scalar multiplication, satisfying\n\n\\(a(bx) = (ab) x\\)\n\\(a(x + y) = ax + ay\\)\n\\(1x = x\\)\n\n\n\nExamples of Vector Spaces\n\n\\((\\mathbb{R}^n,\\mathbb{R},+,\\cdot)\\): the most widely encountered vector space - particularly with \\(n=2\\).\n\\(\\mathbb{R}[x] = \\left\\{\\sum_{i=0}^n p_i x^i : p_i \\in \\mathbb{R}, n = 0,1,\\dots \\right\\}\\): We can add 2 polynomials: \\[(p + q)(x) = p(x) + q(x),\\] and also multiply with a scalar \\[(cp)(x) = c p(x)\\] which makes \\((\\mathbb{R}[x],\\mathbb{R},+,\\cdot)\\) a valid vector space.\n\n\n\nA Linear Basis\n\n\\(V\\) could contain infinitely many vectors.\nWe can reduce this complexity by finding a set of vectors from which to easily generate the other vectors.\n\n\\[ \\begin{align}\ne_1 &= (1,0,\\dots,0)\\\\\ne_2 &= (0,1,\\dots,0)\\\\\n& \\vdots\\\\\ne_n &= (0,0,\\dots,1)\n\\end{align} \\]\nwith those \\(E = (e_1,e_2,\\dots, e_n)\\), we can represent any \\(x \\in V\\) as\n\\[x = \\sum_{i=1}^n e_i x_i, x_i \\in \\mathbb{R}, e_i \\in \\mathbb{R}^n\\]\n\n\\(E\\) is a vector space basis. With addition and scalar multiplication defined, we can create any vector \\(x\\in V\\), i.e. we can span the entire vector space.\n\\(E\\) is like a skeleton for \\(V\\) (\\(\\mathbb{R}^n\\) in this example).\n\n\n\nLinear Combinations\n\nIf instead of \\(e_i\\) we write \\(v_i\\) for any vector, and \\(b_i \\in \\mathbb{R}\\) a weight , we can write any other vector\\[z = \\sum_{i=1}^n v_i b_i, v_i \\in \\mathbb{R}^n\\] as a linear combination\nPotentially, there are multiple combinations of vectors which result in the same combination, e.g. for \\(v_1 = (1,0),v_2 = (0,1), v_3= (1,1)\\), we can have \\[(2,1) = 2 v_1 + v_2 = v_1 + v_3\\] i.e. there are 2 equivalent ways to generate \\((2,1)\\) from those vectors.\nThe set \\(S = \\{v_1 , v_2 , v_3 \\}\\) contains redundant information.\n\n\n\n\n\n\n\nTipLinear Independence\n\n\n\nLet \\(V\\) be a vector space and \\(S=\\{v_1,\\dots,v_n\\}\\) be a subset of its vectors. \\(S\\) is said to be linearly dependent if it only contains the null vector, or if there is a nonzero \\(v_k\\) which can be expressed as a linear combination of the others \\[v_1,\\dots,v_{k-1},v_{k+1},\\dots,v_n\\]\nIf \\(S\\) is not linearly dependent, it is linearly independent.\n\n\nConsider \\(v_k\\), which we write as a linear combination of the other members of \\(S\\):\n\\[v_k = \\sum_{i=1}^{k-1} b_i v_i + \\sum_{i=k+1}^n b_i v_i\\] for nonzero \\(v_k\\). We already say \\(S\\) is linearly dependent with this. But look:\nSubstracting \\(v_k\\) on both sides, we get the null vector:\n\\[0 = \\sum_{i=1}^{n} b_i v_i\\]\nfor some weights \\(b_i\\), where it has to be that \\(b_k =-1\\). So: we can obtain the null vector from a nontrivial linear combination.\n\n\n\n\n\n\nNoteTheorem\n\n\n\nLet \\(V\\) be a vector space and \\(S\\) be a subset of it’s vectors.\n\n\\(S\\) is linearly dependent if and only if the null vector can be generated as a nontrivial linear combination (i.e. a linear combination where not all coefficients are equal to zero)\n\\(S\\) is linearly independent if and only if it is true that \\(0 = \\sum_{i=1}^n x_i v_i\\), then all coefficients \\(x_i = 0\\).\n\n\n\n\n\nBasis Definition\nLet \\(V\\) be a vector space and \\(S\\) be a subset of it’s vectors. \\(S\\) is called basis of \\(V\\) if\n\n\\(S\\) is linearly independent\n\\(S\\) spans \\(V\\), i.e. \\(span(S) = V\\)\n\nWe obtain the span of a set of vectors \\(S\\) by creating all possible linear combinations amongst them. For example, the span of two linearly independent vectors is the plane.\n\nThe elements of a basis set are called basis vectors.\n\n\n\nBasis Example in R2\n\nThe set \\(\\mathbb{R}^2\\) of ordered pairs of real numbers is a vector space \\(V\\) for\n\ncomponent-wise addition: \\((a,b) + (c,d) = (a+c, b+d)\\)\nscalar multiplication: \\(\\lambda (a,b) = (\\lambda a, \\lambda b), \\lambda \\in \\mathbb{R}\\).\n\nOne basis called the standard basis of \\(V\\) consists of two vectors:\n\n\\(e_1 = (1,0)\\)\n\\(e_2 = (0,1)\\)\n\nAny vector \\(v=(a,b) \\in \\mathbb{R}^2\\) can be uniquely written as \\(v = ae_1 + be_2\\)\nAny other pair of linearly independent vectors like \\((1,1)\\) and \\((-1,2)\\) is also a basis of \\(\\mathbb{R}^2\\).\n\n\n\nWhat is a Basis function?\n\nNow we know that every vector \\(\\mathbf{v}\\) in a vector space \\(V\\) can be represented by a linear combination of basis vectors.\nSimilarly, we can represent every continuous function in a particular function space \\(F\\) by a linear combination of basis functions.\n\n\n\n\n\n\n\nNoteBlending Functions\n\n\n\nAnother good name for basis functions is blending functions. We often use a mixture of several basis functions to find an approximation. We will use this machinery now to see how we can approximate function from several, simple, component functions (which will be basis functions).",
    "crumbs": [
      "Home",
      "Lectures",
      "8 - Function Approximation"
    ]
  },
  {
    "objectID": "lectures/lecture8.html#what-polynomial-to-use-what-form-for-phi",
    "href": "lectures/lecture8.html#what-polynomial-to-use-what-form-for-phi",
    "title": "Function Approximation",
    "section": "What Polynomial to use? What form for \\(\\Phi\\)?",
    "text": "What Polynomial to use? What form for \\(\\Phi\\)?\n\nIn principle the monomial basis could be used. It is just the power functions of \\(x\\): \\(1,x,x^2,x^3,\\dots\\)\nStacking this up for each evaluation node gives the Vandermonde Matrix:\n\n\\[\nV = \\left[\\begin{matrix}\n        1 & x_1 & \\dots & x_1^{n-2} & x_1^{n-1} \\\\\n        1 & x_2 & \\dots & x_2^{n-2} & x_2^{n-1} \\\\\n        \\vdots & \\vdots & \\ddots &  & \\vdots \\\\\n        1 & x_m & \\dots & x_m^{n-2} & x_m^{n-1}\n        \\end{matrix} \\right]\n\\]\nfor the case with \\(m\\) evaluation nodes for \\(x\\), and \\(n\\) basis functions for each \\(x_i\\).\n\nChoosing Basis Matrices\n\nIf we choose \\(\\Phi\\) so that it’s elements would share a lot of information, it will be hard to invert it. Basically multicollinearity.\nSo, if the elements share very little information, it’s easier to invert, hence good for us\nFor example, if we have \\(\\Phi\\) close to diagonal (i.e. different basis \\(\\phi_j(x)\\) are non-zero at different \\(x\\)).\nSuch \\(\\Phi\\) are generated by using orthogonal Polynomials (under our inner product definition).\nAn orthogonal Polynomial has 2 sequences of polynomials orthogonal under an inner product: their inner product is zero under a certain weighting function \\(w(x)\\).\nFor example, the Chebyshev polynomials are orthogonal wrt. to weight \\(\\frac{1}{\\sqrt{1-x^2}}\\)\nfor degrees \\(n,m\\) of that polynomial, \\(T_n,T_m\\), we have \\[\n  \\int_{-1}^1 T_n(x)T_m(x)\\,\\frac{dx}{\\sqrt{1-x^2}}=\n  \\begin{cases}\n      0 & n\\ne m \\\\ \\pi & n=m=0 \\\\ \\frac{\\pi}{2} & n=m\\ne 0\n  \\end{cases}\n  \\]\n\n\n\nChebyshev Polynomials of the first kind\n\nThere is a nice recursion to get \\(T_n(x)\\): \\[\\begin{align}\n      T_0(x) & = 1 \\\\\n      T_1(x) & = x \\\\\n      T_{n+1}(x) & = 2xT_n(x) - T_{n-1}(x).\n  \\end{align}\\]\nCode that up and compute \\(T_{3}(0.5)\\)!\nCheck you get the same as with the alternative closed form \\(T_n(x) = \\cos(n \\arccos(x))\\)\n\n\n\nCode\nfunction T(x,n)\n    @assert (x &gt;= -1) & (x &lt;= 1)\n    if n == 0\n        return 1.0\n    elseif n==1\n        return x\n    else\n        2*x*T(x,n-1) - T(x,n-2)\n    end\nend\nT2(x,n) = cos(n* acos(x))\n@show T(0.5,3)\nusing Test\n@assert T2(0.5,3) == T(0.5,3)\n\n\nT(0.5, 3) = -1.0\n\n\nOk let’s check this orthogonality claim. The formula says that if \\(n \\neq m\\), then integrating the product of two polynomials is exactly zero:\n\n# weighting function\nw(x) = 1.0 / sqrt(1-x^2)\nusing Statistics\nusing Test\n\n# simple integration works for n not equal m\n@show sum(T(x,0) * T(x,1) * w(x) for x in range(-0.99,stop = 0.99, length=100))\n@show sum(T(x,4) * T(x,5) * w(x) for x in range(-0.99,stop = 0.99, length=100))\n\nsum((T(x, 0) * T(x, 1) * w(x) for x = range(-0.99, stop = 0.99, length = 100))) = -1.7763568394002505e-15\nsum((T(x, 4) * T(x, 5) * w(x) for x = range(-0.99, stop = 0.99, length = 100))) = -4.440892098500626e-15\n\n\n-4.440892098500626e-15\n\n\n\n\nChebyshev Nodes\n\nChebyshev Nodes are the roots of the chebyshev polynomial (i.e. where \\(P(x) = 0\\)).\nThey are defined in the interval \\([-1,1]\\) as \\[ x_i = \\cos\\left(\\frac{2k-1}{2n} \\pi\\right), k=1,\\dots,n \\]\nWhich maps to general interval \\([a,b]\\) as \\[ x_i = \\frac{1}{2} (a+b) + \\frac{1}{2} (b-a) \\cos\\left(\\frac{2k-1}{2n} \\pi\\right) , k=1,\\dots,n \\]\nChebyshev nodes are not evenly spaced: there are more points towards the boundaries. You might remember that from Gaussian Integration nodes.\n\n\nusing FastGaussQuadrature\ngcnodes = gausschebyshev(11)  # generate 11 Chebyshev Nodes\nscatter(gcnodes,ones(11),ylims=(0.9,1.1))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstructing \\(\\Phi\\) as \\(T\\) evaluated at the Chebyshev Nodes\n\nCombining Chebyshev nodes evaluated at the roots \\(T\\) of the Cheby polynomial to construct \\(\\Phi\\) is a particularly good idea.\nDoing so, we obtain an interpolation matrix \\(\\Phi\\) with typical element \\[ \\phi_{ij} = \\cos\\left( \\frac{(n-i+0.5)(j-1)\\pi}{n}\\right)  \\]\nAnd we obtain that \\(\\Phi\\) is indeed orthogonal \\[ \\Phi^T \\Phi = \\text{diag}\\{n,n/2,n/2,\\dots,n/2\\}  \\]\n\n\nusing BasisMatrices\nx = range(-4, stop = 4 ,length = 10)\nϕ = Basis(ChebParams(length(x),-4,4))\nϕ\n\n1 dimensional Basis on the hypercube formed by (-4.0,) × (4.0,).\nBasis families are Cheb\n\n\n\nS, y = nodes(ϕ)\nΦ = BasisMatrix(ϕ, Expanded(), S, 0)\nΦ.vals[1]' * Φ.vals[1]\n\n10×10 Matrix{Float64}:\n 10.0          -1.11022e-16  -2.22045e-16  …  -2.10942e-15  -5.32907e-15\n -1.11022e-16   5.0          -1.41698e-15     -4.571e-15    -2.46571e-15\n -2.22045e-16  -1.41698e-15   5.0             -2.33515e-15  -5.35038e-15\n -1.88738e-15  -5.05353e-16  -1.91117e-15     -4.6033e-15   -2.2613e-15\n -1.22125e-15  -2.38199e-15  -1.46093e-15     -2.66302e-15  -4.90537e-15\n -2.77556e-15  -1.43304e-15  -2.71577e-15  …  -4.67867e-15  -2.25549e-15\n -1.66533e-15  -3.38498e-15  -2.09657e-15     -2.23671e-15  -4.98201e-15\n -3.88578e-15  -1.76844e-15  -3.9379e-15      -4.87009e-15  -2.42363e-15\n -2.10942e-15  -4.571e-15    -2.33515e-15      5.0          -4.90931e-15\n -5.32907e-15  -2.46571e-15  -5.35038e-15     -4.90931e-15   5.0\n\n\n\n\n(Chebyshev) Interpolation Proceedure\n\nLet’s summarize this proceedure.\nInstead of Chebyshev polynomials we could be using any other suitable family of polynomials.\nTo obtain a Polynomial interpolant \\(\\hat{f}\\), we need:\n\na function to \\(f\\) interpolate. We need to be able to get the function values somehow.\nA set of (Chebyshev) interpolation nodes at which to compute \\(f\\)\nAn interpolation matrix \\(\\Phi\\) that corresponds to the nodes we have chosen.\nA resulting coefficient vector \\(c\\)\n\nTo obtain the value of the interpolation at \\(x'\\) off our grid, we also need a way to evaluate \\(\\Phi(x')\\).\n\nEvaluate the Basis function \\(\\Phi\\) at \\(x'\\): get \\(\\Phi(x')\\)\nobtain new values as \\(y = \\Phi(x') c\\).\n\n\n\n\nPolynomial Interpolation with Julia: ApproxFun.jl\n\nApproxFun.jl is a Julia package based on the Matlab package chebfun. It is quite amazing.\nMore than just function approximation. This is a toolbox to actually work with functions.\ngiven 2 functions \\(f,g\\), we can do algebra with them, i.e. \\(h(x) = f(x) + g(x)^2\\)\nWe can differentiate and integrate\nSolve ODE’s and PDE’s\nrepresent periodic functions\nHead over to the website and look at the readme.\n\n\nusing LinearAlgebra, SpecialFunctions, Plots, ApproxFun\nx = Fun(identity,0..10)\nf = sin(x^2)\ng = cos(x)\n\nh = f + g^2\nr = roots(h)\nrp = roots(h')\n\nplot(h,labels = \"h\")\nscatter!(r,h.(r),labels=\"roots h\")\nscatter!(rp,h.(rp),labels = \"roots h'\")\n\nWARNING: using ApproxFun.transform in module Notebook conflicts with an existing identifier.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# whats the first deriv of that function at at 0.785?\nprintln(f'(0.785))\n\n1.281223714282486",
    "crumbs": [
      "Home",
      "Lectures",
      "8 - Function Approximation"
    ]
  },
  {
    "objectID": "lectures/lecture8.html#interpolation-with-interpolations.jl",
    "href": "lectures/lecture8.html#interpolation-with-interpolations.jl",
    "title": "Function Approximation",
    "section": "Interpolation with Interpolations.jl",
    "text": "Interpolation with Interpolations.jl\n\nInterpolations.jl assumes that data is uniformly spaced on grid 1:N\nSame for multiple dimensions\nHowever, we can scale 1:N to different domains\nfinally, we can also supply our own, non-uniform grids.\nCan get gradients of interpolations right away.\n\n\nusing Interpolations\nimport Interpolations: interpolate as itper\nA = rand(10,5)\nitp = itper(A, Interpolations.BSpline(Quadratic(Reflect(OnCell()))))\nitp(1.9,4.9)\n\nWARNING: using Interpolations.knots in module Notebook conflicts with an existing identifier.\n\n\n0.3613597951472\n\n\n\nA_x = 1.:2.:40.   # x in [1,40]\nA = [log(x) for x in A_x] # f(x)\nitp = itper(A, Interpolations.BSpline(Cubic(Interpolations.Line(OnGrid()))))\nsitp = scale(itp, A_x)  # scale x-axis of interpolator\n@show sitp(3.) # exactly log(3.)\n@show sitp(3.5) # approximately log(3.5)\n@show itp(3);  # is the 3rd index(!) in A, not the *value* 3\n\nsitp(3.0) = 1.0986122886681098\nsitp(3.5) = 1.2748716241925298\nitp(3) = 1.6094379124341\n\n\nsame for 2D\n\nA_x1 = 1:.1:10\nA_x2 = 1:.5:20\nfff(x1, x2) = log(x1+x2)\nA = [fff(x1,x2) for x1 in A_x1, x2 in A_x2]\nitp = itper(A, Interpolations.BSpline(Cubic(Interpolations.Line(OnGrid()))))\nsitp = scale(itp, A_x1, A_x2)\nsitp(5., 10.) # exactly log(5 + 10)\nsitp(5.6, 7.1) # approximately log(5.6 + 7.1)\n\n2.541602006923902\n\n\n\nVery often we need a gridded interpolation. I.e. we supply the function values on an irregular grid.\nFor this occasion, the GriddedInterpolation type is useful.\nFor now this only works in 3 modes:\n\nGridded(Linear())\nGridded(Constant()) nearest neighbor\nNoInterp (you must supply index ON grid)\n\n\nA = rand(20)\nA_x = collect(1.0:2.0:40.0)\nknots = (A_x,)\nitp = itper(knots, A, Gridded(Linear()))\nitp(2.0)\n# 2D\nA = rand(8,20)\nknots = ([x^2 for x = 1:8], [0.2y for y = 1:20])\nitp = itper(knots, A, Gridded(Linear()))\nitp(4,1.2)  # approximately A[2,6]\n# we can mix modes across dimensions!\nitp = itper(knots, A, (Gridded(Linear()),Gridded(Constant())))\nitp(4,1.2)\n\nWhat about vector valued interpolations?\nSuppose we have a function \\(f : \\mathbb{R} \\mapsto \\mathbb{R}^2\\)\nEconomics example:\n\n\\[  \nf(x) = \\left(\\begin{array}{c} \\text{savings}(x) \\\\ \\text{consumption}(x) \\end{array} \\right)\n\\]\n\n\\(x\\) is cash on hand.\nWe often have situations where several functions are defined on a common support \\(x\\).\nwhat is \\(f(1.75)\\)?\n\nusing StaticArrays\nx = range(1,stop = 3, length = 200)  # cash on hand\na = Float64[log(1+j)*i for j in x, i in 1:2]   # cons and save function\nb = reinterpret(SVector{2,Float64}, a')[:]\nitp = itper(b, Interpolations.BSpline(Quadratic(Reflect(OnCell()))))\n@show itp(3)\nsitp = scale(itp,x)\n@show sitp(3);\nv = sitp(1.75)  # get interpolated values for both function\nplot(x,a,labels=[\"save\" \"cons\"],yticks=convert(Array{Float64},v),xlabel=\"current cash\",lw = 3)\nvline!([1.75],lab=\"\")\nhline!([v[1],v[2]],lab=\"\")\n\nThe CompEcon Toolbox of Miranda and Fackler\n\nanother good alternative:\nCompEcon.jl",
    "crumbs": [
      "Home",
      "Lectures",
      "8 - Function Approximation"
    ]
  },
  {
    "objectID": "lectures/lecture8.html#multidimensional-approximation",
    "href": "lectures/lecture8.html#multidimensional-approximation",
    "title": "Function Approximation",
    "section": "Multidimensional Approximation",
    "text": "Multidimensional Approximation\n\nUp to now, most of what we did was in one dimesion.\nEconomic problems often have more dimension than that.\n\nThe number of state variables in your value functions are the number of dimensions.\n\nWe can readily extend what we learned into more dimensions.\nHowever, we will quickly run into feasibility problems: hello curse of dimensionality.\n\n\nTensor Product of univariate Basis Functions: Product Rule\n\nOne possibility is to approximate e.g. the 2D function \\(f(x,y)\\) by \\[ \\hat{f}(x,y) = \\sum_{i=1}^n \\sum_{j=1}^m c_{i,j} \\phi_i^x(x) \\phi_j^y(y)  \\]\n\nhere \\(\\phi_i^x\\) is the basis function in \\(x\\) space,\nyou can see that the coefficient vector \\(c_{i,j}\\) is indexed in two dimensions now.\nNotice that our initial notation was general enough to encompass this case, as we defined the basis functions as \\(\\mathbb{R}^d \\mapsto \\mathbb{R}\\). So with the product rule, this mapping is just given by \\(\\phi_i^x(x) \\phi_j^y(y)\\).\n\nThis formulation requires that we take the product of \\(\\phi_i^x(x), \\phi_j^y(y)\\) at all combinations of their indices, as is clear from the summations.\nThis is equivalent to the tensor product between \\(\\phi_i^x\\) and \\(\\phi_j^y\\).\n\n\n\nComputing Coefficients from Tensor Product Spaces\n\nExtending this into \\(D\\) dimensions, where in each dim \\(i\\) we have \\(n_i\\) basis functions, we get \\[ \\hat{f}(x_1,x_2,\\dots,x_D) = \\sum_{i_1=1}^{n_1} \\sum_{i_2=1}^{n_2} \\dots  \\sum_{i_D=1}^{n_D} c_{i_1,i_2,\\dots,i_D} \\phi_{i_1}(x_1) \\phi_{i_2}(x_2) \\dots \\phi_{i_D}(x_D)  \\]\nIn Vector notation \\[\n  \\hat{f}(x_1,x_2,\\dots,x_D) =  \\left[ \\phi_{D}(x_D) \\otimes \\phi_{D-1}(x_{D-1})  \\otimes \\dots  \\otimes  \\phi_{1}(x_1) \\right]  c \\] where \\(c\\) is is an \\(n=\\Pi_{i=1}^D n_i\\) column vector\nThe solution is the interpolation equation as before, \\[ \\begin{aligned}\\Phi c =& y \\\\\n                  \\Phi   =& \\Phi_D \\otimes \\Phi_{D-1} \\otimes \\dots \\otimes \\Phi_{1} \\end{aligned} \\]\n\n\n\nThe Problem with Tensor Product of univariate Basis Functions\n\nWhat’s the problem?\nWell, solving \\(\\Phi c = y\\) is hard.\nIf we have as many evaluation points as basis functions in each dimension, i.e. if each single \\(\\Phi_i\\) is a square matrix, \\(\\Phi\\) is of size (n,n).\nInverting this is extremely hard even for moderately sized problems.\nSometimes it’s not even possible to allocate \\(\\Phi\\) in memory.\nHere it’s important to remember the sparsity structure of a spline basis function.\n\nusing ApproXD\ngr()\nbs = ApproXD.BSpline(7,3,0,1) #7 knots, degree 3 in [0,1]\nn = 500\neval_points = collect(range(0,stop = 1.0,length = n))\nB = Array(ApproXD.getBasis(eval_points,bs))\nys = [string(\"Basis\",i) for i = size(B)[2]-1:-1:0]\nheatmap(eval_points,ys,reverse(B',dims=1))\n\nThis is a cubic spline basis. at most \\(k+1=4\\) basis are non-zero for any \\(x\\).\n\nheatmap(eval_points,ys,reverse(B',dims=1) .&gt; 0, cbar = false)\n\n\nUsing Sparsity of Splines\n\nIt may be better to store the splines in sparse format.\nLook at object B by typing B and typeof(B)\nThere are sparse system solvers available.\nCreating and storing the inverse of \\(\\Phi\\) destroys the sparsity structure (inverse of a sparse matrix is not sparse), and may not be a good idea.\nLook back at Computing coefficients form the tensor product\nWe only have to sum over the non-zero entries! Every other operation is pure cost.\nThis is implemented in ApproXD.jl for example via\n\n    function evalTensor2{T}(mat1::SparseMatrixCSC{T,Int64},\n                            mat2::SparseMatrixCSC{T,Int64},\n                            c::Vector{T})",
    "crumbs": [
      "Home",
      "Lectures",
      "8 - Function Approximation"
    ]
  },
  {
    "objectID": "lectures/lecture8.html#high-dimensional-functions-introducing-the-smolyak-grid",
    "href": "lectures/lecture8.html#high-dimensional-functions-introducing-the-smolyak-grid",
    "title": "Function Approximation",
    "section": "High Dimensional Functions: Introducing the Smolyak Grid",
    "text": "High Dimensional Functions: Introducing the Smolyak Grid\n\nThis is a modification of the Tensor product rule.\nIt elemininates points from the full tensor product according to their importance for the quality of approximation.\nThe user controls this quality parameter, thereby increasing/decreasing the size of the grid.\n[@jmmv] is a complete technical reference for this method.\n[@maliar-maliar] chapter 4 is very good overview of this topic, and the basis of this part of the lecture.\n\n\nThe Smolyak Grid in 2 Dimensions\n\nApproximation level \\(\\mu \\in \\mathbb{N}\\) governs the quality of the approximation.\nStart with a unidimensional grid of points \\(x\\): \\[ x = \\left\\{-1,\\frac{-1}{\\sqrt{2}},0,\\frac{1}{\\sqrt{2}},1\\right\\} \\] which are 5 Chebyshev nodes (it’s not important that those are Chebyshev nodes, any grid will work).\nA 2D tensor product \\(x\\otimes x\\) gives 25 grid points \\[ x\\otimes x=\\left\\{(-1,-1),(-1,\\frac{-1}{\\sqrt{2}}),\\dots,(1,1)\\right\\} \\]\nThe Smolyak method proceeds differently.\nWe construct three nested sets:\n\n\\[ \\begin{array}{l}\n        i=1 : S_1 = \\{0\\} \\\\\n        i=2 : S_2 = \\{0,-1,1\\} \\\\\n        i=3 : S_3 = \\left\\{-1,\\frac{-1}{\\sqrt{2}},0,\\frac{1}{\\sqrt{2}},1\\right\\}  \\end{array} \\]\n\nThen, we construct all possible 2D tensor products using elements from these nested sets in a table (next slide).\nFinally, we select only those elements of the table, that satisfy the Smolyak rule: \\[ i_1 + i_2 \\leq d + \\mu \\] where \\(i_1,i_2\\) are column and row index, respectively, and \\(d,\\mu\\) are the number of dimensions and the quality of approximation.\n\n\n\nThe Smolyak Grid in 2D: Tensor Table\n\n\n\n[@maliar-maliar] table 3: All Tensor Products\n\n\n\nSelecting Elements\n\nDenote the Smolyak grid for \\(d\\) dimensions at level \\(\\mu\\) by \\(\\mathcal{H}^{d,\\mu}\\).\nif \\(\\mu=0\\) we have \\(i_1+i_2\\leq 2\\). Only one point satisfies this, and \\[ \\mathcal{H}^{2,0} = \\{(0,0)\\} \\]\nif \\(\\mu=1\\) we have \\(i_1+i_2\\leq 3\\). Three cases satisfy this:\n\n\\(i_1 = 1, i_2=1 \\rightarrow (0,0)\\)\n\\(i_1 = 1, i_2=2 \\rightarrow (0,0),(0,-1),(0,1)\\)\n\\(i_1 = 2, i_2=1 \\rightarrow (0,0),(-1,0),(1,0)\\)\n\n\nTherefore, the unique elements from the union of all of those is \\[ \\mathcal{H}^{2,1} = \\{(0,0),(-1,0),(1,0),(0,-1),(0,1)\\} \\]\n\nif \\(\\mu=2\\) we have \\(i_1+i_2\\leq 4\\). Six cases satisfy this:\n\n\\(i_1 = 1, i_2=1\\)\n\\(i_1 = 1, i_2=2\\)\n\\(i_1 = 2, i_2=1\\)\n\\(i_1 = 1, i_2=3\\)\n\\(i_1 = 2, i_2=2\\)\n\\(i_1 = 3, i_2=1\\)\n\n\nTherefore, the unique elements from the union of all of those is \\[ \\mathcal{H}^{2,2} = \\left\\{(-1,1),(0,1),(1,1),(-1,0),(0,0),(1,0),(-1,-1),(0,-1),(1,-1),\\left(\\frac{-1}{\\sqrt{2}},0\\right),\\left(\\frac{1}{\\sqrt{2}},0\\right),\\left(0,\\frac{-1}{\\sqrt{2}}\\right),\\left(0,\\frac{1}{\\sqrt{2}}\\right)\\right\\} \\]\n\nNote that those elements are on the diagonal from top left to bottom right expanding through all the tensor products on table 3.\n\n\n\n\nSize of Smolyak Grids\n\nThe Smolyak grid grows much slower (at order \\(d\\) to a power of \\(\\mu\\)) than the Tensor grid (exponential growth)\n\n\n\n\n[@maliar-maliar] figure 2: Tensor vs Smolyak in 2D\n\n\n\n\n\n[@maliar-maliar] figure 4: Tensor vs Smolyak in 2D, number of grid points\n\n\n\n\nSmolyak Polynomials\n\nCorresponding to the construction of grid points, there is the Smolyak way of constructing polynomials.\nThis works exactly as before. We start with a one-dimensional set of basis functions (again Chebyshev here, again irrelevant): \\[ \\left\\{1,x,2x^2-1,4x^3-3x,8x^4-8x^2+1\\right\\} \\]\nThree nested sets:\n\n\\[ \\begin{array}{l}\n    i=1 : S_1 = \\{1\\} \\\\\n    i=2 : S_2 = \\{1,x,2x^2-1\\} \\\\\n    i=3 : S_3 = \\left\\{1,x,2x^2-1,4x^3-3x,8x^4-8x^2+1\\right\\}  \\end{array}\n\\]\n\nDenoting \\(\\mathcal{P}^{d,\\mu}\\) the Smolyak polynomial, we follow exactly the same steps as for the grids to select elements of the full tensor product table 5:\n\n\n\n\n[@maliar-maliar] figure 5: All Smolyak Polynomials in 2D\n\n\n\n\nSmolyak Interpolation\nThis proceeds as in the previouses cases:\n\nEvaluate \\(f\\) at all grid points \\(\\mathcal{H}^{d,\\mu}\\).\nEvaluate the set of basis functions given by \\(\\mathcal{P}^{d,\\mu}\\) at all grid points \\(\\mathcal{H}^{d,\\mu}\\).\nSolve for the interpolating coefficients by inverting the Basis function matrix.\n\n\nThere is a lot of redundancy in computing the grids the way we did it.\nMore sophisticated approaches take care not to compute repeated elements.\n\n\n\nSmolyak Grids in Julia\n\nThere are at least 2 julia packages that implement this idea:\n\nhttps://github.com/QuantEcon/BasisMatrices.jl\nhttps://github.com/RJDennis/SmolyakApprox.jl\n\n\n\n\nUsing (Smolyak from) BasisMatrices\n\nBasisMatrices provides general support for all kinds of basis matrices.\nCheb for chebyshev basis matrix\nSpline for spline\nLin for linear\nWe usually start with defining Params for each type: bounds, grid points, degrees, etc\nThen we construct a Basis type basis\nThen we can get evaluation nodes, i.e. points at which to evaluate our function with nodes(basis)\nThen get an evaluted basis matrix with BasisMatrix(basis)\n\nusing BasisMatrices\ngrid1 = range(1,stop = 3,length = 10)\nlp = LinParams(grid1)\nsp = SplineParams(collect(grid1),0,3)  # nodes, whether only 2 nodes, degree of spline\nsm = SmolyakParams(2,1,[3],[2])   # dims,mu,lower bound(s), upper bound(s)\nsm2 = SmolyakParams(2,[1,2],[0,-0.5],[3,4.4])   # dims,mu,lower bound(s), upper bound(s)\nsm3 = SmolyakParams(3,[1,2,2],[0,-0.5,1],[3,4.4,5])   # dims,mu,lower bound(s), upper bound(s)\n# simple interpolation\nfi(x) = x.^2 - 0.5*x.^3\n\nsp = SplineParams(collect(grid1),0,3)  # nodes, whether only 2 nodes, degree of spline\nsb = Basis(sp)\ns,n = BasisMatrices.nodes(sb)\nsv =fi(s)\ncoef, bs_direct = funfitxy(sb, s, sv)   # compute coefficients\nplot(fi,1,3,label=\"truth\")\nnewx = 1 .+ rand(10)*2\nnewy = funeval(coef,sb,newx)   # evaluate basis at new points\nscatter!(newx,newy,label=\"approx\")\nusing PlotlyJS\n# multiple dimensions\nbasis = Basis(SplineParams(25, -1, 1, 2),SplineParams(20, -1, 2, 1))   # quadratic and linear spline\n# get nodes\nX, x12 = BasisMatrices.nodes(basis)\n\n# function to interpolate\nf2(x1, x2) = cos.(x2) ./ exp.(x1)\nf2(X::Matrix) = f2.(X[:, 1], X[:, 2])\n\n# f at nodes\ny = f2(X)\nYmat = [f2(i,j) for i in x12[1], j in x12[2]]\n\ncoefs, bbs = funfitxy(basis, X, y)\nynew = funeval(coefs, basis, X[5:5, :])[1]\nusing Test\n@test maximum(abs, ynew -  y[5]) &lt;= 1e-12\n\nplotlyjs()\nPlots.surface(x12[1],x12[2],Ymat',alpha=0.7,cbar=false)\n\nxnew=hcat(-1 .+ rand(10)*2 , -1 .+ rand(10)*3)\nynew = [funeval(coefs, basis, xnew[i,:]) for i in 1:size(xnew,1)]\nPlots.scatter!(xnew[:,1],xnew[:,2],ynew,markersize=2,markershape=:rect)\n# same with the smolyak grid\n# multiple dimensions\nusing LinearAlgebra\nsp = SmolyakParams(2,3, [-1,-1],[1,2])\nbasis = Basis(sp)   \n# get nodes\nX, x12 = BasisMatrices.nodes(basis)\n\n# # f at nodes\ny = f2(X)\nYmat = [f2(i,j) for i in unique(X[:,1]), j in unique(X[:,2])]\n\n\n# map domain into unit hypercube\ncube = BasisMatrices.dom2cube(X, basis.params[1])\n# evaluate basis matrix\neb   = BasisMatrices.build_B(sp.d, sp.mu, cube, sp.pinds)\ncoef = pinv(eb) * y\n\nxnew = hcat(-1 .+ rand(30)*2 , -1 .+ rand(30)*3)\ncube = BasisMatrices.dom2cube(xnew, sp)\neb   = BasisMatrices.build_B(sp.d, sp.mu, cube, sp.pinds)\nynew = eb * coef\n\nscatter(X[:,1],X[:,2],y,markersize=2)\nscatter!(xnew[:,1],xnew[:,2],ynew,markersize=2,markershape=:rect)",
    "crumbs": [
      "Home",
      "Lectures",
      "8 - Function Approximation"
    ]
  },
  {
    "objectID": "lectures/lecture8.html#sparse-grids",
    "href": "lectures/lecture8.html#sparse-grids",
    "title": "Function Approximation",
    "section": "Sparse Grids",
    "text": "Sparse Grids\n\nSparse Grids are widely used.\nThe Tasmanian library is excellent.\nThe Tasmanian.jl is a simple wrapper to that library.\nSimon Scheidegger has many useful resources on his website, in particular their joint paper with Johannes Brumm features an application of sparse grids for high dimensional economics problems.\n\nusing Tasmanian\nTasmanian.ex2()\n\nTasmanian.ex3()",
    "crumbs": [
      "Home",
      "Lectures",
      "8 - Function Approximation"
    ]
  },
  {
    "objectID": "homeworks/hw4.html",
    "href": "homeworks/hw4.html",
    "title": "Homework 4",
    "section": "",
    "text": "This time we will extend our optimization efforts into a world with constraints. We have an application from finance: which assets best to invest in, if they have different expected returns?\nGet the homework notebook here. right click and save this notebook, or copy the link directly into a running Pluto session to open!\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 4"
    ]
  },
  {
    "objectID": "homeworks/hw4.html#homework-submission",
    "href": "homeworks/hw4.html#homework-submission",
    "title": "Homework 4",
    "section": "Homework Submission",
    "text": "Homework Submission\n\nWhen: You have until 25/3.\nWho: teams of max 2/3\nWhat: one rendered HTML output of your notebook. make sure all cells (code?) that you want me to see are visible in your notebook, or they won’t be visible in the HTML either.\nHow: dropbox link via slack.",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 4"
    ]
  },
  {
    "objectID": "homeworks/hw2.html",
    "href": "homeworks/hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Let’s continue with some epidemic stuff! This time we’ll add a spatial dimension to our simulation.\nGet the homework notebook here. right click and save this notebook, or copy the link directly into a running Pluto session to open!\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 2"
    ]
  },
  {
    "objectID": "homeworks/hw2.html#homework-submission",
    "href": "homeworks/hw2.html#homework-submission",
    "title": "Homework 2",
    "section": "Homework Submission",
    "text": "Homework Submission\n\nWhen: You have until next wednesday.\nWho: teams of max 2/3\nWhat: one rendered HTML output of your notebook. make sure all cells (code?) that you want me to see are visible in your notebook, or they won’t be visible in the HTML either.\nHow: dropbox link via slack.",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 2"
    ]
  },
  {
    "objectID": "homeworks/hw1.html",
    "href": "homeworks/hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "This homework will refer to something we have not done in class, but which you can easily read up on by yourself. Here is a notebook that introduces the SIR model. In the homework, I ask you to copy sections of code from there into your homework notebook, which you will then submit.\nGet the homework notebook here. right click and save this notebook, or copy the link directly into a running Pluto session to open!\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 1"
    ]
  },
  {
    "objectID": "homeworks/hw1.html#homework-submission",
    "href": "homeworks/hw1.html#homework-submission",
    "title": "Homework 1",
    "section": "Homework Submission",
    "text": "Homework Submission\n\nWhen: You have until next monday.\nWho: teams of max 2/3\nWhat: one rendered HTML output of your notebook. make sure all cells (code?) that you want me to see are visible in your notebook, or they won’t be visible in the HTML either.\nHow: dropbox link via slack.",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Economics for PhDs",
    "section": "",
    "text": "Teacher: Florian Oswald, florian.oswald@unito.it\nClass Times: tbc\nClass Location: Collegio Carlo Alberto\nSlack: There will be a slack channel for all communication\n© Florian Oswald, 2026"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Computational Economics for PhDs",
    "section": "Course Overview",
    "text": "Course Overview\nThis is a course for PhD and Allievi students at Collegio Carlo Alberto in Computational Economics. You will learn about some commonly used methods in Computational Economics and Structural Econometrics. These methods are being used in all fields of Economics. The course has a clear focus on applying what you learn. We will cover the theoretical concepts that underlie each topic, but you should expect a fair amount of hands on action required on your behalf. In the words of the great Che-Lin Su:\n\nDoing Computation is the only way to learn Computation. Doing Computation is the only way to learn Computation. Doing Computation is the only way to learn Computation.\n\nTrue to that motto, there will be homeworks for you to try out what you learned in class. There will also be a term project."
  },
  {
    "objectID": "index.html#outline-of-topics",
    "href": "index.html#outline-of-topics",
    "title": "Computational Economics for PhDs",
    "section": "Outline of Topics",
    "text": "Outline of Topics\n\n\n\nSection\nTopic\n\n\n\n\n1\nIntroduction, Why Julia\n\n\n2\njulia crash course 1\n\n\n3\njulia crash course 2\n\n\n4\nOptimization 1: Recap of theory, Automatic Differentiation, setting up an optimizer\n\n\n5\nOptimization 2: Algorithms\n\n\n6\nOptimization 3: Constraints\n\n\n7\nParallel Computing\n\n\n8\nData and Machine Learning\n\n\n9\nDynamic Programming 1: Basics\n\n\n9\nDynamic Programming 2: Aiyagari and Shimer+Smith Models\n\n\n10\nDiscrete and Continous Choice Problems\n\n\n11\nDynamic Discrete Choice Models"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Computational Economics for PhDs",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou need a laptop.\nYou should be familiar with the material from Introduction to Programming - more below.\nYou must sign up for a free account at github.com. Choose a reasonable user name and upload a profile picture.\nBefore you come the first class, please follow closely the installation instructions.\n\n\nGetting Programming Skills\n\nCheck out what is being taught in the Introduction to Programming course. I want you to read through sessions 1-3 on your own. I will give you a quiz to complete on this before week 3.\nWe will be using Julia for this course.\n\nNoteworthy Differences from Other Languages\nMATLAB, Python, Julia Syntax Comparison"
  },
  {
    "objectID": "index.html#term-project",
    "href": "index.html#term-project",
    "title": "Computational Economics for PhDs",
    "section": "Term Project",
    "text": "Term Project\nThere are two options:\n\nReplicate a published paper.\nDevelop the computational aspects of your own work.\n\n\nReplication\nThe requirements for choice of paper to replicate are:\n\nIt’s an economics paper.\nPublished version and replication kit is available online.\nThe paper to replicate must not use julia.\nYou must use julia for your replication.\n\nIdeally your choice will involve at least some level of computational interest (i.e. more than an IV regression)\nHowever, you can replicate a paper with an IV regression, but you have to go all the way to get the exact same results as in the paper. I.e. if the author typed the stata command ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age), cluster(year) you will have to write (or find) julia code which will match all output from this, including standard errors.\n\nYou need to set up a public github repository where you will build a documentation website of your implementation. You’ll learn how to do this in the course.\nI encourage you to let the world know about your replication effort via social media and/or email to the authors directly. This is independent of whether you were able or not to replicate the results. Replication is not about finding errors in other peoples’ work. If you are able to replicate some result in julia, this may be very interesting for others.\n\n\nReplication Resources\n\nhere is a great list by the AEA\nECTA code and data\nRevEconDynamics codes\nEach issue of RevEconDynamics , e.g. https://www.economicdynamics.org/volume-39-2021/\nThe AEA Data Editor’s website\nThe Restud Data Editor and their zenodo repo of replication kits\nThe Social Science Data Editor’s joint website\n\n\n\n\nDevelop Your Own Work\nYou can develop your own work as well. Requirements:\n\nsetup a github repository which contains the code (your decision whether public or private, in any case you have to share it with me)\nproduce a short document (max 10 pages, ideally much less) which describes\n\nthe aim of the project\nthe computational problem\nyour computational strategy to solve that problem\n\nThe main focus for me will lie on\n\nHow easy is it to use your code?\nHow easy is it to understand your code (code readability and provided documentation)?\nDid you provide unit tests? Can I be convinced that your code does what it is supposed to do?"
  },
  {
    "objectID": "index.html#grade",
    "href": "index.html#grade",
    "title": "Computational Economics for PhDs",
    "section": "Grade",
    "text": "Grade\nYour grade will be 60% homeworks, 40% term project."
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Computational Economics for PhDs",
    "section": "Textbooks",
    "text": "Textbooks\nThere are some excellent references for computational methods out there. This course will use material from\n\nThe Classics\n\nFackler and Miranda (2002), Applied Computational Economics and Finance, MIT Press\nKenneth Judd (1998), Numerical Methods in Economics, MIT Press\nNocedal, Jorge, and Stephen J. Wright (2006): Numerical Optimization, Springer-Verlag\n\n\n\nNewcomers\n\nJulia for Data Analysis (2023), Bogumił Kamiński, Manning Publications.\nAlgorithms for Optimization (2019), Mykel J. Kochenderfer and Tim A. Wheeler, Algorithms for Optimization, MIT Press.\nA Gentle Introduction to Effective Computing in Quantitative Research - What Every Research Assistant Should Know, Harry J. Paarsch and Konstantin Golyaev\nStatistics with Julia (2021), Yoni Nazarathy and Hayden Klok, Springer.\nQuantitative Economics with Julia by Perla, Sargent and Stachurski is a wonderful resource and we use it a lot in this course."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Computational Economics for PhDs",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nIf you decide to participate in this course, I expect you to abide by the following minimal code of conduct.\n\nBe polite to the other class participants.\nWhile in class, do not spend time on messaging apps, chat rooms, computer games, or similar content.\n\nYou can expect your instructor to abide by the same code of conduct, so this is a matter of mutual respect. If you are found in breach of the above you will be given a single warning, and I will ask you to no longer join the course after a second time. Your grade will be “fail”."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Computational Economics for PhDs",
    "section": "License",
    "text": "License\nThe copyright notice to be included in any copies and other derivative work of this material is:\nCopyright 2026 Florian Oswald, florian.oswald@gmail.com\nThank you.\n This is licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License"
  },
  {
    "objectID": "homeworks/hw0.html",
    "href": "homeworks/hw0.html",
    "title": "Homework 0 (zero)",
    "section": "",
    "text": "The first week’s homework is to make you familiar with the Pluto environment. Given it’s homework number 0, it won’t count towards your grade!\nYou can do the homework in teams of max 2 people (try not to do it alone!)\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 0 (zero)"
    ]
  },
  {
    "objectID": "homeworks/hw0.html#you-have-3-tasks",
    "href": "homeworks/hw0.html#you-have-3-tasks",
    "title": "Homework 0 (zero)",
    "section": "You have 3 Tasks",
    "text": "You have 3 Tasks\nI want you to go through the first 3 Pluto “featured notebooks”: “Getting Startet”, “Markdown” and “Basic Mathematics”. You can see those upon doing\nusing Pluto\nPluto.run()\nand scrolling down a bit.\nOnce the notebook is loaded, click top right on “edit” this notebook. Complete all tasks.",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 0 (zero)"
    ]
  },
  {
    "objectID": "homeworks/hw0.html#homework-submission",
    "href": "homeworks/hw0.html#homework-submission",
    "title": "Homework 0 (zero)",
    "section": "Homework Submission",
    "text": "Homework Submission\nsubmit by next class.\nHow and What to Submit?\n\nWhat? three static html files of each of your notebooks (click top right on “sharing” symbol) and select “Static HTML”. The first cell in your notebook must contain the names of students who worked on it.\nHow? Upload your .html files in format HW01-name1-name2.html, HW02-name1-name2.html and HW03-name1-name2.html the dropbox file request link which will be shared with you via slack.",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 0 (zero)"
    ]
  },
  {
    "objectID": "homeworks/hw3.html",
    "href": "homeworks/hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Time for some optimization around here! Let me introduce you to the well known probit model! 😁\nGet the homework notebook here. right click and save this notebook, or copy the link directly into a running Pluto session to open!\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 3"
    ]
  },
  {
    "objectID": "homeworks/hw3.html#homework-submission",
    "href": "homeworks/hw3.html#homework-submission",
    "title": "Homework 3",
    "section": "Homework Submission",
    "text": "Homework Submission\n\nWhen: You have until 17/3.\nWho: teams of max 2/3\nWhat: one rendered HTML output of your notebook. make sure all cells (code?) that you want me to see are visible in your notebook, or they won’t be visible in the HTML either.\nHow: dropbox link via slack.",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 3"
    ]
  },
  {
    "objectID": "homeworks/hw5.html",
    "href": "homeworks/hw5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Homework - Harold Zurcher\n\nHow could we not talk about Harold Zurcher in this course!\n\nFirst things first: Who was Harold Zurcher? Generations of Econ PhD students have heard about this man. Harold Zurcher even had a parody account on twitter, while twitter still existed. Why the fame? Well he was the leading actor in John Rust’s groundbreaking paper on Bus Engine Replacement. Much of what we do nowadays in structural micro econometrics can be traced back to that paper. Even I have a paper on the Bus Engine Replacement Model!\n\nThis homework is easy 🍿\nIn this very easy homework, I will outsource you to the phantastic video material of Fedor Iskhakov. Fedor is among the world’s leading experts of that model class - amongst other things - and therefore (trying to) redo what he did in his course is just pointless. We will next time in class implement the numerical solution in julia. Be sure to check out all the other great stuff that Fedor does in his course.\nSo, what’s to do?\n\nWatch the 45 minute video of Fedor here. It’s full of relevant info beyond the Rust model at hand.\n\n\n\n\n\n\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Homeworks",
      "Homework 5"
    ]
  },
  {
    "objectID": "lectures/lecture9.html",
    "href": "lectures/lecture9.html",
    "title": "Numerical Integration",
    "section": "",
    "text": "We want to evaluate the potentially multidimensional definite integral\n\\[\\begin{equation}\nI = \\int_\\Omega f(x) dx\n\\end{equation}\\]\nwhere it’s important to keep track of the volume of the function domain, i.e. we say \\(\\Omega\\) is a subset of \\(\\mathbb{R}^m\\) with volume\n\\[\\begin{equation}\nV = \\int_\\Omega dx\n\\end{equation}\\]\nHere is an example of \\(f\\):\n\\[\nf(x) = \\sin(x)^2 + 0.1 x\n\\]\nWe will evaluate this over \\(x \\in [0,5]\\), where we find using standard calculus techniques, that\n\\[\\begin{equation}\n\\int_0^5 f(x) dx = 3.886\n\\end{equation}\\]\nLet’s make a plot of this, where \\(f\\) is a black line, and the red shaded area is \\(I\\).\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "9 - Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture9.html#problem-definition",
    "href": "lectures/lecture9.html#problem-definition",
    "title": "Numerical Integration",
    "section": "",
    "text": "We want to evaluate the potentially multidimensional definite integral\n\\[\\begin{equation}\nI = \\int_\\Omega f(x) dx\n\\end{equation}\\]\nwhere it’s important to keep track of the volume of the function domain, i.e. we say \\(\\Omega\\) is a subset of \\(\\mathbb{R}^m\\) with volume\n\\[\\begin{equation}\nV = \\int_\\Omega dx\n\\end{equation}\\]\nHere is an example of \\(f\\):\n\\[\nf(x) = \\sin(x)^2 + 0.1 x\n\\]\nWe will evaluate this over \\(x \\in [0,5]\\), where we find using standard calculus techniques, that\n\\[\\begin{equation}\n\\int_0^5 f(x) dx = 3.886\n\\end{equation}\\]\nLet’s make a plot of this, where \\(f\\) is a black line, and the red shaded area is \\(I\\).",
    "crumbs": [
      "Home",
      "Lectures",
      "9 - Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture9.html#code-setup",
    "href": "lectures/lecture9.html#code-setup",
    "title": "Numerical Integration",
    "section": "Code Setup",
    "text": "Code Setup\nTo run the code in this document you need have an environment where the following packages are installed:\n\n# you need to have those packages installed.\nusing CairoMakie\nusing Random\nusing LaTeXStrings\nusing OrderedCollections   # for OrderedDict\nusing FastGaussQuadrature  # for intergration rules\nusing DataFrames           # to display a table\nusing Sobol                # to get sobol sequences\nset_theme!()  # resetting all quarto default values for fig width etc\ntrue_I = 3.886\n\n3.886\n\n\n\n\nShow Code\nx = 0:0.01:5\nf(x) = sin(x)^2 + 0.1x\ny = f.(x)\nfig = Figure(size = (800,600))\nax = Axis(fig[1,1], xlabel = L\"x\", ylabel = L\"\\sin(x)^2 + 0.1 x\")\nlines!(ax, x, y, label = L\"f\", color = :black, linewidth = 2)\nband!(ax, x, fill(0,length(x)), y, color = (:red, 0.5), label = \"Integral\")\naxislegend(; merge = true, position = :lt)\n\nfig\n\n\n\n\n\n\n\n\n\nAs with Riemann Integrals where we split the continuous domain of \\(f\\) into smaller and smaller chunks, which we then sum up (\\(\\int\\)), the numerical counterpart does the same: measure the value of f (the height of the black line) at different points, and sum over them. The main question is:\nAt which points?",
    "crumbs": [
      "Home",
      "Lectures",
      "9 - Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture9.html#monte-carlo-integration",
    "href": "lectures/lecture9.html#monte-carlo-integration",
    "title": "Numerical Integration",
    "section": "Monte Carlo Integration",
    "text": "Monte Carlo Integration\nA very intuitive first solution is to draw N random points from the domain of f, evalute the function there, and compute their average. We would approximate \\(I\\) as follows, where \\(x_i \\in \\Omega\\) is a randomly chosen point from the function’s domain:\n\\[\\begin{equation}\nI \\approx Q_N \\equiv V \\frac{1}{N} \\sum_{i=1}^N f(x_i) = V \\bar{f}\n\\end{equation}\\]\nThis works because the law of large numbers tells us that\n\\[\\begin{equation}\n\\lim_{N \\to \\infty} Q_N  = I.\n\\end{equation}\\]\nThe uncertainty from this method is easily quantifiable by the resulting variation in our estimate:\n\\[\\begin{equation}\nVar(f) = \\sigma_N^2 = \\frac{1}{N-1} \\sum_{i=1}^N (f(x_i) -  \\bar{f})^2\n\\end{equation}\\]\nfrom which we get the variance of \\(Q_N\\) as \\(Var(Q_N) = V^2 \\frac{\\sigma_N^2}{N}\\). Hence,clearly visible that this decreases as \\(N\\) increases. We usually report the standard error of the estimator, so we report\n\\[\\begin{equation}\n\\sigma_Q \\equiv \\sqrt{Var(Q_N)} = V \\frac{\\sigma_N}{\\sqrt{N}}\n\\end{equation}\\]\n👍\n\n\n\n\n\n\nTipCompute \\(\\sigma_Q\\)!\n\n\n\nWrite a function that takes \\(f\\) from above, and computes standard error \\(\\sigma_Q\\) for \\(N\\) points. Your function should take arguments sample_points, which is a vector of evaluation points, and fun, which is a function to evaluate.\n\n\n\n\nShow Code\nfunction σ(sample_points,fun)\n    N = length(sample_points)\n    ys = fun.(sample_points)\n    ybar = sum(ys) / N  # mean\n    var = 1 / (N-1) * sum( (ys .- ybar) .^ 2 )\n    sqrt(var)\nend\n\n\nσ (generic function with 1 method)\n\n\n\n\n\n\n\n\nTipCompute the monte carlo integral and it’s error!\n\n\n\nWrite a function mc_integrate that takes \\(f\\) from above, and computes both the monte carlo integration \\(Q_N\\) as well as its standard error \\(\\sigma_Q\\) for a set of \\(N\\) randomly chosen points. Your function should take arguments N, which is a vector of evaluation points, and fun, which is a function to evaluate. Set a random seed to ensure reproducibility. Then, call your function to compute \\(Q\\) for \\(N \\in \\{2,4,10,20,50,100,1000,10000\\}\\) and compare the outputs. Produce an OrderedDict where the keys are those values for \\(N\\).\n\n\n\n\nShow Code\nfunction mc_integrate(N,fun)\n    Random.seed!(0)   # any number works\n    V = 5 # integrate dx from 0 to 5\n    pts = rand(N) .* V  # N random numbers in [0,5]\n    mc_integral = V / N * sum( fun.(pts) )\n    mc_error = V * σ(pts,fun) / sqrt(N)\n    mc_integral, mc_error\nend\n\nns = [2,4,10,20,50,100,1000,10000]\n\nmc_results = OrderedDict(k =&gt;\n    mc_integrate(k,f) for k in ns);\n\n\n\n\n\n\n\n\nTipNow make a plot!\n\n\n\nTaking the dict from the above question, write a function that makes a line plot with \\(N\\) on the x axis and your estimate of the integral on the axis. Also add the error bars with band! function! Your function should take the output OrderedDict from above as argument. Scale the x-axis as log10.\n\n\n\n\nShow Code\nfunction plot_mc(od::OrderedDict; errors = true)\n    x = collect(keys(od))\n    v = collect(values(od))\n    Q = [i[1] for i in v]\n    E = [i[2] for i in v]\n\n    fig = Figure(size = (800,600))\n    ax = Axis(fig[1,1], xlabel = \"N\", ylabel = \"Qn\", xscale = log10)\n    lines!(ax, x, Q, label = L\"Q_n\", color = :black, linewidth = 1)\n    scatter!(ax, x, Q, label = L\"Q_n\", color = :black, markersize = 15)\n    hlines!(true_I, color = (:red,0.5), label = \"Truth\", linestyle = :dash)\n    if errors errorbars!(ax, x, Q, E; whiskerwidth = 1, color = (:red)) end\n    axislegend(; merge = true, position = :rb, unique = true)\n\n    return fig\nend\n\nplot_mc(mc_results)\n\n\n\n\n\n\n\n\n\nYou can see the red dashed line as being the true value of the integral. The random nature of evaluation points means that while with \\(N=100\\) points we are ok, with \\(N=1000\\) points we are off the mark. We need to go up to \\(N=10000\\) to be sure we got this right. You can see, it takes us quite long until the monte carlo method converges to that value. So, that’s the main drawback of this method.",
    "crumbs": [
      "Home",
      "Lectures",
      "9 - Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture9.html#quasi-monte-carlo",
    "href": "lectures/lecture9.html#quasi-monte-carlo",
    "title": "Numerical Integration",
    "section": "Quasi-Monte carlo",
    "text": "Quasi-Monte carlo\nThat’s a version where we do not choose random numbers as evaluation points, but sub-random sequences or low discrepancy sequences of random numbers, which aim at variance reduction. Everything else is the same.\n\n\n\n\n\n\nTipModify your mc_integrate for Quasi-MC\n\n\n\nModify your function from above so that instead of rand it chooses numbers from the Sobol sequences. Then make the plot again.\n\n\n\n\n\n\n\n\nNoteUsing Sobol.jl\n\n\n\n\nmake a constructor: s = SobolSeq(lb,ub) where ub,lb are vectors of upper and lower bounds\nget n sobol-random numbers into a vector with reduce(vcat, next!(s) for i in 1:n)\n\n\n\n\n\nShow Code\nfunction qmc_integrate(N,fun)\n    Random.seed!(0)   # any number works\n    V = 5 # integrate dx from 0 to 5\n\n    s = SobolSeq([0], [5])\n    pts = reduce(vcat, next!(s) for i in 1:N)\n    mc_integral = V / N * sum( fun.(pts) )\n    mc_error = V * σ(pts,fun) / sqrt(N)\n    mc_integral, mc_error\nend\n\nqmc_results = OrderedDict(k =&gt;\n    qmc_integrate(k,f) for k in ns);\n\n\n\n\n\n\n\n\n\n\n\nOne of the merits of quasi monte carlo integration is that it’s rate of convergence is faster. You see that here various integrations stabilize at the “true” value (where \\(N=1000\\)) earlier than before. Notice I removed the error bars from the plot because I the previous formula is no longer correct. However it’s good to know that the relationship between QMC and MC errors is\n\\[\nO\\left(\\frac{(\\log N)^s}{N} \\right) \\quad vs \\quad O\\left( \\frac{1}{\\sqrt{N}} \\right)\n\\]\ntherefore, for QMC to do better than MC, we need \\(s\\) small and \\(N\\) large. In other words, in high-dimensional settings (high \\(s\\)), you might actually do better with straight MC.",
    "crumbs": [
      "Home",
      "Lectures",
      "9 - Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture9.html#gaussian-quadrature-integration",
    "href": "lectures/lecture9.html#gaussian-quadrature-integration",
    "title": "Numerical Integration",
    "section": "Gaussian Quadrature integration",
    "text": "Gaussian Quadrature integration\nBased on an early contribution of Carl Friedrich Gauss, we have that an \\(n\\)-point quadrature rule will yield an exact integration result to functions that look like polynomials of degree \\(2n -1\\), or less, by choosing \\(n\\) suitable nodes \\(x_i\\) and weights \\(w_i\\). That is quite the result. The wikipedia entry is very interesting. Basically, we will now concentrate on how to do better than in monte carlo integration, where each point gets the same weight \\(1/N\\), and where our only hope is to generate a very large set of sample points, which may be costly.\nWe continue in the above framework, i.e. in order to compute the expected value of a function \\(G\\), say, we do the following:\n\\[\nE[G(\\epsilon)] = \\int_{\\mathbb{R}^N} G(\\epsilon) p(\\epsilon) d\\epsilon \\approx \\sum_{j=1}^J w_j G(\\epsilon_j)\n\\]\nWe have some explanation to do:\n\n\\(N\\) is the dimensionality of the integration problem.\n\\(G:\\mathbb{R}^N \\mapsto \\mathbb{R}\\) is the function we want to integrate wrt \\(\\epsilon \\in \\mathbb{R}^N\\).\n\\(p\\) is a density function s.t. \\(\\int_{\\mathbb{R}^n} p(\\epsilon) d\\epsilon = 1\\).\n\\(w\\) are integration weights such that (most of the time) \\(\\sum_{j=1}^J w_j = 1\\).\n\\(\\epsilon_j\\) are integration nodes, i.e. the points where we choose to evaluate function \\(G\\). Notice that nodes and weights come in pairs.\nWe will look at normal shocks \\(\\epsilon \\sim N(0_N,I_N)\\)\nin that case, the weighting function becomes \\(w(\\epsilon) = (2\\pi)^{-N/2} \\exp \\left(-\\frac{1}{2}\\epsilon^T \\epsilon \\right)\\)\n\\(I_N\\) is the n by n identity matrix, i.e. there is no correlation among the shocks for now.\nOther random processes will require different weighting functions, but the principle is identical.\nFor now, let’s say that \\(N=1\\)\n\n\nDifferent Quadrature Rules\n\nWe focus exclusively on those and leave Simpson and Newton Cowtes formulas out.\n\nThis is because Quadrature is the method that in many situations gives highes accuracy with lowest computational cost.\n\nQuadrature provides a rule to compute weights \\(w_j\\) and nodes \\(\\epsilon_j\\).\nThere are many different quadrature rules.\nThey differ in their domain and weighting function.\nwikipedia again has a useful table for us.\nIn general, we can convert our function domain to a rule-specific domain with change of variables.\n\n\n\nGauss-Hermite: Expectation of a Normally Distributed Variable\n\nThere are many different rules, all specific to a certain random process.\nGauss-Hermite is designed for an integral of the form \\[ \\int_{-\\infty}^{+\\infty} e^{-x^2} G(x) dx \\] and where we would approximate \\[ \\int_{-\\infty}^{+\\infty} e^{-x^2} f(x) dx \\approx \\sum_{i=1}^n w_i G(x_i) \\]\nNow, let’s say we want to approximate the expected value of function \\(f\\) when it’s argument is \\(z\\sim N(\\mu,\\sigma^2)\\): \\[ E[f(z)] = \\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( -\\frac{(z-\\mu)^2}{2\\sigma^2} \\right) f(z) dz \\]\n\n\n\nGauss-Quadrature with \\(N&gt;1\\)?\nEasy: we just take the kronecker product of all univariate rules, i.e. the kronecker product amongst all weights and all nodes. Let’s look at an example.\n\nThis works well as long as \\(N\\) is not too large. The number of required function evaluations grows exponentially. \\[ E[G(\\epsilon)] = \\int_{\\mathbb{R}^N} G(\\epsilon) p(\\epsilon) d\\epsilon \\approx \\sum_{j_1=1}^{J_1} \\cdots \\sum_{j_N=1}^{J_N} w_{j_1}^1 \\cdots w_{j_N}^N G(\\epsilon_{j_1}^1,\\dots,\\epsilon_{j_N}^N) \\] where \\(\\omega_{j_1}^1\\) stands for weight index \\(j_1\\) in dimension 1, same for \\(\\epsilon\\).\nTotal number of nodes: \\(J=J_1 J_2 \\cdots J_N\\), and \\(J_i\\) can differ from \\(J_k\\).\nSuppose we have \\(\\epsilon^i \\sim N(0,1),i=1,2,3\\) as three uncorrelated random variables.\nLet’s take \\(J=3\\) points in all dimensions, so that in total we have \\(J^N=27\\) points.",
    "crumbs": [
      "Home",
      "Lectures",
      "9 - Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture9.html#quadrature-with-julia",
    "href": "lectures/lecture9.html#quadrature-with-julia",
    "title": "Numerical Integration",
    "section": "Quadrature with julia",
    "text": "Quadrature with julia\n\nnp = 3  # number of points\n\n# functions from FastGaussQuadrature.jl\nrules = Dict(\"hermite\" =&gt; gausshermite(np),\n             \"chebyshev\" =&gt; gausschebyshev(np),\n             \"legendre\" =&gt; gausslegendre(np),\n             \"lobatto\" =&gt; gausslobatto(np));\n\nHere are the respective nodes and weights for each of those four rules:\n\n\n4×3 DataFrame\n\n\n\nRow\nRule\nnodes\nweights\n\n\n\nSymbol\nArray…\nArray…\n\n\n\n\n1\nlobatto\n[-1.0, 0.0, 1.0]\n[0.333333, 1.33333, 0.333333]\n\n\n2\nhermite\n[-1.22474, -1.11022e-15, 1.22474]\n[0.295409, 1.18164, 0.295409]\n\n\n3\nlegendre\n[-0.774597, 0.0, 0.774597]\n[0.555556, 0.888889, 0.555556]\n\n\n4\nchebyshev\n[-0.866025, 6.12323e-17, 0.866025]\n[1.0472, 1.0472, 1.0472]\n\n\n\n\n\n\n\nApproximating an AR1 process\nhttp://karenkopecky.net/Rouwenhorst_WP.pdf",
    "crumbs": [
      "Home",
      "Lectures",
      "9 - Numerical Integration"
    ]
  },
  {
    "objectID": "lectures/lecture10.html",
    "href": "lectures/lecture10.html",
    "title": "Lecture 10: Dynamic Discrete Choice Models",
    "section": "",
    "text": "We gave an overview of discrete choice models on the board (also contained in the first notebook below)\nWe introduced the bus model.\nWe did talk about estimation of dynamic programming models. Here are some slides.\n\nHere are some notebooks!\n\n\n\nTopic\nNotebook\n\n\n\n\nSolving the Model\ndownload notebook\n\n\nEstimating the Model\ndownload notebook\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "10 - Dynamic Discrete Choice"
    ]
  },
  {
    "objectID": "lectures/lecture10.html#the-rust-bus-model",
    "href": "lectures/lecture10.html#the-rust-bus-model",
    "title": "Lecture 10: Dynamic Discrete Choice Models",
    "section": "",
    "text": "We gave an overview of discrete choice models on the board (also contained in the first notebook below)\nWe introduced the bus model.\nWe did talk about estimation of dynamic programming models. Here are some slides.\n\nHere are some notebooks!\n\n\n\nTopic\nNotebook\n\n\n\n\nSolving the Model\ndownload notebook\n\n\nEstimating the Model\ndownload notebook",
    "crumbs": [
      "Home",
      "Lectures",
      "10 - Dynamic Discrete Choice"
    ]
  },
  {
    "objectID": "lectures/lecture3.html",
    "href": "lectures/lecture3.html",
    "title": "Lecture 3: Optimization 1",
    "section": "",
    "text": "Creating a package is a great way to ensure reproducibility of your work.\nIt helps to make your work shareable.\nIt helps to test the code which makes up your work.\n👉 Let’s do it!\n\n\n\n\nStart julia\ntwo equivalent options:\n\nenter Pkg mode (hit ]), then generate path/to/new/package, or\nsay using Pkg, then Pkg.generate(\"path/to/new/package\")\n\n\nHere is an example:\n\n]    # this jumps into Pkg mode\n\n(@v1.11) pkg&gt; generate Mypkg\n  Generating  project Mypkg:\n    Mypkg/Project.toml\n    Mypkg/src/Mypkg.jl\n\n(@v1.11) pkg&gt; \nDepending on where you started your julia session, there is now a new folder Mypkg:\nshell&gt; ls Mypkg/\nProject.toml  src/\nIt’s a good idea to start a new VSCode window at that folder location. Doing so, you would see this:\n\n\n\nOur julia package\n\n\n\ngreat. Start a julia repl in the usual way.\nNotice how the bottom bar in VSCode indicates that we are in Mypkg env - VScode asked me whether I wanted to change into this. If this is not the case, you won’t be able to load our package:\n\njulia&gt; using Mypkg\nERROR: ArgumentError: Package Mypkg not found in current path.\n- Run `import Pkg; Pkg.add(\"Mypkg\")` to install the Mypkg package.\n\nWe need to switch into the environment of this package before we can load it locally. This is called activate an environment:\n\n]  # in Pkg mode\n(@v1.11) pkg&gt; activate .  # `.` for current directory\n  Activating project at `~/Mypkg`\n\n(Mypkg) pkg&gt;  # hit backspace\n\njulia&gt; using Mypkg\n[ Info: Precompiling Mypkg [c4d85591-a952-48fb-b3d1-49a9454516b2] \nAlternatively, just click on the env indicator in VSCode and choose the current folder.\nGreat, now we can use the functions contained in the package. Let’s see:\njulia&gt; Mypkg.greet()\nHello World!\n\n\n\n\n\n\nNoteCode Loading\n\n\n\nThere are two ways in which we can load code into a running julia session:\n\nBy includeing code - equivalent to copy and pasting code into the REPL, and what happens when we say execute active file in REPL in VSCode. In practice, those mechanisms execute the function include(\"some_file.jl\").\nVia package loading: We import a set of functions contained in a package, via the using or import statements.\n\nNotice how we did not have to say run current file in REPL or similar commands. Saying using Mypkg immediately made our code available in the current session.\n\n\n\n\n\nNext question: How can we now work on the code and investigate it’s changes in the REPL?\n\nWe can obviously execute the current file in the REPL (basically copy and paste the code into the REPL). Again, copy and paste, or include(\"file.jl\"). But that’s cumbersome.\nThere is a great alternative - Revise.jl. Loading this package before you import your package means that Revise will track changes in your source code and expose them immediately in what you see in the REPL. Revise tracks all changes in our code. Let’s go and look the package documentation.\n\nGood, let’s try this out. Restart the REPL in the Mypkg project. First, we add Revise to our package’s environment, so we can always load it.\n] # for pkg mode\n(Mypkg) pkg&gt; add Revise\nNext, let’s load Revise before we import any other code we want to work on:\nusing Revise\nusing Mypkg\nsee again if that works now:\njulia&gt; Mypkg.greet()\nHello World!\n\nGreat! Now let’s open VSCode in that location and make some changes. Like, let’s just change the greet function slightly and save the Mypkg.jl file:\n\ngreet() = print(\"Hello Earthlings!\")\n\nExecute again in the REPL (notice no code loading action necessary on our behalf!)\n\njulia&gt; Mypkg.greet()\nHello Earthlings!\n\nAwesome! So we can change our code and immediately try out it’s effects. Notice that a limitation of Revise tracking are changes to type definitions and removal of exports. In early stages of development, when you change the content of your types frequently, that can be an issue. Either restart the REPL after each change of types, or rename them, as illustrated here.\nLet us add some more functionality to our package now.\n\nmodule Mypkg\n\ngreet() = print(\"Hello Earthlings!\")\n\nmutable struct MPoint\n    x::Number\n    y::Number\nend\n# want to add a `+` method: must import all known `+` first\nimport Base.:+\n+(a::MPoint,b::MPoint) = MPoint(a.x + b.x, a.y + b.y)\n\nend # module Mypkg\n\nWe added a custom data type MPoint, and our version of the + function for it. Let’s try it out in the REPL!\n\njulia&gt; a = Mypkg.MPoint(2,3)\nMypkg.MPoint(2, 3)\n\njulia&gt; b = Mypkg.MPoint(3,1)\nMypkg.MPoint(3, 1)\n\njulia&gt; a + b\nMypkg.MPoint(5, 4)\n\nOk, seems to work. Isn’t it a bit annoying that we always have to type Mypkg in front of our functions, though? Does it even work without typing this? What’s the deal here?\n\n\n\n\n\n\n\nTipModule Namespace and Export\n\n\n\n\nBy default, none of the objects (functions, variables, etc) contained in a Module are visible from outside of it.\nThe keyword export xyz will export the name xyz from your package into the scope where it was loaded, hence, make it visible to the outside.\n\n\n\n\nLet’s add export MPoint in our module definition and try again:\n\njulia&gt; a = MPoint(2,3)\nMPoint(2, 3)\n\njulia&gt; b = MPoint(3,1)\nMPoint(3, 1)\n\njulia&gt; a + b\nMPoint(5, 4)\n🎉\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "3 - Optimization"
    ]
  },
  {
    "objectID": "lectures/lecture3.html#creating-a-julia-package-for-your-work",
    "href": "lectures/lecture3.html#creating-a-julia-package-for-your-work",
    "title": "Lecture 3: Optimization 1",
    "section": "",
    "text": "Creating a package is a great way to ensure reproducibility of your work.\nIt helps to make your work shareable.\nIt helps to test the code which makes up your work.\n👉 Let’s do it!\n\n\n\n\nStart julia\ntwo equivalent options:\n\nenter Pkg mode (hit ]), then generate path/to/new/package, or\nsay using Pkg, then Pkg.generate(\"path/to/new/package\")\n\n\nHere is an example:\n\n]    # this jumps into Pkg mode\n\n(@v1.11) pkg&gt; generate Mypkg\n  Generating  project Mypkg:\n    Mypkg/Project.toml\n    Mypkg/src/Mypkg.jl\n\n(@v1.11) pkg&gt; \nDepending on where you started your julia session, there is now a new folder Mypkg:\nshell&gt; ls Mypkg/\nProject.toml  src/\nIt’s a good idea to start a new VSCode window at that folder location. Doing so, you would see this:\n\n\n\nOur julia package\n\n\n\ngreat. Start a julia repl in the usual way.\nNotice how the bottom bar in VSCode indicates that we are in Mypkg env - VScode asked me whether I wanted to change into this. If this is not the case, you won’t be able to load our package:\n\njulia&gt; using Mypkg\nERROR: ArgumentError: Package Mypkg not found in current path.\n- Run `import Pkg; Pkg.add(\"Mypkg\")` to install the Mypkg package.\n\nWe need to switch into the environment of this package before we can load it locally. This is called activate an environment:\n\n]  # in Pkg mode\n(@v1.11) pkg&gt; activate .  # `.` for current directory\n  Activating project at `~/Mypkg`\n\n(Mypkg) pkg&gt;  # hit backspace\n\njulia&gt; using Mypkg\n[ Info: Precompiling Mypkg [c4d85591-a952-48fb-b3d1-49a9454516b2] \nAlternatively, just click on the env indicator in VSCode and choose the current folder.\nGreat, now we can use the functions contained in the package. Let’s see:\njulia&gt; Mypkg.greet()\nHello World!\n\n\n\n\n\n\nNoteCode Loading\n\n\n\nThere are two ways in which we can load code into a running julia session:\n\nBy includeing code - equivalent to copy and pasting code into the REPL, and what happens when we say execute active file in REPL in VSCode. In practice, those mechanisms execute the function include(\"some_file.jl\").\nVia package loading: We import a set of functions contained in a package, via the using or import statements.\n\nNotice how we did not have to say run current file in REPL or similar commands. Saying using Mypkg immediately made our code available in the current session.\n\n\n\n\n\nNext question: How can we now work on the code and investigate it’s changes in the REPL?\n\nWe can obviously execute the current file in the REPL (basically copy and paste the code into the REPL). Again, copy and paste, or include(\"file.jl\"). But that’s cumbersome.\nThere is a great alternative - Revise.jl. Loading this package before you import your package means that Revise will track changes in your source code and expose them immediately in what you see in the REPL. Revise tracks all changes in our code. Let’s go and look the package documentation.\n\nGood, let’s try this out. Restart the REPL in the Mypkg project. First, we add Revise to our package’s environment, so we can always load it.\n] # for pkg mode\n(Mypkg) pkg&gt; add Revise\nNext, let’s load Revise before we import any other code we want to work on:\nusing Revise\nusing Mypkg\nsee again if that works now:\njulia&gt; Mypkg.greet()\nHello World!\n\nGreat! Now let’s open VSCode in that location and make some changes. Like, let’s just change the greet function slightly and save the Mypkg.jl file:\n\ngreet() = print(\"Hello Earthlings!\")\n\nExecute again in the REPL (notice no code loading action necessary on our behalf!)\n\njulia&gt; Mypkg.greet()\nHello Earthlings!\n\nAwesome! So we can change our code and immediately try out it’s effects. Notice that a limitation of Revise tracking are changes to type definitions and removal of exports. In early stages of development, when you change the content of your types frequently, that can be an issue. Either restart the REPL after each change of types, or rename them, as illustrated here.\nLet us add some more functionality to our package now.\n\nmodule Mypkg\n\ngreet() = print(\"Hello Earthlings!\")\n\nmutable struct MPoint\n    x::Number\n    y::Number\nend\n# want to add a `+` method: must import all known `+` first\nimport Base.:+\n+(a::MPoint,b::MPoint) = MPoint(a.x + b.x, a.y + b.y)\n\nend # module Mypkg\n\nWe added a custom data type MPoint, and our version of the + function for it. Let’s try it out in the REPL!\n\njulia&gt; a = Mypkg.MPoint(2,3)\nMypkg.MPoint(2, 3)\n\njulia&gt; b = Mypkg.MPoint(3,1)\nMypkg.MPoint(3, 1)\n\njulia&gt; a + b\nMypkg.MPoint(5, 4)\n\nOk, seems to work. Isn’t it a bit annoying that we always have to type Mypkg in front of our functions, though? Does it even work without typing this? What’s the deal here?\n\n\n\n\n\n\n\nTipModule Namespace and Export\n\n\n\n\nBy default, none of the objects (functions, variables, etc) contained in a Module are visible from outside of it.\nThe keyword export xyz will export the name xyz from your package into the scope where it was loaded, hence, make it visible to the outside.\n\n\n\n\nLet’s add export MPoint in our module definition and try again:\n\njulia&gt; a = MPoint(2,3)\nMPoint(2, 3)\n\njulia&gt; b = MPoint(3,1)\nMPoint(3, 1)\n\njulia&gt; a + b\nMPoint(5, 4)\n🎉",
    "crumbs": [
      "Home",
      "Lectures",
      "3 - Optimization"
    ]
  },
  {
    "objectID": "lectures/lecture3.html#unit-testing",
    "href": "lectures/lecture3.html#unit-testing",
    "title": "Lecture 3: Optimization 1",
    "section": "Unit Testing",
    "text": "Unit Testing\nLet’s take a quick moment to appreciate what we have done just now:\n\nWe added a new feature to our package (added MPoint and +).\nWe (or rather, Revise.jl) updated the loaded code in our REPL.\nWe checked that it works (by typing a series of commands, see above).\n\nWith some imagination, we could call this process unit testing: We added one new aspect (a feature, a unit, a piece,…) to our project, and we tested whether it works as we intended it to work.\n\n\n\n\n\n\nWarningAutomate Testing!\n\n\n\nIn a more complex environment, we will forget how to establish our check of this works. There will be interdepencies between different parts of our code, which we fail to see, and other reasons. We may simple not remember what the setting was when we test this piece of code when we wrote it.\n👉 We should write the test itself down as a piece of code which we regularly execute. Better still: which someone else executes for us.\n\n\n\nTesting\n\nJulia has extensive testing capabilities built in. We need to load the built-in Test library to access the tools. See here in the manual.\nThere is a variety of addon packages which smooth the experience somewhat. I recommend the TestItemRunner.jl package, which nicely integrates with the VSCode environment:\n\n]  # pkg\nadd Test  \nadd TestItemRunner\n\nyou have now access to a basic macro called @test which checks a boolean outcome:\n\njulia&gt; using Test\n\njulia&gt; @test true\nTest Passed\n\njulia&gt; @test false\nTest Failed at REPL[19]:1\n  Expression: false\n\nERROR: There was an error during testing\n\nOk, let’s import the TestItemRunner into our package (not Test!), and let’s write our first TestItem!\n\nmodule Mypkg\n\ngreet() = print(\"Hello Earthlings!\")\n\nusing TestItemRunner  # allows using @testitem\n\nmutable struct MPoint\n    x::Number\n    y::Number\nend\n\nimport Base.:+\n+(a::MPoint,b::MPoint) = MPoint(a.x + b.x, a.y + b.y)\n\n@testitem \"Test MPoint +\" begin\n    x = [rand(1:10) for i in 1:4]\n    A = MPoint(x[1],x[2])\n    B = MPoint(x[3],x[4])\n    C = A + B \n    @test C isa MPoint\n    @test C.x == A.x + B.x\n    @test C.y == A.y + B.y\n    @test C.x == x[1] + x[3]\n    @test C.y == x[2] + x[4]\nend\n\nexport MPoint\nend # module Mypkg\n\nNotice the green play symbol which appears in our VSCode next to the line where the testitem starts. Click it! 😉\n\n\n\nOrganizing Files\n\nOur package is starting to look a bit cluttered by now.\nYou can freely arrange your code over multiple files, which you then include(\"file1.jl\") into your module. Also, let’s move the tests to a dedicated directory. Let’s try to arrange everything into this view in VSCode:\n\n\n\n\nGrowing our package: Here we see the main Module definition including the code for MPoint, we see in the left file browser the structure of the package, and we illustrate how the Project.toml file has evolved so far, keeping track of our dependencies.",
    "crumbs": [
      "Home",
      "Lectures",
      "3 - Optimization"
    ]
  },
  {
    "objectID": "lectures/lecture3.html#debugging",
    "href": "lectures/lecture3.html#debugging",
    "title": "Lecture 3: Optimization 1",
    "section": "Debugging",
    "text": "Debugging\n\nWith debugging we generally mean the ability to step through our code in an interactive fashion to repair bugs 🐛 as they appear in our code. General concepts to know are a debugger (a program which knows how to attach to our actual program), a breakpoint (a location in our code where the program will stop - ideally before an error occurs), and stepping in various forms.\nDebugging simple scripts or packages is the same workflow.\nLet’s add another function to our package now at the bottom of mpoint.jl maybe? An economic model of sorts:\n\nfunction econ_model(; startval = 1.0)\n    # make an Mpoint\n    x = MPoint(startval, startval-0.5)\n    # ... and evaluate a utility function\n    MPoint(log(x.x),log(x.y))\nend\n\nMake sure to try out that it works.\n\njulia&gt; Mypkg.econ_model()\nMypkg.MPoint(0.0, -0.6931471805599453)\n\nOk great. Now what about that? Try it out!\n\njulia&gt; Mypkg.econ_model(startval = 0.3)\n\nError. Good. 😜 Let’s pretend we don’t know what’s going on and we need to investigate this function more in detail.\n\n\nDebugging Strategies\n\nAdd println statements: simplest is to just print output along the way, before an error occurs.\nUse the Logging module. Add @debug statements. This is preferable, because you can leave the @debug statements in your code without any performance implication. Logging works as follows:\n\ninsert debug statements in your code: @info, @warn, @debug etc\ncreate a logger at a certain logging level\nrun code\n\njulia&gt; using Logging  # loads the standard logger at Info level\n\njulia&gt; @info \"just for info\"\n[ Info: just for info\n\njulia&gt; @debug \"whoaa, this looks suspicious! 😬\"\nNotice that this prints nothing! Let’s use debug logger instead for this one:\njulia&gt; with_logger(ConsoleLogger(stdout, Logging.Debug)) do\n           @debug \"whoaa, this looks suspicious! 😬\"\n       end\n┌ Debug: whoaa, this looks suspicious! 😬\n└ @ Main REPL[30]:2\nWe can set the global_logger to capture all messages like this:\nglobal_logger(ConsoleLogger(stdout, Logging.Debug)) # Logging.Debug sets level to `Debug`\nold_log = global_logger(debug_logger)  # returns previous logger, so can set back later.\nUse an actual debugger to step through our code.\n\nVSCode exports by default the @enter macro. type: @enter Mypkg.econ_model(startval = -0.3)\nclick on the play symbol. program hits an error.\nset a break point just before\nclick on replay.",
    "crumbs": [
      "Home",
      "Lectures",
      "3 - Optimization"
    ]
  },
  {
    "objectID": "lectures/lecture3.html#some-julia-bootcamp-stuff",
    "href": "lectures/lecture3.html#some-julia-bootcamp-stuff",
    "title": "Lecture 3: Optimization 1",
    "section": "Some Julia-Bootcamp stuff",
    "text": "Some Julia-Bootcamp stuff\n\n\n\nTopic\nNotebook\n\n\n\n\nIntro to Macros\nclick for notebook\n\n\nIntro to Differential Equations\nclick for notebook\n\n\nPlotting with Plots.jl\nclick for notebook\n\n\nPlotting with Makie.jl\nclick for website\n\n\nInteractive\nclick for notebook",
    "crumbs": [
      "Home",
      "Lectures",
      "3 - Optimization"
    ]
  },
  {
    "objectID": "lectures/lecture3.html#optimization-finally",
    "href": "lectures/lecture3.html#optimization-finally",
    "title": "Lecture 3: Optimization 1",
    "section": "Optimization, Finally!",
    "text": "Optimization, Finally!\n\n\n\nTopic\nNotebook\n\n\n\n\nReview of Optimization Algorithms\ndownload notebook",
    "crumbs": [
      "Home",
      "Lectures",
      "3 - Optimization"
    ]
  },
  {
    "objectID": "lectures/absorbing-mc.html",
    "href": "lectures/absorbing-mc.html",
    "title": "Absorbing Markov Chains",
    "section": "",
    "text": "This elaborates on Bogumił’s blog post about the problem of generating a sequence of coin tosses. We will add some theory to his simulation study.\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "Absorbing Markov Chains"
    ]
  },
  {
    "objectID": "lectures/absorbing-mc.html#problem-statement",
    "href": "lectures/absorbing-mc.html#problem-statement",
    "title": "Absorbing Markov Chains",
    "section": "Problem Statement",
    "text": "Problem Statement\nTo recap, we have two players Alice (A) and Bob (B), and each is equipped with fair coin. Each plays a game whereby they toss the coin, and record the outcome as either heads (H) or tails (T). An example of a sequence after 9 coin tosses for each player could look like this:\ntoss: 123456789\nA:    TTHHHTHTH\nB:    THTTHTHHT\nAlice and Bob have different ways of winning this game. A wins if the sequence HT occurs, and B wins if the sequence HH occurs. Following the above example, A would have won after toss number 6. B had to wait until toss number 8 complete his winning sequence HH, hence he lost this particular contest.",
    "crumbs": [
      "Home",
      "Lectures",
      "Absorbing Markov Chains"
    ]
  },
  {
    "objectID": "lectures/absorbing-mc.html#questions",
    "href": "lectures/absorbing-mc.html#questions",
    "title": "Absorbing Markov Chains",
    "section": "Questions",
    "text": "Questions\n\nIn expectation, who is more likely to win this game? Suppose we performed a very large number of contests like the one illustrated above.\nHow many coin tosses does Alice have to perform, on average, until her winning sequence shows up?\nHow many tosses does Bob have to perform until his winning sequence appears?",
    "crumbs": [
      "Home",
      "Lectures",
      "Absorbing Markov Chains"
    ]
  },
  {
    "objectID": "lectures/absorbing-mc.html#simulations",
    "href": "lectures/absorbing-mc.html#simulations",
    "title": "Absorbing Markov Chains",
    "section": "Simulations",
    "text": "Simulations\nLet us follow in Bogumił’s foot steps and re-propose the simulation study he did on his blog.\nWho is more likely to win this game? We will later work out the probability that each will win, but we can easily to a simulation study following a frequentist approach: just count how many times each player wins!\nLet’s create a function which will simulate us tossing an imaginary coin until one of both players wins. The function should return the initial of the winning player.\n\nfunction whowins()\n    t1 = rand(('H', 'T'))  # toss number 1 \n    while true  # keep going until we get a valid sequence.\n        t2 = rand(('H', 'T'))  # toss number 2 \n        if t1 == 'T'  # invalid first toss for both.\n            t1 = t2   # reassign t2 to t1 and keep going\n        else  # t1 is 'H' ! t2 now decides the winner!\n            return t2 == 'H' ? \"B\" : \"A\"\n        end\n    end\nend\n\nwhowins (generic function with 1 method)\n\n\nYou should try it out a few times to convince yourself it’s working.\n\nSimulating the Expected Probability of Winning\nNow, lets see how often each of them wins if we repeat this contest many times over:\n\nusing Random, FreqTables\nRandom.seed!(10101)\n\nfreqtable( [whowins() for _ in 1:100_000_000] )\n\n2-element Named Vector{Int64}\nDim1  │ \n──────┼─────────\nA     │ 50003096\nB     │ 49996904\n\n\nOk, that would tell us that if we were to have them play a close to infinite number of times, each would win pretty much exactly half the games.\n\n\nSimulating expected number of steps till winning\nThis is fairly easy to simulate, given our above code.\n\nfunction count_alice()\n    t1 = rand(('H', 'T'))  # toss number 1 \n    tosses = 1  # counter\n    while true  # keep going until we get a winner: HT\n        t2 = rand(('H', 'T'))  # toss number 2 \n        tosses += 1 \n        # if sequence is correct, return, else re-do\n        t1 == 'H' && t2 == 'T' && return tosses\n\n        # if did not return above, keep going\n        t1 = t2\n    end\nend\n\ncount_alice (generic function with 1 method)\n\n\nYou should try to implement count_bob() yourself now.\n\n\nCode\nfunction count_bob()\n    t1 = rand(('H', 'T'))  # toss number 1 \n    tosses = 1  # counter\n    while true  # keep going until we get a winner: HH\n        t2 = rand(('H', 'T'))  # toss number 2 \n        tosses += 1 \n        # if sequence is correct, return, else re-do\n        t1 == 'H' && t2 == 'H' && return tosses\n\n        # if did not return above, keep going\n        t1 = t2\n    end\nend\n\n\ncount_bob (generic function with 1 method)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nDo you expect the count_* functions to return the same number each time you call them, or not?\nIf not, which function seems to return smaller numbers on average?\n\n\n\nThe last one is an easy question to answer with a simulation study again, isn’t it? This time it’s not about counting who wins how many times, however, but rather…🤔\n\n\nCode\nusing Statistics  # for mean()\n\n(\n    Alice = mean( [count_alice() for _ in 1:100_000_000] ),\n    Bob = mean( [count_bob() for _ in 1:100_000_000] )\n)\n\n\n(Alice = 3.99997351, Bob = 5.99957595)\n\n\n…wait, this is surprising, not? Alice has to toss 4 times on average, while Bob has to wait for 6 tosses until his winning sequence comes up? How come?",
    "crumbs": [
      "Home",
      "Lectures",
      "Absorbing Markov Chains"
    ]
  },
  {
    "objectID": "lectures/absorbing-mc.html#the-absorbing-transition-matrix-p_a",
    "href": "lectures/absorbing-mc.html#the-absorbing-transition-matrix-p_a",
    "title": "Absorbing Markov Chains",
    "section": "The Absorbing Transition Matrix \\(P_a\\)",
    "text": "The Absorbing Transition Matrix \\(P_a\\)\nIn this setting we have \\(t\\) transient states (i.e. states from which you can escape), and \\(r\\) absorbing states - once you enter such a state, you stay there, in other words, the chain ends.\nWe have to re-write our transition matrix for this case. It will look as follows:\n\\[P_a = \\left[\\begin{array}{cc}\nQ & R \\\\\n\\mathbf{0} & \\mathbf{I}_r\n\\end{array}\\right] \\tag{2}\\]\nwhere\n\n\\(Q\\) is the \\((t,t)\\) transition matrix of transient states\n\\(R\\) is a \\((t,r)\\) matrix of transition probabilities from transient state \\(t\\) into absorbing state \\(r\\)\n\\(\\mathbf{0}\\) is an \\((r,t)\\) matrix of zeros, and\n\\(\\mathbf{I}_r\\) is the \\((r,r)\\) identity matrix.\n\nLet’s work this out for Alice now. Imagine that we want to fill out each of her 2 empty slots for strings with either H or T. Remember that her winning sequence was HT. It’s easiest to model Alice in three possible states:\n\nState e: She has an empty sequence, {}. She is waiting to fill this with two valid strings.\nState H: Her sequence has H in first position, i.e. looks like {H}. This is great, because if the next coin toss yields a T, she wins! If she gets another H, no problem, she stays at state {H} (we can just throw away the previous H in the sequence!).\nState HT: She wins the game.\n\nSo, we define the following transient states for her:\n\ne: empty sequence i.e {}\nH: the string sequence containing only H, i.e. {H}\n\nNext, we define the absorbing state for her:\n\nHT: upon tossing T after a H, i.e completing the sequence {HT}, the game ends.\n\n\n\n\n\n\n\nTipObservations\n\n\n\n\nIn this example we have t=2 transient states\nT in first position is not a valid sequence for Alice. Whilst we could think of {T} as a transient state, it is easier to think that we go back to e and start again if we hit T in the first toss.\nThere is a single absorbing state, {HT}, i.e. r=1.\n\n\n\nThe situation for Alice is as follows:\n\n\n\n\n\ngraph LR\n    e((e)) --&gt; |0.5| e\n    e((e)) --&gt; |0.5| H\n    H      --&gt; |0.5| H\n    H      --&gt; |0.5| HT\n\n\n Absorbing Markov Chain of Alice \n\n\n\nNotice how we assume that if your first toss is invalid (i.e. a T), you stay with your empty sequence. Next, here is the transition matrix \\(Q_a\\) for her transient states {e,H} (putting e in the first row/column), and the (2,1) matrix of transitions from transient into absorbing state, \\(R_a\\):\n\\[Q_a = \\left[\\begin{array}{cc}\n0.5 & 0.5 \\\\\n0   & 0.5\n\\end{array}\\right],\\quad\nR_a = \\left[\\begin{array}{c}\n0 \\\\\n0.5\n\\end{array}\\right]\n\\tag{3}\\]\nLet us assemble those into \\(P_a(\\text{Alice})\\) now, and observe that this maintains the properties of stochastic matrix \\(P\\):\n\\[P_a(\\text{Alice}) = \\left[\\begin{array}{ccc}\n0.5 & 0.5 & 0 \\\\\n0   & 0.5 & 0.5 \\\\\n0   &  0  &  1\n\\end{array}\\right]\n\\tag{4}\\]",
    "crumbs": [
      "Home",
      "Lectures",
      "Absorbing Markov Chains"
    ]
  },
  {
    "objectID": "lectures/absorbing-mc.html#the-fundamental-matrix-n",
    "href": "lectures/absorbing-mc.html#the-fundamental-matrix-n",
    "title": "Absorbing Markov Chains",
    "section": "The Fundamental Matrix \\(N\\)",
    "text": "The Fundamental Matrix \\(N\\)\nThe next piece we need is called the fundamental matrix. It will give us the expected number of visits to transient state \\(j\\) before we reach an absorbing state. It is obtained by iterating \\(Q_a\\) forward until infinity. It can be shown that it looks like this, similar to the infinite sum of a geometric series:\n\\[\nN = \\sum_{k=0}^\\infty Q^k = (I - Q)^{-1}\n\\]\nSo, for Alice, it looks as follows:\n\\[\nN_a = (I - Q_a)^{-1} = \\left( \\left[\\begin{array}{cc}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right] - \\left[\\begin{array}{cc}\n0.5 & 0.5 \\\\\n0   & 0.5\n\\end{array}\\right] \\right)^{-1} = \\left(\\left[\\begin{array}{cc}\n0.5 & -0.5 \\\\\n0   & 0.5\n\\end{array}\\right] \\right)^{-1}\n\\]\nDo we all still remember how to compute the inverse of a 2x2 matrix? I dont’t, but here is a formula 🙈\n\n\n\n\n\n\nNoteInverse of a 2x2 matrix\n\n\n\nFor \\(A = \\left[\\begin{array}{cc}\na & b \\\\\nc   & d\n\\end{array}\\right]\\) we have that the inverse matrix of \\(A\\) is\n\\[A^{-1} = \\frac{1}{ad - bc} \\left[\\begin{array}{cc}\nd & -b \\\\\n-c   & a\n\\end{array}\\right]\\]\n\n\nWith that out of the way, you can calculate that\n\\[\nN_a =  \\left[\\begin{array}{cc}\n2 & 2 \\\\\n0 & 2\n\\end{array}\\right]\n\\tag{5}\\]\nWe can immediately get the expected number of steps before being absorbed from this by summing across rows:\n\\[\nN_a \\mathbf{1}_t =  \\left[\\begin{array}{c}\n4 \\\\\n2\n\\end{array}\\right]\n\\tag{6}\\]\nSo, starting in state e, the expected number of steps before reaching absorbing state HT is \\(4\\) for Alice. Once in state H, she expects only two more steps. Here a step is a toss of a coin.",
    "crumbs": [
      "Home",
      "Lectures",
      "Absorbing Markov Chains"
    ]
  },
  {
    "objectID": "lectures/absorbing-mc.html#states-and-transitions-for-bob",
    "href": "lectures/absorbing-mc.html#states-and-transitions-for-bob",
    "title": "Absorbing Markov Chains",
    "section": "States and Transitions for Bob?",
    "text": "States and Transitions for Bob?\nFor Bob, the situation is slightly different. Remember that Bob wins if HH comes up. For him, we have the following definition of states:\n\nState e: The empty sequence, {}. Waiting to fill this with two valid strings.\nState H: The sequence has H in first position, i.e. looks like {H}.\nState HH: He wins the game.\n\nNotice the one big difference here: If Bob is in state {H}, and hits a T in the next toss, he is not as fortunate as Alice, who can stay in the same state. Bob’s problem is that he does not have a winning sequence that could start with T (neither does Alice, but upon hitting T while in state {H}, she wins!)\nHere’s the graphs for Bob and Alice side by side:\n\n\n\n\n\n\n\ngraph LR\n    e((e)) --&gt; |0.5| e\n    e((e)) --&gt; |0.5| H\n    H      --&gt; |0.5| e\n    H      --&gt; |0.5| HH\n\n\n\n Markov Chain of Bob \n\n\n\n\n\n\n\n\n\ngraph LR\n    ea((e)) --&gt; |0.5| ea\n    ea((e)) --&gt; |0.5| Ha[H]\n    Ha[H]      --&gt; |0.5| Ha\n    Ha[H]      --&gt; |0.5| HT\n\n\n Markov Chain of Alice \n\n\n\n\n\nThe problem for Bob is the arrow going back to e if he hits a bad toss at state {H}. Let us quickly assemble the transition matrix for Bob, and compute his expected steps:\n\\[Q_b = \\left[\\begin{array}{cc}\n0.5 & 0.5 \\\\\n0.5 & 0\n\\end{array}\\right],\\quad\nR_b = \\left[\\begin{array}{c}\n0 \\\\\n0.5\n\\end{array}\\right]\n\\tag{7}\\]\n\\[P_a(\\text{Bob}) = \\left[\\begin{array}{ccc}\n0.5 & 0.5 & 0 \\\\\n0.5   & 0 & 0.5 \\\\\n0   &  0  &  1\n\\end{array}\\right]\n\\tag{8}\\]\nWith those in hand, you can easily verify that Bob will have\n\\[\nN_b =  \\left[\\begin{array}{cc}\n4 & 2 \\\\\n2 & 2\n\\end{array}\\right]\n\\tag{9}\\]\nwhich, in turn, means that his expected steps until absorbing state, starting from either {} or {H} are given by\n\\[\nN_b \\mathbf{1}_t =  \\left[\\begin{array}{c}\n6 \\\\\n4\n\\end{array}\\right]\n\\tag{10}\\]\n👉 exactly as in our (Bogumił’s!) simulation experiment from the start.",
    "crumbs": [
      "Home",
      "Lectures",
      "Absorbing Markov Chains"
    ]
  },
  {
    "objectID": "lectures/lecture7.html",
    "href": "lectures/lecture7.html",
    "title": "Lecture 7: Parallel Computing",
    "section": "",
    "text": "You hear often: “sure, that’s a big problem you have there. But you can parallelize it!” Whether that will help in your case is strictly problem-specific. You need to try it out in order to know.\nFirst, you want to make sure your application produces correct results. You test it.\nThen you try to make it as efficient as possible on a single process, i.e. in serial mode. There are a great, many things to know about how to write efficient code; starting with the hardware you dispose of\nFinally, you can attempt to scale your application to more than one processes. That’s what we’ll talk about today.\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "7 - Parallel Computing"
    ]
  },
  {
    "objectID": "lectures/lecture7.html#caveats-first",
    "href": "lectures/lecture7.html#caveats-first",
    "title": "Lecture 7: Parallel Computing",
    "section": "",
    "text": "You hear often: “sure, that’s a big problem you have there. But you can parallelize it!” Whether that will help in your case is strictly problem-specific. You need to try it out in order to know.\nFirst, you want to make sure your application produces correct results. You test it.\nThen you try to make it as efficient as possible on a single process, i.e. in serial mode. There are a great, many things to know about how to write efficient code; starting with the hardware you dispose of\nFinally, you can attempt to scale your application to more than one processes. That’s what we’ll talk about today.",
    "crumbs": [
      "Home",
      "Lectures",
      "7 - Parallel Computing"
    ]
  },
  {
    "objectID": "lectures/lecture7.html#general-points",
    "href": "lectures/lecture7.html#general-points",
    "title": "Lecture 7: Parallel Computing",
    "section": "General Points",
    "text": "General Points\n\nParallel computation can be beneficial strategy to speed up long running computational tasks, like for instance computing the solution to your economic model.\nThe gains from parallel computation depend on how much communication (data transfer) a certain task involves: transferring large amounts of data to and from different compute units takes time.\nThe largest HPC systems in the world connect hundreds of thousands of computers into a compute cluster\nThe smallest parallel computation unit lives on your CPU.\nThe basic idea is simple: If a computational task of duration M can split into N subtasks, which can be performed concurrently (at the same time), without interfering with each other, then in theory, the duration of the task should fall to M/N. In practice we almost never achieve this theoretical bound, because of time spent communicating between computational units and other system time.",
    "crumbs": [
      "Home",
      "Lectures",
      "7 - Parallel Computing"
    ]
  },
  {
    "objectID": "lectures/lecture7.html#resources",
    "href": "lectures/lecture7.html#resources",
    "title": "Lecture 7: Parallel Computing",
    "section": "Resources",
    "text": "Resources\n\nJulia whitepaper\nGuide by Jesus Fernandez-Villaverde and David Zurrak-Valencia\nParallel Datascience tutorial",
    "crumbs": [
      "Home",
      "Lectures",
      "7 - Parallel Computing"
    ]
  },
  {
    "objectID": "lectures/lecture7.html#definitions",
    "href": "lectures/lecture7.html#definitions",
    "title": "Lecture 7: Parallel Computing",
    "section": "Definitions",
    "text": "Definitions\n\ntask: a unit of work to be executed\nthread: sequences of instructions that can be excuted by a CPU core\ncore: an individual processing unit within a CPU that can perform tasks independently\nmachine: individual computer with own hardware",
    "crumbs": [
      "Home",
      "Lectures",
      "7 - Parallel Computing"
    ]
  },
  {
    "objectID": "lectures/lecture7.html#parallel-paradigms-achievable-with-julia",
    "href": "lectures/lecture7.html#parallel-paradigms-achievable-with-julia",
    "title": "Lecture 7: Parallel Computing",
    "section": "Parallel Paradigms achievable with julia",
    "text": "Parallel Paradigms achievable with julia\n\nAsyncronous Tasks or Coroutines\nMulti-Threading\nDistributed Computing\nGPU Computing\n\nParadigms 1. and 2. live in a shared memory world, i.e. they operate within the same physical infrastructure: your computer has a CPU, and a block of RAM where data is stored for computation. You do not have to worry about your compute unit, one of your thread’s for example, not being able to access information in RAM.\nParadigms 3. and 4. are different, because they (can) rely on separate hardware. We can start a distributed julia process inside a single machine, but this will behave like another computer (almost), so you will have to worry about making data and functions accessible to all worker processes.",
    "crumbs": [
      "Home",
      "Lectures",
      "7 - Parallel Computing"
    ]
  },
  {
    "objectID": "lectures/lecture7.html#asyncronous-tasks-or-coroutines",
    "href": "lectures/lecture7.html#asyncronous-tasks-or-coroutines",
    "title": "Lecture 7: Parallel Computing",
    "section": "Asyncronous Tasks or Coroutines",
    "text": "Asyncronous Tasks or Coroutines\nEven within a single thread there may be computational operations which can be performed next to each other. Coroutines split a single task into multiple chunks, and there is an efficient process which allows non-blocking execution. This may be beneficial in I/O operations or when we wait for something to arrive via a network. It is not beneficial if our operation involvs many CPU cycles (i.e. if it’s compute intensive). Let’s see an example.\n\n# make sure we only have a single thread\n# this is default startup behaviour\nThreads.nthreads()\n\n1\n\n\nSuppose we have this long running operation (waiting for a server to respond, or something else to arrive):\n\nfunction long_process()\n    sleep(3)\n    return 42 # the answer to life, the universe, and everything\nend\n\nlong_process (generic function with 1 method)\n\n\nNow let’s run that function three times.\n\n@elapsed begin\n    p1 = long_process()\n    p2 = long_process()\n    p3 = long_process()\n    (p1, p2, p3)\nend\n\n9.005572291\n\n\nSo this takes roughly 9 seconds, as expected.\nLook at how we wrote the above program, pretty standard. We do one task after the other (nevermind that here it’s the same task three times - irrelevant). It’s how you think a computer works, no? 🤔 You think it does one thing after the other.\nWell…that’s not always accurate. Check this out from the manual. You can think of a Task as a handle to a unit of computational work to be performed. It has a create-start-run-finish lifecycle. Tasks are created by calling the Task constructor on a 0-argument function to run, or using the @task macro:\n\nt = @task begin; sleep(5); println(\"done\"); end\n\nTask (runnable) @0x000000010b20c330\n\n\nthat thing will sleep for 5 seconds and then print done. but why is nothing happening?\nwell, we just created the task, which is runnable. we havne’t run it yet! We run something by scheduling it to be run. Like this:\njulia&gt; schedule(t);\n\njulia&gt; \nnotice how the prompt returns so we can do other stuff in the meantime…and how we get a done after 5 seconds!\nok, back to our example:\n\nt0 = @elapsed begin\n    p1 = long_process()\n    p2 = long_process()\n    p3 = long_process()\n    (p1, p2, p3)\nend\n\nt0\n\n9.006291167\n\n\nNow, check this out. Let’s write this as tasks that should get scheduled.\n\nt1 = @elapsed begin\n    t1 = Task(long_process); schedule(t1)\n    t2 = Task(long_process); schedule(t2)\n    t3 = Task(long_process); schedule(t3)\n    (fetch(t1), \n    fetch(t2), \n    fetch(t3)) # fetch triggers execution\nend\n\n3.009115\n\n\n🤯 3 seconds?! Keep in mind that\n\nThreads.nthreads()\n\n1\n\n\nWhat happened?\nThe Task means that each job t1,t2,t3 was started as a separate unit of work. Not as we thought, that defining t2 on the line below t1 would by definition mean that one runs after the other. No, given those are Tasks means that the scheduler is free to allocate those chunks of work to whichever resources are currently free on your CPU. Here is a picture from the whitepaper:\n\n\n\nfrom julia whitepaper\n\n\nYou can see that julia makes it extremely easy to run concurrent tasks. Check out more in the manual: https://docs.julialang.org/en/v1/manual/parallel-computing/",
    "crumbs": [
      "Home",
      "Lectures",
      "7 - Parallel Computing"
    ]
  },
  {
    "objectID": "lectures/lecture7.html#multi-threading",
    "href": "lectures/lecture7.html#multi-threading",
    "title": "Lecture 7: Parallel Computing",
    "section": "Multi-Threading",
    "text": "Multi-Threading\n\nWhat are threads again? Threads are sequences of instructions given to the CPU which can be executed concurrently. So, multithreading is something that happens on a core of CPU. Your CPU may have several cores.\nThe manual is the authorative source - very comprehensive.\n\nFirst question you should ask: how many cores does my computer have?\n\nlength(Sys.cpu_info())\n\n10\n\n\n\n\n\nphysical cores\n\n\nOk, next question: how to start julia with multiple threads?\n# starting julia on the command line with the --threads flag\nfloswald@PTL11077 ~&gt; julia --threads=4\n\njulia&gt; Threads.nthreads()\n4\n\njulia&gt; \nalternatively you could set the environment variable JULIA_NUM_THREADS=4 or use the auto function to choose the best number.\nfloswald@PTL11077 ~&gt; julia --threads=auto\n\njulia&gt; Threads.nthreads()\n8\nFinally, in your VScode, you can edit the file settings.json (command palette and type settings.json) and add \"julia.NumThreads\": 4 - this will start the VSCode REPL with 4 threads each time, \"julia.NumThreads\": \"auto\" will set the flag to auto etc.\nAlright, let’s do it finally.\nusing Base.Threads\n\njulia&gt; for i = 1:16\n           println(\"Hello from thread \", threadid())\n       end\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nHello from thread 1\nSo far, so good. Let’s evaluate this loop now over our available threads. Easy with the @threads macro:\njulia&gt; @threads for i = 1:16\n           println(\"Hello from thread \", threadid())\n       end\nHello from thread 2\nHello from thread 4\nHello from thread 1\nHello from thread 1\nHello from thread 8\nHello from thread 2\nHello from thread 4\nHello from thread 3\nHello from thread 8\nHello from thread 6\nHello from thread 5\nHello from thread 3\nHello from thread 7\nHello from thread 6\nHello from thread 7\nHello from thread 5\nOr, to come back to the above example:\njulia&gt; @elapsed begin\n           @threads for _ in 1:3\n               long_process()\n           end\n       end\n3.030319708\n🤔 How could we prove that are executing something on multiple threads? Let’s use the @spawn macro to distribute a piece of code over available threads. Let’s make the task an infinite loop that runs for as long as the variable x is equal to 1. Check out how the prompt returns immediately.\njulia&gt; x = 1\n1\n\njulia&gt; @spawn begin\n           while x == 1\n                # infinite loop \n           end\n           println(\"infinite loop done\")\n       end\nTask (runnable) @0x000000010dcdddc0\n\njulia&gt; \nWell, why, that inifinite loop is running on one of my 4 threads now:\n\n\n\nphysical cores\n\n\nuntil we kill it by saying\njulia&gt; x = 3\ninfinite loop done\n3\n\n\n\nphysical cores\n\n\nThe difference between @threads and @spawn is that the former works well for balanced loops (same amount of work load for each iteration) while the latter works better for unbalanced workloads. Example, shamelessly stolen from Jeff Bezanon’s multithreading webinar - Let us compute the Mandelbrot set. The computations are very different at different iterations, hence the workload is super unbalanced. Some threads will have a ton of work, others, very little.\njulia&gt; \"\"\"\n           mandelbrot set escape time algorithm\n\n           https://en.wikipedia.org/wiki/Plotting_algorithms_for_the_Mandelbrot_set\n\n       \"\"\"\n       function escapetime(z; maxiter = 80)\n           c = z\n           for n = 1:maxiter\n               if abs(z) &gt; 2\n                   return n-1\n               end\n               z = z^2 + c\n           end\n           return maxiter\n       end\nescapetime\nand here is a function that computes the whole set by using this function:\njulia&gt; function mandel(; width = 1600, height = 1200, maxiter = 500)\n           out = zeros(Int,height, width)\n           real = range(-2.0,0.5,length=width)\n           imag = range(-1.0,1.0, length=height)\n           for x in 1:width\n               for y in 1:height\n                   z = real[x] + imag[y]*im \n                   out[y,x] = escapetime(z,maxiter = maxiter)\n               end\n           end\n           return out\n       end\nmandel (generic function with 1 method)\nLet’s run it in serial model, i.e. no multi-threading at all:\njulia&gt; # time it\n       m_serial = @elapsed m = mandel()\n1.565549583\n\n\n# plot it\nimg = Gray.((m .% 500) ./ 100)\n\nalright, time to parallelize this over our threads.\nlet’s try and parallelize this now.\njulia&gt; function mandel_outer_thread(; width = 1600, height = 1200, maxiter = 500)\n           out = zeros(Int,height, width)\n           real = range(-2.0,0.5,length=width)\n           imag = range(-1.0,1.0, length=height)\n           # we parellize over the x direction\n           @threads for x in 1:width     \n               for y in 1:height\n                   z = real[x] + imag[y]*im \n                   out[y,x] = escapetime(z,maxiter = maxiter)\n               end\n           end\n           return out\n       end\nmandel_outer_thread (generic function with 1 method)\n\njulia&gt; m_thread = @elapsed mandel_outer_thread();\n\njulia&gt; m_serial / m_thread\n3.3293004734319482\nok, about a 3x speedup. For 8 threads! That’s far from linear scaling. problem here is how the julia scheduler assigns jobs to threads in this loop. by default it gives the same number of jobs to each thread. But in this example you see that some iterations are much more labor intensive than others. So that’s not great.\nLet’s split the work along the columns of the out matrix and lets use @spawn, which will assign tasks to threads as they become available\njulia&gt; function mandel_spawn(; width = 1600, height = 1200, maxiter = 500)\n           out = zeros(Int,height, width)\n           real = range(-2.0,0.5,length=width)\n           imag = range(-1.0,1.0, length=height)\n           # we want to wait in the end for all threads to finish\n           @sync for x in 1:width \n                # here we say 'distribute columns to free threads' \n                @spawn for y in 1:height  \n                    z = real[x] + imag[y]*im \n                    out[y,x] = escapetime(z,maxiter = maxiter)\n               end\n           end\n           return out\n       end\nmandel_spawn (generic function with 1 method)\n\njulia&gt; m_spawn = @elapsed mandel_spawn();\n\njulia&gt; m_serial / m_spawn\n6.243123492042317\nOk, that’s better!\nIn general, I have found multithreading to work well at a level of a workload of a few seconds. I.e. if you can choose at which level of a nested loop to put the @threads macro, the tradeoff between sending data across threads and taking advantage of parallelization for me seemed optimal if the that task takes a few seconds. That said, any such statements are very context specific, and getting good performance out of threaded models takes a good amount of experimentation, in my experience.\nx = [1,2,3]\nlck = Threads.SpinLock()\n\nThreads.@threads for i in 1:100\n    position = i % 3 + 1\n    lock(lck) do\n        x[position] += 1\n    end\nend\n\nData Race Conditions\njulia&gt; function sum_single(a)\n           s = 0\n           for i in a\n               s += i\n           end\n           s\n       end\nsum_single (generic function with 1 method)\n\njulia&gt; sum_single(1:1_000_000)\n500000500000\n\njulia&gt; function sum_multi_bad(a)\n           s = 0\n           Threads.@threads for i in a\n               s += i\n           end\n           s\n       end\nsum_multi_bad (generic function with 1 method)\n\njulia&gt; sum_multi_bad(1:1_000_000)\n70140554652\nThat’s clearly wrong, and worse yet, it will be wrong in a different way each time you run it (because thread availability changes). We could instead assign chunks of work to each thread and the collect in the end:\njulia&gt; function sum_multi_good(a)\n           chunks = Iterators.partition(a, length(a) ÷ Threads.nthreads())\n           tasks = map(chunks) do chunk\n               Threads.@spawn sum_single(chunk)\n           end\n           chunk_sums = fetch.(tasks)\n           return sum_single(chunk_sums)\n       end\nsum_multi_good (generic function with 1 method)\n\njulia&gt; sum_multi_good(1:1_000_000)\n500000500000\n\n\nReal Life Example of Data Race\nIn this research project - [private repo] we encountered a situation very similar to the above.",
    "crumbs": [
      "Home",
      "Lectures",
      "7 - Parallel Computing"
    ]
  },
  {
    "objectID": "lectures/lecture7.html#distributed-computing",
    "href": "lectures/lecture7.html#distributed-computing",
    "title": "Lecture 7: Parallel Computing",
    "section": "Distributed Computing",
    "text": "Distributed Computing\nThe manual is again very helpful here.\nLet’s start a new julia session now, without multithreading, but with multiple processes.\nfloswald@PTL11077 ~&gt; julia -p 2                                                                                                                                                 (base) \n\n# -p implicitly loads the `Distributed` module\n\njulia&gt; nprocs()\n3\n\njulia&gt; workers()\n2-element Vector{Int64}:\n 2\n 3\nSo, by default we have process number 1 (which is the one where we type stuff into the REPL, i.e. where we interact). Then, we said -p 2 meaning add 2 processes, that is why workers() shows ids 2 and 3.\nDistributed programming in Julia is built on two primitives: remote references and remote calls. A remote reference is an object that can be used from any process to refer to an object stored on a particular process. A remote call is a request by one process to call a certain function on certain arguments on another (possibly the same) process.\nIn the manual you can see a low level API which allows you to directly call a function on a remote worker, but that’s most of the time not what you want. We’ll concentrate on the higher-level API here. One big issue here is:\n\nCode and Data Availability\nWe must ensure that the code we want to execute is available on the process that runs the computation. That sounds fairly obvious. But now try to do this. First we define a new function on the REPL, and we call it on the master, as usual:\njulia&gt; function new_rand(dims...)\n           return 3 * rand(dims...)\n       end\nnew_rand (generic function with 1 method)\n\njulia&gt; new_rand(2,2)\n2×2 Matrix{Float64}:\n 0.407347  2.23388\n 1.29914   0.985813\nNow, we want to spawn running of that function on any available process, and we immediately fetch it to trigger execution:\njulia&gt; fetch(@spawnat :any new_rand(2,2))\nERROR: On worker 3:\nUndefVarError: `#new_rand` not defined\nStacktrace:\nIt seems that worker 3, where the job was sent with @spawnat, does not know about our function new_rand. 🧐\nProbably the best approach to this is to define your functions inside a module, as we already discussed. This way, you will find it easy to share code and data across worker processes. Let’s define this module in a file in the current directory. let’s call it DummyModule.jl:\nmodule DummyModule\n\nexport MyType, new_rand\n\nmutable struct MyType\n    a::Int\nend\n\nfunction new_rand(dims...)\n    return 3 * rand(dims...)\nend\n\nprintln(\"loaded\") # just to check\n\nend\nRestart julia with -p 2. Now, to load this module an all processes, we use the @everywhere macro. In short, we have this situation:\nfloswald@PTL11077 ~/compecon&gt; ls                                 \nDummyModule.jl\nfloswald@PTL11077 ~/compecon&gt; julia -p 2                                                                    \n\njulia&gt; @everywhere include(\"DummyModule.jl\")\nloaded     # message from process 1\n      From worker 2:    loaded   # message from process 2\n      From worker 3:    loaded   # message from process 3\n\njulia&gt; \nNow in order to use the code, we need to bring it into scope with using. Here is the master process:\njulia&gt; using .DummyModule   # . for locally defined package\n\njulia&gt; MyType(9)\nMyType(9)\n\njulia&gt; fetch(@spawnat 2 MyType(9))\nERROR: On worker 2:\nUndefVarError: `MyType` not defined\nStacktrace:\n\njulia&gt; fetch(@spawnat 2 DummyModule.MyType(7))\nMain.DummyModule.MyType(7)\nAlso, we can execute a function on a worker. Notice, remotecall_fetch is like fetch(remotecall(...)), but more efficient:\n# calls function new_rand from the DummyModule\n# on worker 2\n# 3,3 are arguments passed to `new_rand`\njulia&gt; remotecall_fetch(DummyModule.new_rand, 2, 3, 3)\n3×3 Matrix{Float64}:\n 0.097928  1.93653  1.16355\n 1.58353   1.40099  0.511078\n 2.11274   1.38712  1.71745\n\n\nData Movement\nI would recommend\n\nto keep data movement to a minimum\navoid global variables\ndefine all required data within the module you load on the workers, such that each of them has access to all required data. This may not be feasible if you require huge amounts of input data.\nExample: [private repo again]\n\n\n\nExample Setup Real World HPC Project\nSuppose we have the following structure on an HPC cluster.\n\nfloswald@PTL11077 ~/.j/d/LandUse (rev2)&gt; tree -L 1 \n.\n├── Manifest.toml\n├── Project.toml\n├── slurm_runner.run\n├── run.jl\n├── src\n├── test\nwith this content for the file run.jl:\nusing Distributed\n\nprintln(\"some welcome message from master\")\n\n# add 10 processes from running master\n# notice that we start them in the same project environment!\naddprocs(10, exeflags = \"--project=.\")  \n\n# make sure all packages are available everywhere\n@everywhere using Pkg\n@everywhere Pkg.instantiate()\n\n# load code for our application\n@everywhere using LandUse\n   \n   \nLandUse.bigtask(some_arg1 = 10, some_arg2 = 4)\nThe corresponding submit script for the HPC scheduler (SLURM in this case) would then just call this file:\n#!/bin/bash\n#SBATCH --job-name=landuse\n#SBATCH --output=est.out\n#SBATCH --error=est.err\n#SBATCH --partition=ncpulong\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=11 # same number as we addproc'ed + 1\n#SBATCH --mem-per-cpu=4G   # memory per cpu-core\n\njulia --project=. run.jl\n\n\nJuliaHub\nThe best alternative out there IMHO is juliahub. Instead of run.jl, you’d have this instead:\nusing Distributed\nusing JSON3\nusing CSV\nusing DelimitedFiles\n\n@everywhere using bk\n\nresults = bk.bigjob()  # runs a parallel map over workers with `pmap` or similar.\n\n# bigjob writes plots and other stuff to path_results\n\n# oputput path\npath_results = \"$(@__DIR__)/outputs\"\nmkpath(path_results)\nENV[\"RESULTS_FILE\"] = path_results\n\n# write text results to JSON (numbers etc)\nopen(\"results.json\", \"w\") do io\n    JSON3.pretty(io, results)\nend\n\nENV[\"RESULTS\"] = JSON3.write(results)\n\n\nParallel Map and Loops\nThis is most relevant use case in most of our applications. tbc.",
    "crumbs": [
      "Home",
      "Lectures",
      "7 - Parallel Computing"
    ]
  },
  {
    "objectID": "lectures/lecture4.html",
    "href": "lectures/lecture4.html",
    "title": "Lecture 4: Optimization 2 and 3",
    "section": "",
    "text": "We continued our exploration of optimization algorithms. We also talked about how to solve systems of equations arising in many economic models, and we showcased an integer programming problem based on my research with Marleen where we reallocate buses across garages in London. 🚌\n\n\n\nTopic\nNotebook\n\n\n\n\nAlgorithms, and JuMP\ndownload notebook\n\n\nConstraints, (Mixed) Integer Problems\ndownload notebook\n\n\nSolving Systems of equations\ndownload notebook\n\n\nMIP Bus reallocation\ndownload notebook\n\n\n\n\n\n\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Lectures",
      "4 - Optimization 2"
    ]
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation Instructions",
    "section": "",
    "text": "On this page you find the relevant installation instructions for this course. Please come with all components installed to the first class.\n© Florian Oswald, 2026",
    "crumbs": [
      "Home",
      "Setup",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#windows",
    "href": "installation.html#windows",
    "title": "Installation Instructions",
    "section": "1.1 Windows",
    "text": "1.1 Windows\n\nYou need at least windows 10, better windows 11.\nYou need to install the Windows Subsystem for Windows (WSL) version 2, i.e. WSL2. You should never just do what I say, but instead consult the official documentation. Therein, you will see that installing this is as easy as typing\n# start your PowerShell as Administrator\n# (right click on PowerShell...)\nwsl --install\n# choose all the default settings (in particular, install the Ubuntu Distribution)\n# restart your computer after this is completed.\nAfter successful installation and restart, have a look at your file browser. There is a like a second filesystem now - the one for linux!\nStart the Ubuntu terminal by typing “Ubuntu” in your Windows Start Button.\nIn the running terminal type commands, followed each time by enter:\npwd  # tells you where you are\nwhoami # tells you who you are \nhostname # tells you the name of this host",
    "crumbs": [
      "Home",
      "Setup",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#nix",
    "href": "installation.html#nix",
    "title": "Installation Instructions",
    "section": "1.2 *nix",
    "text": "1.2 *nix\nComputers not running a Windows OS are typically grouped together under the term *nix (star nix) - i.e. most current unix based OS out there. This includes MacOS.\nIf you have one of those, you have it easier.\n\n1.2.1 MacOS\nOpen the terminal application and type\ngit --version\nif that throws an error, type\nxcode-select --install\n\n\n1.2.2 Ubuntu\nIf you have any Linux distribution (Ubuntu, RedHat etc), just make sure you have the main tools installed.\n# type this and enter\nwhich git make gcc python3\nYou could then install any missing packages with\nsudo apt update\nsudo apt install -y build-essential git python3 python-is-python3",
    "crumbs": [
      "Home",
      "Setup",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#via-juliaup",
    "href": "installation.html#via-juliaup",
    "title": "Installation Instructions",
    "section": "2.1 via juliaup",
    "text": "2.1 via juliaup\nThe best way to install julia (allows for multiple installed versions and will always alert to new releases) is via the instructions at juliaup - please follow those for your OS.",
    "crumbs": [
      "Home",
      "Setup",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#via-direct-download",
    "href": "installation.html#via-direct-download",
    "title": "Installation Instructions",
    "section": "2.2 via direct download",
    "text": "2.2 via direct download\nyou can equally well download the current release for your OS from here",
    "crumbs": [
      "Home",
      "Setup",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#second-time-running-pluto-opening-a-notebook",
    "href": "installation.html#second-time-running-pluto-opening-a-notebook",
    "title": "Installation Instructions",
    "section": "5.1 Second time: Running Pluto & opening a notebook",
    "text": "5.1 Second time: Running Pluto & opening a notebook\nRepeat the following steps whenever you want to work on a project or homework assignment which requires Pluto.\n\n5.1.1 Start Pluto\nStart the Julia REPL, like you did during the setup. In the REPL, type:\njulia&gt; using Pluto\n\njulia&gt; Pluto.run()\n\n\n\nimage\n\n\nThe terminal tells us to go to http://localhost:1234/ (or a similar URL). Let’s open Firefox or Chrome and type that into the address bar.\n\n\n\nimage\n\n\n\nIf you’re curious about what a Pluto notebook looks like, have a look at the sample notebooks. Samples 1, 2 and 6 may be useful for learning some basics of Julia programming.\nIf you want to hear the story behind Pluto, have a look a the JuliaCon presentation.\n\nIf nothing happens in the browser the first time, close Julia and try again.\n\n\n5.1.2 Opening a notebook from the web\nThis is the main menu - here you can create new notebooks, or open existing ones. Our homework assignments will always be based on a template notebook, available in this GitHub repository. To start from a template notebook on the web, you can paste the URL into the blue box and press ENTER.\nFor example, homework 0 is available here. Copy this link (right click -&gt; Copy Link), and paste it into the box. Press ENTER, and select OK in the confirmation box.\n\n\n\nimage\n\n\nThe first thing we will want to do is to save the notebook somewhere on our own computer; see below.\n\n\n5.1.3 Opening an existing notebook file\nWhen you launch Pluto for the second time, your recent notebooks will appear in the main menu. You can click on them to continue where you left off.\nIf you want to run a local notebook file that you have not opened before, then you need to enter its full path into the blue box in the main menu. More on finding full paths in step 3.",
    "crumbs": [
      "Home",
      "Setup",
      "Installation"
    ]
  },
  {
    "objectID": "notebooks/week4/Figures.html",
    "href": "notebooks/week4/Figures.html",
    "title": "BFGS and GradientDescent",
    "section": "",
    "text": "using Interact, Optim, Plots, OptimTestProblems, LineSearches, LinearAlgebra\n\n\n\n    Unable to load WebIO. Please make sure WebIO works for your Jupyter client.\n    For troubleshooting, please see \n    the WebIO/IJulia documentation.\n    \n\n\n\n\nprob = UnconstrainedProblems.examples[\"Rosenbrock\"]\nf = prob.f\ng! = prob.g!\nh! = prob.h!\n\nrosenbrock_hessian! (generic function with 1 method)\n\n\n\nx = [-1,1.]\nres = optimize(f, x, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\n@manipulate for ix = slider(1:Optim.iterations(res), value = 1)\n    contour(-2.5:0.01:2, -1.5:0.01:2, (x,y)-&gt;sqrt(f([x, y])), fill=true, color=:deep, legend=false)\n    xtracemat = hcat(Optim.x_trace(res)...)\n    plot!(xtracemat[1, 1:ix], xtracemat[2, 1:ix], mc = :white, lab=\"\")\n    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, lab=\"\")\n    scatter!([1.], [1.], mc=:red, msc=:red, lab=\"\")\n    scatter!(x[1:1], x[2:2], mc=:yellow, msc=:black, label=\"start\", legend=true)\nend\n\n\n    \n\n\n\n\nx = [-4, -3.]\n@manipulate for ix = slider(120:160, value = 120)\n    c = contour(-.85:0.01:-0.60, 0:0.01:1.2, (x,y)-&gt;sqrt(f([x, y])), fill=true, color=:deep, legend=false)\n    res = optimize(f, x, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\n    xtracemat = hcat(Optim.x_trace(res)...)\n    \n    plot!(xtracemat[1, 120:ix], xtracemat[2, 120:ix], mc = :white, legend=true, label=\"Gradient Descent\")\n    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, label = \"\")\n    scatter!(xtracemat[1:1,120], xtracemat[2:2,120], mc=:blue, msc=:blue, label=\"start\")\n    p = plot(1:ix, [Optim.trace(res)[i].metadata[\"Current step size\"] for i  = 1:ix], \"alpha\")\n    plot(c, p, layout=(2,1))\nend\n\n\nx = [-1., -1.]\nres = optimize(f, x, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\nres2 = optimize(f, x, Newton(), Optim.Options(store_trace=true, extended_trace=true))\n@manipulate for ix = slider(1:Optim.iterations(res2), value = 1)\n    p = contour(-2.5:0.01:2, -1.5:0.01:2, (x,y)-&gt;sqrt(f([x, y])), fill=true, color=:deep, legend=false)\n    xtracemat = hcat(Optim.x_trace(res)...)\n    plot!(xtracemat[1, 1:ix], xtracemat[2, 1:ix], mc = :white,  label=\"Gradient Descent\", legend=true)\n    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, label=\"\")\n    \n    xtracemat2 = hcat(Optim.x_trace(res2)...)\n    plot!(xtracemat2[1, 1:ix], xtracemat2[2, 1:ix], c=:blue, label=\"Newton\")\n    scatter!(xtracemat2[1:1,ix], xtracemat2[2:2,ix], mc=:black, msc=:blue, label=\"\")\n    scatter!([1.], [1.], mc=:red, msc=:red, label=\"\")\n    scatter!(x[1:1], x[2:2], mc=:yellow, msc=:yellow, label=\"start\")\nend\n\n\n## Linesearch\n# say we're at x = [-0.5,-0.5]\n# What is the gradient descent step?\nx_ls = [1.5,1.01]\nstor = similar(x_ls)\nh_stor = zeros(2,2)\ng!(stor, x_ls)\nh!(h_stor, x_ls)\nprintln(\"Gradient descent search direction: \", stor)\nprintln(\"Newton's method search direction: \", h_stor\\stor)\n\n\nf_ls=f(x_ls)\nxs = (0:0.0001:0.0051)\nplot(plot(xs,t-&gt; f(x_ls-t*stor),label=\"Gradient Descent\"),\nplot(xs,t-&gt;f(x_ls-t*(h_stor\\stor)), label=\"Newton's method\"))\n\n\n@manipulate for xx = slider(-2.:0.5:2., value = -2., label=\"x₁\"), yy = slider(-2.:0.5:2., value = -2., label=\"x₂\"), solver = Dict(\"GradientDescent()\" =&gt; GradientDescent, \"Newton()\" =&gt; Newton, \"BFGS()\" =&gt; BFGS), linesearch = Dict(\"HagerZhang()\" =&gt; LineSearches.HagerZhang(), \"MoreThuente()\" =&gt; LineSearches.MoreThuente(), \"BackTracking(order=3)\" =&gt; LineSearches.BackTracking(order=3), \"BackTracking(order=2)\" =&gt; LineSearches.BackTracking(order=2))\n    x = [xx, yy]\n    contour(-2.5:0.01:2.5, -2.5:0.01:4.0, (x,y)-&gt;sqrt(f([x, y])), fill=true, color=:deep, legend=false)\n    res = optimize(f, g!, h!, x, solver(linesearch=linesearch), Optim.Options(store_trace=true, extended_trace=true))\n    xtracemat = hcat(Optim.x_trace(res)...)\n    plot!(xtracemat[1, :], xtracemat[2, :], mc = :white, lab=\"\")\n    scatter!(xtracemat[1, 1:10:end], xtracemat[2, 1:10:end], mc = :white, lab=\"\")\n    scatter!([1.], [1.], mc=:red, msc=:red, lab=\"\")\n    scatter!(x[1:1], x[2:2], mc=:yellow, msc=:black, label=\"start\", legend=true)\nend\n\n\n# g(x, y) = dot([x, y]'*diagm([1.0, 1.0]),[x,y])\ng(x, y, v) = dot([x, y]'*diagm([1.0, v]),[x,y])\n\n\n@manipulate for xx = slider(-2.:.1:2., value = -2., label=\"x2\"), val = slider(1.0:0.5:150.0, label=\"v\", value = 1.0)\n    _x = [5., xx]\n    contour(-6.5:0.1:5.5, -3.5:0.1:3.5, (x,y)-&gt;g(x,y,val), fill=false, color=:deep, legend=false)\n    res = optimize(x-&gt;g(x[1],x[2],val), _x, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\n    xtracemat = hcat(Optim.x_trace(res)...)\n    plot!(xtracemat[1, :], xtracemat[2, :], mc = :white)\n    scatter!(xtracemat[1, :], xtracemat[2, :], mc = :white)\n   #\n    scatter!([0.], [0.], mc=:red, msc=:red)\nend\n\n\nfh(x, y, v) = dot([x, y]'*[1.0 v/10;0.0 v],[x,y])\n\n\n@manipulate for xx = slider(-2.:.1:2., value = -2., label=\"x2\"), val = slider(10.0:0.5:150.0, value = 1.0, label=\"v\"), solver = Dict(:GradientDescent =&gt; GradientDescent(), :Newton =&gt; Newton(), :BFGS =&gt; BFGS())\n    _x = [5., xx]\n    contour(-6.5:0.1:5.5, -3.5:0.1:3.5, (x,y)-&gt;fh(x,y,val), fill=false, color=:deep, legend=false)\n    res = optimize(x-&gt;fh(x[1],x[2],val), _x, solver, Optim.Options(store_trace=true, extended_trace=true))\n    xtracemat = hcat(Optim.x_trace(res)...)\n    plot!(xtracemat[1, :], xtracemat[2, :], mc = :white)\n    scatter!(xtracemat[1, :], xtracemat[2, :], mc = :white)\n   #\n    scatter!([0.], [0.], mc=:red, msc=:red)\nend\n\n\n@manipulate for xx = slider(-2.:.1:2., value = -2., label=\"x2\"), val = slider(1.0:0.5:150.0, value = 1.0, label=\"v\")\n    _x = [5., xx]\n    contour(-6.5:0.1:5.5, -3.5:0.1:3.5, (x,y)-&gt;fh(x,y,val), fill=false, color=:deep, legend=false)\n    res = optimize(x-&gt;fh(x[1],x[2],val), _x, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\n    res2 = optimize(x-&gt;fh(x[1],x[2],val), _x, BFGS(), Optim.Options(store_trace=true, extended_trace=true))\n    xtracemat = hcat(Optim.x_trace(res)...)\n    plot!(xtracemat[1, :], xtracemat[2, :], mc = :white, label=\"GradientDescent\", legend=true)\n    scatter!(xtracemat[1, :], xtracemat[2, :], mc = :white, label=\"\")\n    xtracemat2 = hcat(Optim.x_trace(res2)...)\n    plot!(xtracemat2[1, :], xtracemat2[2, :], c = :black, mc = :blue, label=\"BFGS\")\n    scatter!(xtracemat2[1, :], xtracemat2[2, :], mc = :blue, label=\"\")\n\n   #\n    scatter!([0.], [0.], mc=:red, msc=:red)\nend\n\n\n\n\n© Florian Oswald, 2026"
  }
]