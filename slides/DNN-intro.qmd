---
title: Very Short Intro Deep Learning
format:
  revealjs:
    theme: _extensions/clean/clean.scss
filters:
  - fontawesome
engine: julia
---

## Purpose

* I want to give you a crash course in Deep Learning with Julia
* We don't have the time to give a full overview of this huge field. 
* I recommend the interested reader to consult 
  * [http://statlearning.com/](http://statlearning.com/)
  * [https://bio322.epfl.ch/](https://bio322.epfl.ch/)
* Julia Tools:
  * [https://juliaml.ai](https://juliaml.ai)
  * [https://fluxml.ai](https://fluxml.ai)
  * [DataScienceTutorials.jl/](https://juliaai.github.io/DataScienceTutorials.jl/#!)



## What are Machine Learning (ML), AI and Deep Learning?

* ML is a branch of AI concerned with algo development.
* We want to generalise models to unseen data.
* AI wants to develop algos that do not need to be programmed - they *learn* how to improve themselves.
* Deep Learning is a form of an *Artificial Neural Network* with layers.

### (Deep) Neural Networks?

* Take a $p$ dimensional input vector $X$ and build a nonlinear function $f(X)$ to predict output $Y$.
* $f$ obeys a certain structure, which - **importantly** - allows automatic differentiation during optimization and parameter search.

## Taxonomy

1. supervised
2. unsupervised
3. RL

## Taxonomy details


## Taxonomy examples


## Deep learning - Artificial Neural Networks (ANN)

Components of an ANN:

1. Artificial Neurons: each neuron gets an input signal, processes it and outputs another signal (think: *one number in, one number out*)
2. Edges: connections between neurons (undirected, because can we want to go back and forwards)


::: {.callout-tip}
# Real Neurons

Real neurons in a human brain have many more ways of computing, calling this *neurons* is a stretch of imagination. Marketing.

:::



<!-- https://alexlenail.me/NN-SVG/ -->

## Example: A Single Layer ANN

* input $x$ (often a vector) which gets transformed to an 
* output $y$ (can also be a vector). We apply an *activation function* $\phi$ to a linear transformation of the input:


<table>
<tr>
<td style="vertical-align: middle;">
$$ x = 
\left(
\begin{align}
x_{1} \\
x_{2} \\
x_{3}
\end{align}
\right)
$$
</td>
<td>
<img src="images/nn3-1.svg" style="width: 300px; height: 200;"/>
</td>
<td style="vertical-align: middle;">
$$\begin{align} 
  z &= w'x + b \\  
  y &= \phi(z) 
  \end{align}
$$
</td>
</tr>
</table>

::: {.callout-note}
$w$ is a vector or weights, $b$ is an *intercept* or *bias* term. $\phi$ is (in general) a nonlinear function. This example has a 3-dim input and a 1-dim output, and a single layer (i.e. just the output layer).
:::


## Bias? Why Bias?

$$\begin{align} 
  z &= w'x + b \\  
  y &= \phi(z) 
  \end{align}
$$

* Well we know that $b$ is just the intercept. $z$ is nothing but a linear transformation.
* We economists call $z$ "a regression". The intercept shifts the line/hyperplane up and down, else it passes through the origin. 
* So, this *just* takes a linear transform of $x$ and sticks it into a nonlinear function $\phi$.


## What are those $\phi$ functions then?

* It's key that they are *nonlinear*.
* Typical choices are
  * *sigmoid*: $\phi(x) = \frac{1}{1 + \exp(-x)}$
  * ReLU (rectified linear unit): $\phi(x) = \begin{cases} 0 & \text{if } x<0\\x & \text{else.} \end{cases}$
* many others (*softmax*, *tanh*, *Leaky ReLU*, *GELU*,...)


## Deep Neural Networks (DNNs)

Single input layer, multiple hidden layers, single output layer

<img src="images/nn-4-5-2.svg"/>


## DNNs

<img src="images/nn-2-2-1-1.svg"/>

* Notice: each neuron towards the right depends on *entire* network left of it.
* $y$ depends on $h^{(2)}$, which depends on $h_1^{(1)}$ and $h_2^{(1)}$.


## DNNs

<img src="images/nn-2-2-1-1.svg"/>

::: {.columns}
::: {.column}
$$\begin{align}
h_1^{(1)} &= \phi(w_{11}^{(1)} x_1 + w_{12}^{(1)} x_2 + b_1^{(1)}) \\
h_2^{(1)} &= \phi(w_{21}^{(1)} x_1 + w_{22}^{(1)} x_2 + b_2^{(1)})\\
h^{(2)} &= \phi(w_1^{(2)} h_1^{(1)} + w_2^{(2)} h_2^{(1)} + b^{(2)}) \\
y &= \phi(w^{(3)} h^{(2)} + b^{(3)})
\end{align}$$
:::
::: {.column}
* $w_{12}^{(1)}$ strength of $x_2 \rightarrow h_1^{(1)}$
* $\phi$ typically constant in layer
* $\phi$ can change across layers
:::
:::
<!-- end columns -->


::: {.notes}
example of calculations and how the input to each subsequent layer are all the outputs of the preceding layer
:::

# Example: Hand Written Digit Recognition

## Simplest Problem: Detect `/` vs `\`

<br>

::: {.columns width="45%"}
::: {.column}
* Suppose we have 4 pixels of data, arranged as a square
* Each pixel can be white (*on*, `1`) or black (*off*, `0`).
:::
::: {.column width="5%"}
:::
::: {.column width="45%"}
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <rect x="0" y="0" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="0" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="0" y="100" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="100" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
</svg>
:::
:::
<!-- end columns -->

::: {.fragment}

* Imagine this is a *very* low quality digital photograph of somebody's handwritten `/`
* like, 4 pixels of information only. 
* You can see why we chose `/` for this exercise, any other digit is too complex for this. More on that later.
:::


## Simplest Problem: Detect `/` vs `\`

<br>

::: {.columns width="45%"}
::: {.column}
* Suppose we have 4 pixels of data, arranged as a square
* Each pixel can be white (*on*, `1`) or black (*off*, `0`).
:::
::: {.column width="5%"}
:::
::: {.column width="45%"}
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <rect x="0" y="0" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="0" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="0" y="100" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="100" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
</svg>
:::
:::
<!-- end columns -->




::: {.columns width="45%"}
::: {.column}
*  We want to *recognize* from those a *white* forward slash
*  or *white* backward slash
*  Those represent *handwriting* in this example.
:::
::: {.column width="5%"}
:::
::: {.column width="45%"}
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <text  x="100" y="175" font-size="200" text-anchor="middle"  style="fill:white;stroke:white;">⟋</text>
</svg>
:::
:::



## 

<table style="table-layout: fixed!important;width:700px;">
<tr style="border: 0;border-style:hidden;">
<td style="vertical-align: middle;padding:0;width:200px!important;">
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <rect x="0" y="0" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="0" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="0" y="100" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="100" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
</svg>
</td>
<td style="vertical-align: middle;padding:0;width:300px!important;">
<i class="fas fa-arrow-right" style="font-size:100px;padding:100px;"></i>
</td>
<td style="vertical-align: middle;padding:0;padding:0;width:200px!important;">
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <text  x="100" y="175" font-size="200" text-anchor="middle"  style="fill:white;stroke:white;">⟋</text>
</svg>
</td>
</tr>
<tr style="border: 0;border-style:hidden;">
<td style="vertical-align: middle;padding:0;width:200px!important;">
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <rect x="0" y="0" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="0" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
  <rect x="0" y="100" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="100" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
</svg>
</td>
<td style="vertical-align: middle;padding:0;width:300px!important;">
<i class="fas fa-arrow-right" style="font-size:100px;padding:100px;"></i>
</td>
<td style="vertical-align: middle;padding:0;padding:0;width:200px!important;">
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <text  x="100" y="175" font-size="200" text-anchor="middle"  style="fill:white;stroke:white;">⟍</text>
</svg>
</td>
</tr>
</table>



## Simplest Problem: Detect `/` vs `\`


### How does supervised learning work with NNs?

* We provide the algo with *pairs* of values: input, output $(X,Y)$
* We *train* it on those pairs.
* Then we give it a new $X$ and want a new $Y$ back.

::: {.fragment}
* For *regression tasks* that's ok. Vector $x$ in, number $y$ out.
* But *classification* (is this picture `/` or `\`?) needs a tweak.
:::

::: {.fragment}
We need to *encode* classes (`/` or `\`) into numbers somehow.
:::



## Simplest Problem: Detect `/` vs `\`


### Input Encoding

Let's go down column-wise in each box of squares and record `0` for black and `1` for white. Each square is $x_1,\dots,x_4 \in \{0,1\}$

<table style="table-layout: fixed!important;width:700px;">
<tr style="border: 0;border-style:hidden;">
<td style="vertical-align: middle;padding:0;width:200px!important;">
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <rect x="0" y="0" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="0" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="0" y="100" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="100" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
</svg>
</td>
<td style="vertical-align: middle;padding:0;font-size:100px;width:200px!important;">
<i style="padding:100px;"> {{< fa arrow-right >}} </i>
</td>
<td style="vertical-align: middle;padding-bottom:50px;:0;font-size:100px;width:400px!important;">
(0,1,1,0)
</td>
</tr>
<tr style="border: 0;border-style:hidden;padding-bottom:100px">
<td style="vertical-align: middle;padding:0;width:200px!important;"></td>
</tr >

<tr style="border: 0;border-style:hidden;">
<td style="vertical-align: middle;padding:0;width:200px!important;">
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <rect x="0" y="0" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="0" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
  <rect x="0" y="100" width="100" height="100" style="fill:black;stroke:gray;stroke-width:3px;"/>
  <rect x="100" y="100" width="100" height="100" style="fill:white;stroke:gray;stroke-width:3px;"/>
</svg>
</td>
<td style="vertical-align: middle;padding:0;font-size:100px;width:200px!important;">
<i style="padding:100px;"> {{< fa arrow-right >}} </i>
</td>
<td style="vertical-align: middle;padding-bottom:50px;:0;font-size:100px;width:400px!important;">
(1,0,0,1)
</td>
</tr>
</table>


## Simplest Problem: Detect `/` vs `\`

### Output encoding

Same for output $Y$. We use a **one-hot encoding** with a 2-dimensional vector $(y_{1}, y_{2})$.

<br>

::: {.callout-tip}
# One hot encoding vs dummy variables

* Dummy vars typically have a reference category. For K levels you need K-1 columns. Like `red,green,blue` is K=3.
* One-hot encodes *all* categories.
:::




## Simplest Problem: Detect `/` vs `\`

### Output encoding

Same for output $Y$. We use a **one-hot encoding** with a 2-dimensional vector $(y_{1}, y_{2})$.


::: {.columns}
::: {.column}

**Dummy Variables**

```R
green blue
  0     0
  1     0
  0     1
  0     0
  0     1

```

:::
::: {.column}

**One-hot Encoding**

```R
red green blue
 1    0     0
 0    1     0
 0    0     1
 1    0     0
 0    0     1
```
:::
:::


## Simplest Problem: Detect `/` vs `\`


### Output encoding: One-hot

Let's go with the following. You can see that this is arbitrary (we could easily have inverted this without consequences.)

<table style="table-layout: fixed!important;width:700px;">
<tr style="border: 0;border-style:hidden;">
<td style="vertical-align: middle;padding:0;width:200px!important;">
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <text  x="100" y="175" font-size="200" text-anchor="middle"  style="fill:white;stroke:white;">⟋</text>
</svg>
</td>
<td style="vertical-align: middle;padding:0;font-size:100px;width:200px!important;">
<i style="padding:100px;"> {{< fa arrow-right >}} </i>
</td>
<td style="vertical-align: middle;padding-bottom:50px;:0;font-size:100px;width:300px!important;">
(1,0) 
</td>
</tr>
<tr style="border: 0;border-style:hidden;">
<td style="vertical-align: middle;padding:0;width:200px!important;">
<svg width="200" height="200">
  <rect width="200" height="200" style="fill:black;" />
  <text  x="100" y="175" font-size="200" text-anchor="middle"  style="fill:white;stroke:white;">⟍</text>
</svg>
</td>
<td style="vertical-align: middle;padding:0;font-size:100px;width:200px!important;">
<i style="padding:100px;"> {{< fa arrow-right >}} </i>
</td>
<td style="vertical-align: middle;padding-bottom:50px;:0;font-size:100px;width:300px!important;">
(0,1) 
</td>
</tr>
</table>

## Simplest Problem: Detect `/` vs `\`

<table>
<tr>
<td style="vertical-align: middle;">
$$ x = 
\left(
\begin{align}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4}
\end{align}
\right)
$$
</td>
<td>
<img src="images/nn4-2.svg" style="width: 400px; height: 250px; vertical-align: top;"/>

</td>
<td style="vertical-align: middle;">
$$\left( 
  \begin{align} 
  y_{1} \\  
  y_{2} 
  \end{align} \right) = 
  y$$
</td>
</tr>
</table>


::: {.callout-tip}
# Computing parameters
We need $y = \phi(z)$ and $z=Wx + b$.

1. $W$ is a $(2,4)$ matrix of weights
2. $b$ is a $(2,1)$ vector of biases
:::


## Simplest Problem: Detect `/` vs `\`

### Linear Activation Function (for Teaching only)

* Let's assume that $\phi(x)=x$ so we can compute the coefficients.
  
$$y = \phi(Wx + b) = Wx + b$$

* Which, written out is


$$\left( \begin{array}{c} y_{1} \\ y_{2}  \end{array} \right) = \left( \begin{array}{cccc} w_{1,1} & w_{1,2} & w_{1,3} & w_{1,4} \\ w_{2,1} & w_{2,2} & w_{2,3} & w_{2,4}  \end{array} \right) \left( \begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \end{array} \right) + \left( \begin{array}{c} b_{1} \\ b_{2}  \end{array} \right)$$


## 

There are 2 relevant cases, and we need to find values for $W$ and $b$ such that a certain tuple of x values results in a certain y output.

$$\left( \begin{array}{cccc} w_{1,1} & w_{1,2} & w_{1,3} & w_{1,4} \\ w_{2,1} & w_{2,2} & w_{2,3} & w_{2,4}  \end{array} \right) \left( \begin{array}{c} 0 \\ 1\\ 1\\ 0 \end{array} \right) + \left( \begin{array}{c} b_{1} \\ b_{2}  \end{array} \right) = \left( \begin{array}{c} 1 \\ 0  \end{array} \right)$$

$$\left( \begin{array}{cccc} w_{1,1} & w_{1,2} & w_{1,3} & w_{1,4} \\ w_{2,1} & w_{2,2} & w_{2,3} & w_{2,4}  \end{array} \right) \left( \begin{array}{c} 1 \\ 0\\ 0\\ 1 \end{array} \right) + \left( \begin{array}{c} b_{1} \\ b_{2}  \end{array} \right) = \left( \begin{array}{c} 0 \\ 1  \end{array} \right)$$

## {transition="fade" transition-speed="slow"}


There are 2 relevant cases, and we need to find values for $W$ and $b$ such that a certain tuple of x values results in a certain y output.

$$\left( \begin{array}{c} w_{1,2} + w_{1,3}  \\ w_{2,2} + w_{2,3}   \end{array} \right) + \left( \begin{array}{c} b_{1} \\ b_{2}  \end{array} \right) = \left( \begin{array}{c} 1 \\ 0  \end{array} \right)$$

$$\left( \begin{array}{c} w_{1,1} + w_{1,4}  \\ w_{2,1} + w_{2,4}   \end{array} \right) + \left( \begin{array}{c} b_{1} \\ b_{2}  \end{array} \right) = \left( \begin{array}{c} 1 \\ 0  \end{array} \right)$$

## {transition="fade" transition-speed="slow"}


There are 2 relevant cases, and we need to find values for $W$ and $b$ such that a certain tuple of x values results in a certain y output.

$$\begin{aligned}
w_{1,2} + w_{1,3} + b_{1} &= 1 \\
w_{2,2} + w_{2,3} + b_{2} &= 0 \\
w_{1,1} + w_{1,4} + b_{1} &= 1 \\
w_{2,1} + w_{2,4} + b_{2} &= 0
\end{aligned}$$

If we can find values $W$ and $b$ such that this holds, our NN will perfectly recognize `/` and `\`.

::: {.callout-note}
Solving 4 equations with 10 variables: we have 6 degrees of freedom too many. Can just choose an arbitrary number for those and solve system for the remaining four.
:::

## Computed weights and biases

I set $b=0$ and $w_{11},w_{12},w_{21},w_{23}=0.5$. This gives

$$\left( \begin{array}{cccc} 0.5 & 0.5 & 0.5 & -0.5 \\ 0.5 & 0.5 & -0.5 & 0.5  \end{array} \right) \left( \begin{array}{c} 0 \\ 1\\ 1\\ 0 \end{array} \right) + \left( \begin{array}{c} 0 \\ 0  \end{array} \right) = \left( \begin{array}{c} 1 \\ 0  \end{array} \right)$$

$$\left( \begin{array}{cccc} 0.5 & 0.5 & 0.5 & -0.5 \\ 0.5 & 0.5 & -0.5 & 0.5 \end{array} \right) \left( \begin{array}{c} 1 \\ 0\\ 0\\ 1 \end{array} \right) + \left( \begin{array}{c} 0 \\ 0  \end{array} \right) = \left( \begin{array}{c} 0 \\ 1  \end{array} \right)$$


## Simplest Problem: Detect `/` vs `\`

<br>

* A *very* simple NN with a linear activation function was perfectly able to recognize those characters from colored (black or white) pixels
* 4 input neurons 
* 2 output neurons

::: {.fragment}
* Increasing pixels: increasing resolution of the image
* more variables to deal with, and nonlinear activations: cannot solve by hand.
:::


## Demo Setup

```{julia}
#| echo: true
function demo(x=nothing)
    println("Simple Neural Network Demo")
    println("="^40)
    if x === nothing
        x = rand(4)
    elseif !(isa(x, Vector{<:Real}) && length(x) == 4)
        error("Input 'x' must be a vector of 4 real numbers")
    end
    
    # Input
    # Weights and biases
    W = [0.5  0.5  0.5 -0.5;
         0.5  0.5 -0.5  0.5]
    b = [0, 0]
    
    # Output    
    y = W * x + b
    
    println("Input:")
    println("┌─────┬─────┐")
    println("│ $(round(x[1], digits=1)) │ $(round(x[2], digits=1)) │")
    println("├─────┼─────┤")
    println("│ $(round(x[3], digits=1)) │ $(round(x[4], digits=1)) │")
    println("└─────┴─────┘")
    
    # simple rule: (1,0) means `/`
    # so let's say any (q,r) means `/` as long as q > r
    println("Output:")
    if y[1] >= y[2]
        println("┌───┐")
        println("│ ⟋ │")
        println("└───┘")
    end
    if y[1] ≈ y[2]
        println("or")
    end
    if y[2] >= y[1]
        println("┌───┐")
        println("│ ⟍ │")
        println("└───┘")
    end
    
    return y
end
```

## Run Demo

<br>

```{julia}
#| echo: true
demo([0,1,1,0])  # should give `/`
```

## Run Demo

<br>

```{julia}
#| echo: true
demo([1,0,0,1])  # should give `\`
```

## Run Demo on *unseen* data

How does this NN generalize to *test* data?

<br>

```{julia}
#| echo: true
demo([0.5,0.1,0.2,0.4])  # square with shades of grey
```

## An even simpler example: A Line

* Extreme case: 1 input, 1 output. 
* (Of course we don't *need* a neural network for that)

<!-- ```{julia}
truth(x) = 3 + 2x  # true DGP
# x for training and testing
x_train, x_test = hcat(0:5...), hcat(6:10...)
# y too
y_train, y_test = actual.(x_train), actual.(x_test)
``` -->
